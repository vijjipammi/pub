{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "lstm-learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijjipammi/pub/blob/main/lstm_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW47Md64eRmm"
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers import Bidirectional\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufc9HcFWeRmo"
      },
      "source": [
        "## Sequence Estimation for 1 Dimensional Input and One Dimensional Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsUCylXteRmo",
        "outputId": "57481207-c3d0-4715-dc71-5eff218818bc"
      },
      "source": [
        "X = list()\n",
        "Y = list()\n",
        "X = [x+1 for x in range(20)]\n",
        "Y = [y * 15 for y in X]\n",
        "\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
            "[15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180, 195, 210, 225, 240, 255, 270, 285, 300]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSQOh9tGeRmo"
      },
      "source": [
        "X = array(X).reshape(20, 1, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81WMk-jfeRmo",
        "outputId": "72c889e6-9503-460c-cb0b-4b80b76ee132"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 1, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8bITzJHeRmo",
        "outputId": "e6f34a01-7601-48da-ed47-02d940018d08"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(10, activation='relu',return_sequences=True, input_shape=(1, 1)))\n",
        "model.add(LSTM(10, activation='relu'))\n",
        "\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 1, 10)             480       \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 10)                840       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 1,331\n",
            "Trainable params: 1,331\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqf9tj1ZeRmo",
        "outputId": "7c384f05-632e-41bf-d34a-a85fd6780b8c"
      },
      "source": [
        "model.fit(X, Y, epochs=2000, validation_split=0.2, batch_size=5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16 samples, validate on 4 samples\n",
            "Epoch 1/2000\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 20960.8559 - val_loss: 76928.9219\n",
            "Epoch 2/2000\n",
            "16/16 [==============================] - 0s 731us/step - loss: 20947.8486 - val_loss: 76884.2188\n",
            "Epoch 3/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 20935.4182 - val_loss: 76839.0156\n",
            "Epoch 4/2000\n",
            "16/16 [==============================] - 0s 416us/step - loss: 20921.4160 - val_loss: 76794.3594\n",
            "Epoch 5/2000\n",
            "16/16 [==============================] - 0s 605us/step - loss: 20907.6943 - val_loss: 76745.6875\n",
            "Epoch 6/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 20894.2958 - val_loss: 76693.6797\n",
            "Epoch 7/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 20879.4108 - val_loss: 76641.9219\n",
            "Epoch 8/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 20864.1086 - val_loss: 76587.2969\n",
            "Epoch 9/2000\n",
            "16/16 [==============================] - 0s 253us/step - loss: 20846.3782 - val_loss: 76526.1797\n",
            "Epoch 10/2000\n",
            "16/16 [==============================] - 0s 771us/step - loss: 20830.3610 - val_loss: 76458.7578\n",
            "Epoch 11/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 20810.6068 - val_loss: 76387.8438\n",
            "Epoch 12/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 20790.5178 - val_loss: 76312.4531\n",
            "Epoch 13/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 20767.7442 - val_loss: 76230.7188\n",
            "Epoch 14/2000\n",
            "16/16 [==============================] - 0s 189us/step - loss: 20743.8082 - val_loss: 76133.1484\n",
            "Epoch 15/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 20715.5720 - val_loss: 76021.0078\n",
            "Epoch 16/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 20684.0522 - val_loss: 75896.3281\n",
            "Epoch 17/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 20653.8749 - val_loss: 75749.1250\n",
            "Epoch 18/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 20612.6857 - val_loss: 75586.4531\n",
            "Epoch 19/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 20566.9168 - val_loss: 75405.4531\n",
            "Epoch 20/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 20522.5351 - val_loss: 75204.9219\n",
            "Epoch 21/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 20472.5358 - val_loss: 74979.0000\n",
            "Epoch 22/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 20412.7595 - val_loss: 74726.8516\n",
            "Epoch 23/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 20358.9217 - val_loss: 74435.6172\n",
            "Epoch 24/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 20283.1746 - val_loss: 74108.5156\n",
            "Epoch 25/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 20201.5425 - val_loss: 73728.2344\n",
            "Epoch 26/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 20122.1730 - val_loss: 73293.5547\n",
            "Epoch 27/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 20008.8881 - val_loss: 72816.7109\n",
            "Epoch 28/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 19898.5784 - val_loss: 72293.0625\n",
            "Epoch 29/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 19765.6349 - val_loss: 71729.7188\n",
            "Epoch 30/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 19629.3087 - val_loss: 71101.3906\n",
            "Epoch 31/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 19484.5295 - val_loss: 70453.0156\n",
            "Epoch 32/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 19332.1155 - val_loss: 69751.7266\n",
            "Epoch 33/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 19141.6471 - val_loss: 69036.4609\n",
            "Epoch 34/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 18949.6318 - val_loss: 68305.7344\n",
            "Epoch 35/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 18745.9784 - val_loss: 67516.5625\n",
            "Epoch 36/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 18555.8298 - val_loss: 66688.6875\n",
            "Epoch 37/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 18309.0918 - val_loss: 65886.6875\n",
            "Epoch 38/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 18123.7203 - val_loss: 65019.6250\n",
            "Epoch 39/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 17869.2352 - val_loss: 64175.3828\n",
            "Epoch 40/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 17647.8997 - val_loss: 63292.0781\n",
            "Epoch 41/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 17386.7899 - val_loss: 62440.8047\n",
            "Epoch 42/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 17124.7487 - val_loss: 61545.3047\n",
            "Epoch 43/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 16906.7589 - val_loss: 60589.3906\n",
            "Epoch 44/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 16644.3384 - val_loss: 59635.3984\n",
            "Epoch 45/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 16379.1779 - val_loss: 58668.9414\n",
            "Epoch 46/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 16107.2486 - val_loss: 57673.7656\n",
            "Epoch 47/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 15847.0330 - val_loss: 56724.8672\n",
            "Epoch 48/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 15597.2196 - val_loss: 55806.2422\n",
            "Epoch 49/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 15339.0697 - val_loss: 54912.7422\n",
            "Epoch 50/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 15075.4232 - val_loss: 54024.0234\n",
            "Epoch 51/2000\n",
            "16/16 [==============================] - ETA: 0s - loss: 19169.519 - 0s 0us/step - loss: 14846.7072 - val_loss: 53033.4180\n",
            "Epoch 52/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 14549.5313 - val_loss: 52004.2930\n",
            "Epoch 53/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 14292.0773 - val_loss: 50865.7461\n",
            "Epoch 54/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 13966.8695 - val_loss: 49769.6562\n",
            "Epoch 55/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 13681.9288 - val_loss: 48671.7617\n",
            "Epoch 56/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 13384.8204 - val_loss: 47601.0938\n",
            "Epoch 57/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 13102.6785 - val_loss: 46520.1406\n",
            "Epoch 58/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 12789.3777 - val_loss: 45449.1680\n",
            "Epoch 59/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 12509.4095 - val_loss: 44353.1719\n",
            "Epoch 60/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 12220.7213 - val_loss: 43219.8477\n",
            "Epoch 61/2000\n",
            "16/16 [==============================] - 0s 690us/step - loss: 11883.9266 - val_loss: 41970.6719\n",
            "Epoch 62/2000\n",
            "16/16 [==============================] - 0s 3us/step - loss: 11544.1741 - val_loss: 40645.1094\n",
            "Epoch 63/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 11197.1211 - val_loss: 39302.3594\n",
            "Epoch 64/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 10834.9855 - val_loss: 38024.9766\n",
            "Epoch 65/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 10469.8612 - val_loss: 36852.4414\n",
            "Epoch 66/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 10149.5117 - val_loss: 35686.5430\n",
            "Epoch 67/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 9877.6196 - val_loss: 34555.7812\n",
            "Epoch 68/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 9534.8509 - val_loss: 33489.7500\n",
            "Epoch 69/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 9218.2124 - val_loss: 32413.2969\n",
            "Epoch 70/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 8929.8738 - val_loss: 31301.5000\n",
            "Epoch 71/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 8632.8397 - val_loss: 30263.5898\n",
            "Epoch 72/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 8328.8131 - val_loss: 29287.0430\n",
            "Epoch 73/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 8084.5079 - val_loss: 28303.0430\n",
            "Epoch 74/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 7785.4851 - val_loss: 27357.7383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 75/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 7516.8210 - val_loss: 26325.6582\n",
            "Epoch 76/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 7228.0608 - val_loss: 25258.4688\n",
            "Epoch 77/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 6939.1683 - val_loss: 24207.3125\n",
            "Epoch 78/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 6618.5619 - val_loss: 23227.6758\n",
            "Epoch 79/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 6363.7257 - val_loss: 22227.3105\n",
            "Epoch 80/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 6076.0963 - val_loss: 21290.4609\n",
            "Epoch 81/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 5818.4876 - val_loss: 20346.3008\n",
            "Epoch 82/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 5564.8497 - val_loss: 19462.2109\n",
            "Epoch 83/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 5333.9210 - val_loss: 18596.1367\n",
            "Epoch 84/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 5082.3799 - val_loss: 17767.8770\n",
            "Epoch 85/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 4824.0050 - val_loss: 16951.0586\n",
            "Epoch 86/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 4610.8322 - val_loss: 16093.9795\n",
            "Epoch 87/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 4366.0882 - val_loss: 15247.4717\n",
            "Epoch 88/2000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 4147.0400 - val_loss: 14383.3125\n",
            "Epoch 89/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 3883.8450 - val_loss: 13612.5225\n",
            "Epoch 90/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 3679.1339 - val_loss: 12887.6240\n",
            "Epoch 91/2000\n",
            "16/16 [==============================] - 0s 66us/step - loss: 3456.4331 - val_loss: 12181.1934\n",
            "Epoch 92/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 3270.0238 - val_loss: 11441.5166\n",
            "Epoch 93/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 3067.9407 - val_loss: 10727.2285\n",
            "Epoch 94/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 2869.2873 - val_loss: 10020.5664\n",
            "Epoch 95/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 2667.0159 - val_loss: 9350.8965\n",
            "Epoch 96/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 2493.3946 - val_loss: 8737.6426\n",
            "Epoch 97/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 2315.9276 - val_loss: 8190.1426\n",
            "Epoch 98/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 2171.1175 - val_loss: 7680.3721\n",
            "Epoch 99/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 2038.3426 - val_loss: 7194.4385\n",
            "Epoch 100/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1905.3084 - val_loss: 6712.3647\n",
            "Epoch 101/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1764.7897 - val_loss: 6283.8149\n",
            "Epoch 102/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1650.4772 - val_loss: 5883.4795\n",
            "Epoch 103/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1534.1464 - val_loss: 5514.2031\n",
            "Epoch 104/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1434.4917 - val_loss: 5134.1938\n",
            "Epoch 105/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1331.9489 - val_loss: 4757.8452\n",
            "Epoch 106/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1235.0433 - val_loss: 4410.8203\n",
            "Epoch 107/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1131.9119 - val_loss: 4087.2566\n",
            "Epoch 108/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1057.9294 - val_loss: 3746.1672\n",
            "Epoch 109/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 958.4785 - val_loss: 3447.2451\n",
            "Epoch 110/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 878.1574 - val_loss: 3173.3054\n",
            "Epoch 111/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 804.5593 - val_loss: 2906.3323\n",
            "Epoch 112/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 726.3610 - val_loss: 2668.1575\n",
            "Epoch 113/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 666.3494 - val_loss: 2426.4958\n",
            "Epoch 114/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 599.3720 - val_loss: 2217.0342\n",
            "Epoch 115/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 544.5574 - val_loss: 2020.6926\n",
            "Epoch 116/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 493.8697 - val_loss: 1845.7346\n",
            "Epoch 117/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 448.6168 - val_loss: 1685.9216\n",
            "Epoch 118/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 405.4264 - val_loss: 1539.1494\n",
            "Epoch 119/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 367.7143 - val_loss: 1387.2010\n",
            "Epoch 120/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 327.5805 - val_loss: 1249.5698\n",
            "Epoch 121/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 292.4925 - val_loss: 1119.2781\n",
            "Epoch 122/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 261.7119 - val_loss: 1004.6388\n",
            "Epoch 123/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 231.1844 - val_loss: 908.4969\n",
            "Epoch 124/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 209.2941 - val_loss: 822.6772\n",
            "Epoch 125/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 186.1696 - val_loss: 748.7535\n",
            "Epoch 126/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 167.2436 - val_loss: 682.7900\n",
            "Epoch 127/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 151.7587 - val_loss: 620.3259\n",
            "Epoch 128/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 136.3644 - val_loss: 563.3412\n",
            "Epoch 129/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 121.2961 - val_loss: 511.2870\n",
            "Epoch 130/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 108.3170 - val_loss: 463.3345\n",
            "Epoch 131/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 96.5702 - val_loss: 415.4770\n",
            "Epoch 132/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 85.8754 - val_loss: 373.5264\n",
            "Epoch 133/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 75.5923 - val_loss: 337.7328\n",
            "Epoch 134/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 67.5063 - val_loss: 306.0693\n",
            "Epoch 135/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 59.9560 - val_loss: 278.1167\n",
            "Epoch 136/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 53.4943 - val_loss: 253.0928\n",
            "Epoch 137/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 47.7456 - val_loss: 230.7642\n",
            "Epoch 138/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 42.2881 - val_loss: 210.1676\n",
            "Epoch 139/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 37.6141 - val_loss: 189.2139\n",
            "Epoch 140/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 33.0665 - val_loss: 171.1329\n",
            "Epoch 141/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 29.3467 - val_loss: 155.0595\n",
            "Epoch 142/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 25.9410 - val_loss: 141.0123\n",
            "Epoch 143/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 22.9443 - val_loss: 128.3784\n",
            "Epoch 144/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 20.3737 - val_loss: 116.5355\n",
            "Epoch 145/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 17.9347 - val_loss: 106.2063\n",
            "Epoch 146/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 15.7574 - val_loss: 97.0994\n",
            "Epoch 147/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 13.9804 - val_loss: 88.3708\n",
            "Epoch 148/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 12.3424 - val_loss: 79.8585\n",
            "Epoch 149/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 10.8051 - val_loss: 72.2060\n",
            "Epoch 150/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 9.3107 - val_loss: 65.1732\n",
            "Epoch 151/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 976us/step - loss: 8.0819 - val_loss: 58.9527\n",
            "Epoch 152/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 7.1255 - val_loss: 53.2593\n",
            "Epoch 153/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 6.1011 - val_loss: 48.4455\n",
            "Epoch 154/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 5.3421 - val_loss: 43.8979\n",
            "Epoch 155/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 4.6827 - val_loss: 40.0460\n",
            "Epoch 156/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 4.1235 - val_loss: 36.6828\n",
            "Epoch 157/2000\n",
            "16/16 [==============================] - 0s 977us/step - loss: 3.6242 - val_loss: 33.6898\n",
            "Epoch 158/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 3.2668 - val_loss: 30.6516\n",
            "Epoch 159/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 2.8339 - val_loss: 28.1609\n",
            "Epoch 160/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 2.5634 - val_loss: 26.0848\n",
            "Epoch 161/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 2.3268 - val_loss: 24.3584\n",
            "Epoch 162/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 2.1583 - val_loss: 22.8870\n",
            "Epoch 163/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.9966 - val_loss: 21.6490\n",
            "Epoch 164/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.8832 - val_loss: 20.5547\n",
            "Epoch 165/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.7823 - val_loss: 19.6006\n",
            "Epoch 166/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.6891 - val_loss: 18.7615\n",
            "Epoch 167/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.6264 - val_loss: 17.9838\n",
            "Epoch 168/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.5640 - val_loss: 17.3049\n",
            "Epoch 169/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.5114 - val_loss: 16.6863\n",
            "Epoch 170/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.4648 - val_loss: 16.1538\n",
            "Epoch 171/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.4266 - val_loss: 15.6748\n",
            "Epoch 172/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.3934 - val_loss: 15.2230\n",
            "Epoch 173/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.3611 - val_loss: 14.8309\n",
            "Epoch 174/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.3379 - val_loss: 14.4257\n",
            "Epoch 175/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.3060 - val_loss: 14.0369\n",
            "Epoch 176/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.2912 - val_loss: 13.6512\n",
            "Epoch 177/2000\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.666 - 0s 0us/step - loss: 1.2687 - val_loss: 13.2893\n",
            "Epoch 178/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.2512 - val_loss: 12.8995\n",
            "Epoch 179/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.2286 - val_loss: 12.5755\n",
            "Epoch 180/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.2169 - val_loss: 12.3328\n",
            "Epoch 181/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.2082 - val_loss: 12.0739\n",
            "Epoch 182/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.1992 - val_loss: 11.7837\n",
            "Epoch 183/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1892 - val_loss: 11.5815\n",
            "Epoch 184/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.1824 - val_loss: 11.4605\n",
            "Epoch 185/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1766 - val_loss: 11.3828\n",
            "Epoch 186/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.1723 - val_loss: 11.3048\n",
            "Epoch 187/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1688 - val_loss: 11.2279\n",
            "Epoch 188/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.1648 - val_loss: 11.1100\n",
            "Epoch 189/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1633 - val_loss: 11.0137\n",
            "Epoch 190/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.1577 - val_loss: 10.9348\n",
            "Epoch 191/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1534 - val_loss: 10.8395\n",
            "Epoch 192/2000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 1.1503 - val_loss: 10.7567\n",
            "Epoch 193/2000\n",
            "16/16 [==============================] - 0s 456us/step - loss: 1.1480 - val_loss: 10.7054\n",
            "Epoch 194/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1440 - val_loss: 10.7002\n",
            "Epoch 195/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.1414 - val_loss: 10.6715\n",
            "Epoch 196/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1391 - val_loss: 10.6159\n",
            "Epoch 197/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.1330 - val_loss: 10.4477\n",
            "Epoch 198/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1324 - val_loss: 10.2794\n",
            "Epoch 199/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.1287 - val_loss: 10.1792\n",
            "Epoch 200/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1275 - val_loss: 10.1426\n",
            "Epoch 201/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.1234 - val_loss: 10.1865\n",
            "Epoch 202/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1207 - val_loss: 10.2341\n",
            "Epoch 203/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.1192 - val_loss: 10.3358\n",
            "Epoch 204/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1174 - val_loss: 10.4281\n",
            "Epoch 205/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1188 - val_loss: 10.5305\n",
            "Epoch 206/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1146 - val_loss: 10.5382\n",
            "Epoch 207/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1126 - val_loss: 10.5387\n",
            "Epoch 208/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.1106 - val_loss: 10.5153\n",
            "Epoch 209/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1085 - val_loss: 10.3562\n",
            "Epoch 210/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.1038 - val_loss: 10.3129\n",
            "Epoch 211/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.1010 - val_loss: 10.2823\n",
            "Epoch 212/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0987 - val_loss: 10.2611\n",
            "Epoch 213/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0959 - val_loss: 10.2327\n",
            "Epoch 214/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0945 - val_loss: 10.1884\n",
            "Epoch 215/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0886 - val_loss: 10.2262\n",
            "Epoch 216/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0853 - val_loss: 10.2200\n",
            "Epoch 217/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0819 - val_loss: 10.2109\n",
            "Epoch 218/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0782 - val_loss: 10.1642\n",
            "Epoch 219/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0737 - val_loss: 10.0094\n",
            "Epoch 220/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0719 - val_loss: 9.8970\n",
            "Epoch 221/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0686 - val_loss: 9.8145\n",
            "Epoch 222/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0662 - val_loss: 9.7397\n",
            "Epoch 223/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0637 - val_loss: 9.6634\n",
            "Epoch 224/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0616 - val_loss: 9.5477\n",
            "Epoch 225/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0582 - val_loss: 9.4427\n",
            "Epoch 226/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0581 - val_loss: 9.2638\n",
            "Epoch 227/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0549 - val_loss: 9.2043\n",
            "Epoch 228/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0555 - val_loss: 9.1687\n",
            "Epoch 229/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0525 - val_loss: 9.2091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 230/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0498 - val_loss: 9.2134\n",
            "Epoch 231/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0470 - val_loss: 9.2775\n",
            "Epoch 232/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0438 - val_loss: 9.3944\n",
            "Epoch 233/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0404 - val_loss: 9.4846\n",
            "Epoch 234/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0381 - val_loss: 9.5459\n",
            "Epoch 235/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0334 - val_loss: 9.6191\n",
            "Epoch 236/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0316 - val_loss: 9.6993\n",
            "Epoch 237/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0276 - val_loss: 9.8216\n",
            "Epoch 238/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0242 - val_loss: 9.9547\n",
            "Epoch 239/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0214 - val_loss: 10.0418\n",
            "Epoch 240/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0184 - val_loss: 10.0713\n",
            "Epoch 241/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0154 - val_loss: 9.9358\n",
            "Epoch 242/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0112 - val_loss: 9.9159\n",
            "Epoch 243/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0095 - val_loss: 9.9116\n",
            "Epoch 244/2000\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.082 - 0s 0us/step - loss: 1.0067 - val_loss: 9.9604\n",
            "Epoch 245/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0041 - val_loss: 10.0326\n",
            "Epoch 246/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 1.0035 - val_loss: 10.0647\n",
            "Epoch 247/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 1.0006 - val_loss: 9.9344\n",
            "Epoch 248/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9980 - val_loss: 9.8268\n",
            "Epoch 249/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9990 - val_loss: 9.7312\n",
            "Epoch 250/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9945 - val_loss: 9.7449\n",
            "Epoch 251/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9904 - val_loss: 9.8462\n",
            "Epoch 252/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9871 - val_loss: 9.9557\n",
            "Epoch 253/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9857 - val_loss: 10.0907\n",
            "Epoch 254/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9834 - val_loss: 10.1558\n",
            "Epoch 255/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9812 - val_loss: 10.2594\n",
            "Epoch 256/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9783 - val_loss: 10.2436\n",
            "Epoch 257/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9777 - val_loss: 10.0810\n",
            "Epoch 258/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9733 - val_loss: 10.0270\n",
            "Epoch 259/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9707 - val_loss: 10.0433\n",
            "Epoch 260/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9685 - val_loss: 10.0190\n",
            "Epoch 261/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9692 - val_loss: 9.9575\n",
            "Epoch 262/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9631 - val_loss: 10.0111\n",
            "Epoch 263/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9611 - val_loss: 10.1131\n",
            "Epoch 264/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9585 - val_loss: 10.2166\n",
            "Epoch 265/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9573 - val_loss: 10.2452\n",
            "Epoch 266/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9534 - val_loss: 10.1038\n",
            "Epoch 267/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9513 - val_loss: 9.9851\n",
            "Epoch 268/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9491 - val_loss: 9.8542\n",
            "Epoch 269/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9460 - val_loss: 9.7085\n",
            "Epoch 270/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9454 - val_loss: 9.5962\n",
            "Epoch 271/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9449 - val_loss: 9.5136\n",
            "Epoch 272/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9440 - val_loss: 9.5071\n",
            "Epoch 273/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9414 - val_loss: 9.5941\n",
            "Epoch 274/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9387 - val_loss: 9.6790\n",
            "Epoch 275/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9360 - val_loss: 9.7517\n",
            "Epoch 276/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9360 - val_loss: 9.8447\n",
            "Epoch 277/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9314 - val_loss: 9.9300\n",
            "Epoch 278/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9323 - val_loss: 10.0278\n",
            "Epoch 279/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9277 - val_loss: 9.9560\n",
            "Epoch 280/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9265 - val_loss: 9.9430\n",
            "Epoch 281/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9240 - val_loss: 9.9856\n",
            "Epoch 282/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9254 - val_loss: 10.0774\n",
            "Epoch 283/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9204 - val_loss: 9.9995\n",
            "Epoch 284/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9192 - val_loss: 9.8630\n",
            "Epoch 285/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9167 - val_loss: 9.7730\n",
            "Epoch 286/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9159 - val_loss: 9.7387\n",
            "Epoch 287/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9134 - val_loss: 9.7786\n",
            "Epoch 288/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.9119 - val_loss: 9.8150\n",
            "Epoch 289/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9105 - val_loss: 9.9400\n",
            "Epoch 290/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9102 - val_loss: 10.0026\n",
            "Epoch 291/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9044 - val_loss: 9.8542\n",
            "Epoch 292/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9036 - val_loss: 9.6624\n",
            "Epoch 293/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9021 - val_loss: 9.5606\n",
            "Epoch 294/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.9007 - val_loss: 9.5689\n",
            "Epoch 295/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8974 - val_loss: 9.6160\n",
            "Epoch 296/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8952 - val_loss: 9.6690\n",
            "Epoch 297/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8923 - val_loss: 9.7396\n",
            "Epoch 298/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8895 - val_loss: 9.7808\n",
            "Epoch 299/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8875 - val_loss: 9.8441\n",
            "Epoch 300/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8849 - val_loss: 9.8224\n",
            "Epoch 301/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8825 - val_loss: 9.8484\n",
            "Epoch 302/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8806 - val_loss: 9.8321\n",
            "Epoch 303/2000\n",
            "16/16 [==============================] - 0s 977us/step - loss: 0.8769 - val_loss: 9.8200\n",
            "Epoch 304/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8772 - val_loss: 9.9090\n",
            "Epoch 305/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8733 - val_loss: 9.9360\n",
            "Epoch 306/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8702 - val_loss: 9.9395\n",
            "Epoch 307/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8683 - val_loss: 10.0138\n",
            "Epoch 308/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8654 - val_loss: 10.0968\n",
            "Epoch 309/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 976us/step - loss: 0.8651 - val_loss: 10.1723\n",
            "Epoch 310/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8619 - val_loss: 10.2624\n",
            "Epoch 311/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8597 - val_loss: 10.3463\n",
            "Epoch 312/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8577 - val_loss: 10.4625\n",
            "Epoch 313/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8562 - val_loss: 10.5721\n",
            "Epoch 314/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8540 - val_loss: 10.6183\n",
            "Epoch 315/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8517 - val_loss: 10.6175\n",
            "Epoch 316/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8490 - val_loss: 10.6736\n",
            "Epoch 317/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8466 - val_loss: 10.7373\n",
            "Epoch 318/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8449 - val_loss: 10.7436\n",
            "Epoch 319/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8427 - val_loss: 10.7892\n",
            "Epoch 320/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8406 - val_loss: 10.7965\n",
            "Epoch 321/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8385 - val_loss: 10.7742\n",
            "Epoch 322/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8362 - val_loss: 10.7046\n",
            "Epoch 323/2000\n",
            "16/16 [==============================] - 0s 450us/step - loss: 0.8316 - val_loss: 10.5673\n",
            "Epoch 324/2000\n",
            "16/16 [==============================] - 0s 326us/step - loss: 0.8294 - val_loss: 10.4336\n",
            "Epoch 325/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8303 - val_loss: 10.2211\n",
            "Epoch 326/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8256 - val_loss: 10.1280\n",
            "Epoch 327/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8236 - val_loss: 10.0442\n",
            "Epoch 328/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8205 - val_loss: 10.0538\n",
            "Epoch 329/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8179 - val_loss: 10.0268\n",
            "Epoch 330/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8166 - val_loss: 9.9717\n",
            "Epoch 331/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8156 - val_loss: 9.9057\n",
            "Epoch 332/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8119 - val_loss: 9.8753\n",
            "Epoch 333/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8111 - val_loss: 9.9353\n",
            "Epoch 334/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8085 - val_loss: 9.9894\n",
            "Epoch 335/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8061 - val_loss: 10.0639\n",
            "Epoch 336/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8057 - val_loss: 10.0610\n",
            "Epoch 337/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.8020 - val_loss: 9.8912\n",
            "Epoch 338/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.8002 - val_loss: 9.8308\n",
            "Epoch 339/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7987 - val_loss: 9.7344\n",
            "Epoch 340/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7974 - val_loss: 9.5706\n",
            "Epoch 341/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7955 - val_loss: 9.4692\n",
            "Epoch 342/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7941 - val_loss: 9.4406\n",
            "Epoch 343/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7919 - val_loss: 9.4509\n",
            "Epoch 344/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7906 - val_loss: 9.4797\n",
            "Epoch 345/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7882 - val_loss: 9.5390\n",
            "Epoch 346/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7845 - val_loss: 9.5920\n",
            "Epoch 347/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7844 - val_loss: 9.6728\n",
            "Epoch 348/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7820 - val_loss: 9.5548\n",
            "Epoch 349/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7796 - val_loss: 9.5655\n",
            "Epoch 350/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7782 - val_loss: 9.6785\n",
            "Epoch 351/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7750 - val_loss: 9.7749\n",
            "Epoch 352/2000\n",
            "16/16 [==============================] - 0s 977us/step - loss: 0.7728 - val_loss: 9.8578\n",
            "Epoch 353/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7708 - val_loss: 9.9322\n",
            "Epoch 354/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7702 - val_loss: 10.0298\n",
            "Epoch 355/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7678 - val_loss: 10.0929\n",
            "Epoch 356/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7671 - val_loss: 10.1774\n",
            "Epoch 357/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7660 - val_loss: 10.2281\n",
            "Epoch 358/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7670 - val_loss: 10.1261\n",
            "Epoch 359/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7624 - val_loss: 10.1574\n",
            "Epoch 360/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7612 - val_loss: 10.2438\n",
            "Epoch 361/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7591 - val_loss: 10.3490\n",
            "Epoch 362/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7602 - val_loss: 10.4935\n",
            "Epoch 363/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7591 - val_loss: 10.6190\n",
            "Epoch 364/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7590 - val_loss: 10.6021\n",
            "Epoch 365/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7528 - val_loss: 10.3671\n",
            "Epoch 366/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7507 - val_loss: 10.2709\n",
            "Epoch 367/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7490 - val_loss: 10.2734\n",
            "Epoch 368/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7496 - val_loss: 10.2888\n",
            "Epoch 369/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7443 - val_loss: 10.0511\n",
            "Epoch 370/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7412 - val_loss: 9.8719\n",
            "Epoch 371/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7402 - val_loss: 9.7411\n",
            "Epoch 372/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7419 - val_loss: 9.5515\n",
            "Epoch 373/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7396 - val_loss: 9.5238\n",
            "Epoch 374/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7376 - val_loss: 9.5643\n",
            "Epoch 375/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7363 - val_loss: 9.6750\n",
            "Epoch 376/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7337 - val_loss: 9.6977\n",
            "Epoch 377/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7325 - val_loss: 9.7170\n",
            "Epoch 378/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7298 - val_loss: 9.7732\n",
            "Epoch 379/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7286 - val_loss: 9.8981\n",
            "Epoch 380/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7274 - val_loss: 9.9482\n",
            "Epoch 381/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7279 - val_loss: 9.7440\n",
            "Epoch 382/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7231 - val_loss: 9.6515\n",
            "Epoch 383/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7244 - val_loss: 9.5452\n",
            "Epoch 384/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7204 - val_loss: 9.5863\n",
            "Epoch 385/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7192 - val_loss: 9.5920\n",
            "Epoch 386/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7180 - val_loss: 9.6541\n",
            "Epoch 387/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7161 - val_loss: 9.6822\n",
            "Epoch 388/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 0us/step - loss: 0.7130 - val_loss: 9.7213\n",
            "Epoch 389/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7110 - val_loss: 9.7478\n",
            "Epoch 390/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7095 - val_loss: 9.7926\n",
            "Epoch 391/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7076 - val_loss: 9.8142\n",
            "Epoch 392/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7064 - val_loss: 9.8012\n",
            "Epoch 393/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.7034 - val_loss: 9.8714\n",
            "Epoch 394/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7011 - val_loss: 9.9399\n",
            "Epoch 395/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.7007 - val_loss: 10.0674\n",
            "Epoch 396/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6996 - val_loss: 10.1059\n",
            "Epoch 397/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6978 - val_loss: 9.9956\n",
            "Epoch 398/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6952 - val_loss: 9.9407\n",
            "Epoch 399/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6953 - val_loss: 9.7751\n",
            "Epoch 400/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6923 - val_loss: 9.7481\n",
            "Epoch 401/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6906 - val_loss: 9.7167\n",
            "Epoch 402/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6886 - val_loss: 9.6829\n",
            "Epoch 403/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6871 - val_loss: 9.6872\n",
            "Epoch 404/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6845 - val_loss: 9.7629\n",
            "Epoch 405/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6833 - val_loss: 9.7987\n",
            "Epoch 406/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6813 - val_loss: 9.8348\n",
            "Epoch 407/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6795 - val_loss: 9.8360\n",
            "Epoch 408/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6783 - val_loss: 9.7610\n",
            "Epoch 409/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6760 - val_loss: 9.5073\n",
            "Epoch 410/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6751 - val_loss: 9.3813\n",
            "Epoch 411/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6744 - val_loss: 9.3430\n",
            "Epoch 412/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6734 - val_loss: 9.4115\n",
            "Epoch 413/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6729 - val_loss: 9.5786\n",
            "Epoch 414/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6698 - val_loss: 9.7598\n",
            "Epoch 415/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6661 - val_loss: 9.8911\n",
            "Epoch 416/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6656 - val_loss: 10.0938\n",
            "Epoch 417/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6661 - val_loss: 10.2984\n",
            "Epoch 418/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6646 - val_loss: 10.4336\n",
            "Epoch 419/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6640 - val_loss: 10.4690\n",
            "Epoch 420/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6643 - val_loss: 10.5624\n",
            "Epoch 421/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6634 - val_loss: 10.5303\n",
            "Epoch 422/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6598 - val_loss: 10.5073\n",
            "Epoch 423/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6590 - val_loss: 10.4638\n",
            "Epoch 424/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6562 - val_loss: 10.4591\n",
            "Epoch 425/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6551 - val_loss: 10.3939\n",
            "Epoch 426/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6511 - val_loss: 10.1898\n",
            "Epoch 427/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6505 - val_loss: 10.0187\n",
            "Epoch 428/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6469 - val_loss: 10.0028\n",
            "Epoch 429/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6452 - val_loss: 10.0358\n",
            "Epoch 430/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6437 - val_loss: 10.0580\n",
            "Epoch 431/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6460 - val_loss: 10.1870\n",
            "Epoch 432/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6433 - val_loss: 10.2062\n",
            "Epoch 433/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6392 - val_loss: 10.0964\n",
            "Epoch 434/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6376 - val_loss: 10.0350\n",
            "Epoch 435/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6370 - val_loss: 10.0082\n",
            "Epoch 436/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6354 - val_loss: 9.7286\n",
            "Epoch 437/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6311 - val_loss: 9.5968\n",
            "Epoch 438/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6290 - val_loss: 9.4127\n",
            "Epoch 439/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6302 - val_loss: 9.3106\n",
            "Epoch 440/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6256 - val_loss: 9.0220\n",
            "Epoch 441/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6313 - val_loss: 8.7532\n",
            "Epoch 442/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6336 - val_loss: 8.6989\n",
            "Epoch 443/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6305 - val_loss: 8.8164\n",
            "Epoch 444/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6261 - val_loss: 9.0083\n",
            "Epoch 445/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6228 - val_loss: 9.1574\n",
            "Epoch 446/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6214 - val_loss: 9.3630\n",
            "Epoch 447/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6170 - val_loss: 9.5059\n",
            "Epoch 448/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6184 - val_loss: 9.6580\n",
            "Epoch 449/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6159 - val_loss: 9.6214\n",
            "Epoch 450/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6139 - val_loss: 9.6560\n",
            "Epoch 451/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6125 - val_loss: 9.7146\n",
            "Epoch 452/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6110 - val_loss: 9.8178\n",
            "Epoch 453/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6099 - val_loss: 9.9229\n",
            "Epoch 454/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6087 - val_loss: 9.9610\n",
            "Epoch 455/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6092 - val_loss: 9.9327\n",
            "Epoch 456/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6021 - val_loss: 9.6518\n",
            "Epoch 457/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6018 - val_loss: 9.4194\n",
            "Epoch 458/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5988 - val_loss: 9.2003\n",
            "Epoch 459/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6094 - val_loss: 8.7736\n",
            "Epoch 460/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.6036 - val_loss: 8.5630\n",
            "Epoch 461/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.6043 - val_loss: 8.5288\n",
            "Epoch 462/2000\n",
            "16/16 [==============================] - 0s 659us/step - loss: 0.6029 - val_loss: 8.6175\n",
            "Epoch 463/2000\n",
            "16/16 [==============================] - 0s 306us/step - loss: 0.5989 - val_loss: 8.7636\n",
            "Epoch 464/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5953 - val_loss: 8.9897\n",
            "Epoch 465/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5912 - val_loss: 9.1680\n",
            "Epoch 466/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5883 - val_loss: 9.3355\n",
            "Epoch 467/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 0us/step - loss: 0.5861 - val_loss: 9.5228\n",
            "Epoch 468/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5876 - val_loss: 9.6114\n",
            "Epoch 469/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5885 - val_loss: 9.4079\n",
            "Epoch 470/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5841 - val_loss: 9.3630\n",
            "Epoch 471/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5823 - val_loss: 9.3702\n",
            "Epoch 472/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5802 - val_loss: 9.4312\n",
            "Epoch 473/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5794 - val_loss: 9.4915\n",
            "Epoch 474/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5776 - val_loss: 9.5284\n",
            "Epoch 475/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5770 - val_loss: 9.6335\n",
            "Epoch 476/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5748 - val_loss: 9.7568\n",
            "Epoch 477/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5746 - val_loss: 9.8219\n",
            "Epoch 478/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5745 - val_loss: 9.8462\n",
            "Epoch 479/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5704 - val_loss: 9.6185\n",
            "Epoch 480/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5704 - val_loss: 9.4876\n",
            "Epoch 481/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5680 - val_loss: 9.3799\n",
            "Epoch 482/2000\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.427 - 0s 0us/step - loss: 0.5677 - val_loss: 9.2009\n",
            "Epoch 483/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5663 - val_loss: 9.1829\n",
            "Epoch 484/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5655 - val_loss: 9.1142\n",
            "Epoch 485/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5637 - val_loss: 9.1448\n",
            "Epoch 486/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5622 - val_loss: 9.2556\n",
            "Epoch 487/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5616 - val_loss: 9.2872\n",
            "Epoch 488/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5580 - val_loss: 9.1226\n",
            "Epoch 489/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5587 - val_loss: 9.1118\n",
            "Epoch 490/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5569 - val_loss: 9.1576\n",
            "Epoch 491/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5560 - val_loss: 9.1647\n",
            "Epoch 492/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5551 - val_loss: 9.0804\n",
            "Epoch 493/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5544 - val_loss: 8.9617\n",
            "Epoch 494/2000\n",
            "16/16 [==============================] - 0s 975us/step - loss: 0.5517 - val_loss: 9.0203\n",
            "Epoch 495/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5508 - val_loss: 9.2000\n",
            "Epoch 496/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5523 - val_loss: 9.4132\n",
            "Epoch 497/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5475 - val_loss: 9.4643\n",
            "Epoch 498/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5457 - val_loss: 9.4011\n",
            "Epoch 499/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5447 - val_loss: 9.3627\n",
            "Epoch 500/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5430 - val_loss: 9.4495\n",
            "Epoch 501/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5410 - val_loss: 9.5296\n",
            "Epoch 502/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5403 - val_loss: 9.6556\n",
            "Epoch 503/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5418 - val_loss: 9.8781\n",
            "Epoch 504/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5432 - val_loss: 9.9858\n",
            "Epoch 505/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5409 - val_loss: 9.6535\n",
            "Epoch 506/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5339 - val_loss: 9.3157\n",
            "Epoch 507/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5342 - val_loss: 9.1083\n",
            "Epoch 508/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5341 - val_loss: 8.9972\n",
            "Epoch 509/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5316 - val_loss: 9.0407\n",
            "Epoch 510/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5298 - val_loss: 9.0672\n",
            "Epoch 511/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5284 - val_loss: 9.1027\n",
            "Epoch 512/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5269 - val_loss: 9.1511\n",
            "Epoch 513/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5291 - val_loss: 9.3387\n",
            "Epoch 514/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5236 - val_loss: 9.4757\n",
            "Epoch 515/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5233 - val_loss: 9.5556\n",
            "Epoch 516/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5226 - val_loss: 9.6969\n",
            "Epoch 517/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5220 - val_loss: 9.7571\n",
            "Epoch 518/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5222 - val_loss: 9.8185\n",
            "Epoch 519/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5215 - val_loss: 9.8933\n",
            "Epoch 520/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5214 - val_loss: 9.9730\n",
            "Epoch 521/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5208 - val_loss: 9.9966\n",
            "Epoch 522/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5196 - val_loss: 9.9594\n",
            "Epoch 523/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5184 - val_loss: 9.8356\n",
            "Epoch 524/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5143 - val_loss: 9.7201\n",
            "Epoch 525/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5137 - val_loss: 9.5603\n",
            "Epoch 526/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5077 - val_loss: 9.3634\n",
            "Epoch 527/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5077 - val_loss: 8.9449\n",
            "Epoch 528/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5067 - val_loss: 8.7803\n",
            "Epoch 529/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5052 - val_loss: 8.7848\n",
            "Epoch 530/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.5031 - val_loss: 8.8600\n",
            "Epoch 531/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5015 - val_loss: 9.0573\n",
            "Epoch 532/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4995 - val_loss: 9.2593\n",
            "Epoch 533/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4979 - val_loss: 9.4927\n",
            "Epoch 534/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4975 - val_loss: 9.7254\n",
            "Epoch 535/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4993 - val_loss: 9.9391\n",
            "Epoch 536/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5002 - val_loss: 10.0210\n",
            "Epoch 537/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.5005 - val_loss: 10.0446\n",
            "Epoch 538/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4995 - val_loss: 10.0314\n",
            "Epoch 539/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4986 - val_loss: 9.9784\n",
            "Epoch 540/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4973 - val_loss: 9.9892\n",
            "Epoch 541/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4953 - val_loss: 9.8317\n",
            "Epoch 542/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4932 - val_loss: 9.6798\n",
            "Epoch 543/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4898 - val_loss: 9.5993\n",
            "Epoch 544/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4885 - val_loss: 9.6100\n",
            "Epoch 545/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4886 - val_loss: 9.6100\n",
            "Epoch 546/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 0us/step - loss: 0.4862 - val_loss: 9.5595\n",
            "Epoch 547/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4858 - val_loss: 9.6072\n",
            "Epoch 548/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4846 - val_loss: 9.6572\n",
            "Epoch 549/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4835 - val_loss: 9.6074\n",
            "Epoch 550/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4816 - val_loss: 9.5412\n",
            "Epoch 551/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4822 - val_loss: 9.4306\n",
            "Epoch 552/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4776 - val_loss: 9.3979\n",
            "Epoch 553/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4757 - val_loss: 9.1810\n",
            "Epoch 554/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4730 - val_loss: 8.9927\n",
            "Epoch 555/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4711 - val_loss: 8.8932\n",
            "Epoch 556/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4703 - val_loss: 8.8785\n",
            "Epoch 557/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4691 - val_loss: 8.8324\n",
            "Epoch 558/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4682 - val_loss: 8.8071\n",
            "Epoch 559/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4664 - val_loss: 8.8277\n",
            "Epoch 560/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4635 - val_loss: 8.9478\n",
            "Epoch 561/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4619 - val_loss: 9.0787\n",
            "Epoch 562/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4609 - val_loss: 9.2091\n",
            "Epoch 563/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4608 - val_loss: 9.2818\n",
            "Epoch 564/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4590 - val_loss: 9.4129\n",
            "Epoch 565/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4612 - val_loss: 9.6113\n",
            "Epoch 566/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4609 - val_loss: 9.7656\n",
            "Epoch 567/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4630 - val_loss: 9.9182\n",
            "Epoch 568/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4635 - val_loss: 10.0152\n",
            "Epoch 569/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4625 - val_loss: 9.9450\n",
            "Epoch 570/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4610 - val_loss: 9.8946\n",
            "Epoch 571/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4583 - val_loss: 9.7076\n",
            "Epoch 572/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4558 - val_loss: 9.6727\n",
            "Epoch 573/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4536 - val_loss: 9.5909\n",
            "Epoch 574/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4509 - val_loss: 9.4763\n",
            "Epoch 575/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4496 - val_loss: 9.4113\n",
            "Epoch 576/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4483 - val_loss: 9.3849\n",
            "Epoch 577/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4467 - val_loss: 9.3523\n",
            "Epoch 578/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4456 - val_loss: 9.2878\n",
            "Epoch 579/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4445 - val_loss: 9.2011\n",
            "Epoch 580/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4428 - val_loss: 9.1612\n",
            "Epoch 581/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4404 - val_loss: 9.1401\n",
            "Epoch 582/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4387 - val_loss: 9.0895\n",
            "Epoch 583/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4368 - val_loss: 9.0141\n",
            "Epoch 584/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4356 - val_loss: 8.9373\n",
            "Epoch 585/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4364 - val_loss: 8.8184\n",
            "Epoch 586/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4323 - val_loss: 8.8868\n",
            "Epoch 587/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4318 - val_loss: 8.9910\n",
            "Epoch 588/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4291 - val_loss: 8.9095\n",
            "Epoch 589/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4279 - val_loss: 8.8795\n",
            "Epoch 590/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4269 - val_loss: 8.9112\n",
            "Epoch 591/2000\n",
            "16/16 [==============================] - 0s 978us/step - loss: 0.4262 - val_loss: 9.0179\n",
            "Epoch 592/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4250 - val_loss: 9.0498\n",
            "Epoch 593/2000\n",
            "16/16 [==============================] - 0s 975us/step - loss: 0.4238 - val_loss: 9.0526\n",
            "Epoch 594/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4249 - val_loss: 8.9718\n",
            "Epoch 595/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4212 - val_loss: 9.0018\n",
            "Epoch 596/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4218 - val_loss: 9.1669\n",
            "Epoch 597/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4222 - val_loss: 9.3230\n",
            "Epoch 598/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4211 - val_loss: 9.3183\n",
            "Epoch 599/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4185 - val_loss: 9.4160\n",
            "Epoch 600/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4240 - val_loss: 9.5047\n",
            "Epoch 601/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4167 - val_loss: 9.3060\n",
            "Epoch 602/2000\n",
            "16/16 [==============================] - 0s 856us/step - loss: 0.4193 - val_loss: 9.0335\n",
            "Epoch 603/2000\n",
            "16/16 [==============================] - 0s 464us/step - loss: 0.4120 - val_loss: 8.8582\n",
            "Epoch 604/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4093 - val_loss: 8.6601\n",
            "Epoch 605/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4083 - val_loss: 8.4516\n",
            "Epoch 606/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4075 - val_loss: 8.3001\n",
            "Epoch 607/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4078 - val_loss: 8.2563\n",
            "Epoch 608/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4079 - val_loss: 8.1797\n",
            "Epoch 609/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4108 - val_loss: 7.8731\n",
            "Epoch 610/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4107 - val_loss: 7.8275\n",
            "Epoch 611/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4089 - val_loss: 7.9746\n",
            "Epoch 612/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4047 - val_loss: 8.1189\n",
            "Epoch 613/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.4026 - val_loss: 8.3306\n",
            "Epoch 614/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.4014 - val_loss: 8.5386\n",
            "Epoch 615/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3977 - val_loss: 8.6741\n",
            "Epoch 616/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3964 - val_loss: 8.8733\n",
            "Epoch 617/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3989 - val_loss: 9.0802\n",
            "Epoch 618/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3981 - val_loss: 9.0833\n",
            "Epoch 619/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3975 - val_loss: 9.0311\n",
            "Epoch 620/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3963 - val_loss: 8.9958\n",
            "Epoch 621/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3948 - val_loss: 8.9221\n",
            "Epoch 622/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3928 - val_loss: 8.7471\n",
            "Epoch 623/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3918 - val_loss: 8.6093\n",
            "Epoch 624/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3924 - val_loss: 8.4642\n",
            "Epoch 625/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3888 - val_loss: 8.4438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 626/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3879 - val_loss: 8.4715\n",
            "Epoch 627/2000\n",
            "16/16 [==============================] - 0s 977us/step - loss: 0.3867 - val_loss: 8.5210\n",
            "Epoch 628/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3884 - val_loss: 8.6876\n",
            "Epoch 629/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3847 - val_loss: 8.8050\n",
            "Epoch 630/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3859 - val_loss: 8.9391\n",
            "Epoch 631/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3849 - val_loss: 9.0257\n",
            "Epoch 632/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3852 - val_loss: 8.9897\n",
            "Epoch 633/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3831 - val_loss: 8.8399\n",
            "Epoch 634/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3779 - val_loss: 8.5117\n",
            "Epoch 635/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3802 - val_loss: 8.1968\n",
            "Epoch 636/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3771 - val_loss: 8.0891\n",
            "Epoch 637/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3789 - val_loss: 7.9982\n",
            "Epoch 638/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3759 - val_loss: 8.1001\n",
            "Epoch 639/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3767 - val_loss: 8.2572\n",
            "Epoch 640/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3736 - val_loss: 8.3712\n",
            "Epoch 641/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3752 - val_loss: 8.5643\n",
            "Epoch 642/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3716 - val_loss: 8.5942\n",
            "Epoch 643/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3721 - val_loss: 8.5531\n",
            "Epoch 644/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3697 - val_loss: 8.5466\n",
            "Epoch 645/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3686 - val_loss: 8.5985\n",
            "Epoch 646/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3680 - val_loss: 8.5915\n",
            "Epoch 647/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3670 - val_loss: 8.5131\n",
            "Epoch 648/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3659 - val_loss: 8.3007\n",
            "Epoch 649/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3641 - val_loss: 8.2797\n",
            "Epoch 650/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3629 - val_loss: 8.3091\n",
            "Epoch 651/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3624 - val_loss: 8.4307\n",
            "Epoch 652/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3616 - val_loss: 8.4862\n",
            "Epoch 653/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3603 - val_loss: 8.4915\n",
            "Epoch 654/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3593 - val_loss: 8.6254\n",
            "Epoch 655/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3603 - val_loss: 8.8014\n",
            "Epoch 656/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3607 - val_loss: 8.8982\n",
            "Epoch 657/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3603 - val_loss: 8.9014\n",
            "Epoch 658/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3593 - val_loss: 8.9018\n",
            "Epoch 659/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3592 - val_loss: 8.9214\n",
            "Epoch 660/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3590 - val_loss: 8.9235\n",
            "Epoch 661/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3574 - val_loss: 8.8755\n",
            "Epoch 662/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3564 - val_loss: 8.8187\n",
            "Epoch 663/2000\n",
            "16/16 [==============================] - 0s 978us/step - loss: 0.3543 - val_loss: 8.7272\n",
            "Epoch 664/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3526 - val_loss: 8.6518\n",
            "Epoch 665/2000\n",
            "16/16 [==============================] - 0s 975us/step - loss: 0.3502 - val_loss: 8.4458\n",
            "Epoch 666/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3476 - val_loss: 8.3392\n",
            "Epoch 667/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3461 - val_loss: 8.2222\n",
            "Epoch 668/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3466 - val_loss: 8.0379\n",
            "Epoch 669/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3439 - val_loss: 8.0075\n",
            "Epoch 670/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3437 - val_loss: 7.9213\n",
            "Epoch 671/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3405 - val_loss: 7.5945\n",
            "Epoch 672/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3446 - val_loss: 7.3805\n",
            "Epoch 673/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3453 - val_loss: 7.2959\n",
            "Epoch 674/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3443 - val_loss: 7.2863\n",
            "Epoch 675/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3425 - val_loss: 7.3858\n",
            "Epoch 676/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3400 - val_loss: 7.3752\n",
            "Epoch 677/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3395 - val_loss: 7.4001\n",
            "Epoch 678/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3392 - val_loss: 7.2774\n",
            "Epoch 679/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3381 - val_loss: 7.2317\n",
            "Epoch 680/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3377 - val_loss: 7.0256\n",
            "Epoch 681/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3496 - val_loss: 6.6617\n",
            "Epoch 682/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3470 - val_loss: 6.6075\n",
            "Epoch 683/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3464 - val_loss: 6.6742\n",
            "Epoch 684/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3411 - val_loss: 6.8469\n",
            "Epoch 685/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3357 - val_loss: 7.1395\n",
            "Epoch 686/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3315 - val_loss: 7.4904\n",
            "Epoch 687/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3270 - val_loss: 7.7852\n",
            "Epoch 688/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3262 - val_loss: 8.1226\n",
            "Epoch 689/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3222 - val_loss: 8.3866\n",
            "Epoch 690/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3273 - val_loss: 8.6239\n",
            "Epoch 691/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3297 - val_loss: 8.6680\n",
            "Epoch 692/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3259 - val_loss: 8.4534\n",
            "Epoch 693/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3230 - val_loss: 8.2397\n",
            "Epoch 694/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3209 - val_loss: 8.1193\n",
            "Epoch 695/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3205 - val_loss: 8.0627\n",
            "Epoch 696/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3214 - val_loss: 7.7368\n",
            "Epoch 697/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3188 - val_loss: 7.5153\n",
            "Epoch 698/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3168 - val_loss: 7.4943\n",
            "Epoch 699/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3153 - val_loss: 7.5340\n",
            "Epoch 700/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3150 - val_loss: 7.5343\n",
            "Epoch 701/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3131 - val_loss: 7.6494\n",
            "Epoch 702/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3153 - val_loss: 7.8874\n",
            "Epoch 703/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3117 - val_loss: 8.0675\n",
            "Epoch 704/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3119 - val_loss: 8.2360\n",
            "Epoch 705/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 0us/step - loss: 0.3125 - val_loss: 8.2648\n",
            "Epoch 706/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3135 - val_loss: 8.2188\n",
            "Epoch 707/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3105 - val_loss: 8.2930\n",
            "Epoch 708/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3102 - val_loss: 8.2837\n",
            "Epoch 709/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3090 - val_loss: 8.2275\n",
            "Epoch 710/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3093 - val_loss: 8.1850\n",
            "Epoch 711/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3050 - val_loss: 7.8808\n",
            "Epoch 712/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3010 - val_loss: 7.6071\n",
            "Epoch 713/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3091 - val_loss: 7.0773\n",
            "Epoch 714/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3066 - val_loss: 6.9067\n",
            "Epoch 715/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3034 - val_loss: 6.9950\n",
            "Epoch 716/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.3009 - val_loss: 7.0730\n",
            "Epoch 717/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.3022 - val_loss: 7.1890\n",
            "Epoch 718/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2995 - val_loss: 7.0658\n",
            "Epoch 719/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2992 - val_loss: 7.0371\n",
            "Epoch 720/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2974 - val_loss: 7.2236\n",
            "Epoch 721/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2961 - val_loss: 7.4403\n",
            "Epoch 722/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2916 - val_loss: 7.6307\n",
            "Epoch 723/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2918 - val_loss: 7.7436\n",
            "Epoch 724/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2922 - val_loss: 7.9337\n",
            "Epoch 725/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2950 - val_loss: 8.1318\n",
            "Epoch 726/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2956 - val_loss: 8.2021\n",
            "Epoch 727/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2989 - val_loss: 8.1620\n",
            "Epoch 728/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2976 - val_loss: 7.6157\n",
            "Epoch 729/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2850 - val_loss: 7.3383\n",
            "Epoch 730/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2859 - val_loss: 7.2358\n",
            "Epoch 731/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2854 - val_loss: 7.2489\n",
            "Epoch 732/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2850 - val_loss: 7.3882\n",
            "Epoch 733/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2832 - val_loss: 7.4289\n",
            "Epoch 734/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2823 - val_loss: 7.4471\n",
            "Epoch 735/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2837 - val_loss: 7.4195\n",
            "Epoch 736/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2799 - val_loss: 7.5479\n",
            "Epoch 737/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2819 - val_loss: 7.7389\n",
            "Epoch 738/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2820 - val_loss: 7.8519\n",
            "Epoch 739/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2808 - val_loss: 7.9260\n",
            "Epoch 740/2000\n",
            "16/16 [==============================] - 0s 103us/step - loss: 0.2815 - val_loss: 7.9251\n",
            "Epoch 741/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2813 - val_loss: 7.9409\n",
            "Epoch 742/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2786 - val_loss: 7.6757\n",
            "Epoch 743/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2739 - val_loss: 7.4551\n",
            "Epoch 744/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2727 - val_loss: 7.2361\n",
            "Epoch 745/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2723 - val_loss: 7.2040\n",
            "Epoch 746/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2732 - val_loss: 7.3122\n",
            "Epoch 747/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2713 - val_loss: 7.2804\n",
            "Epoch 748/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2697 - val_loss: 7.3014\n",
            "Epoch 749/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2703 - val_loss: 7.4278\n",
            "Epoch 750/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2687 - val_loss: 7.4267\n",
            "Epoch 751/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2684 - val_loss: 7.4518\n",
            "Epoch 752/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2671 - val_loss: 7.5565\n",
            "Epoch 753/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2665 - val_loss: 7.6393\n",
            "Epoch 754/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2669 - val_loss: 7.7257\n",
            "Epoch 755/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2688 - val_loss: 7.6675\n",
            "Epoch 756/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2651 - val_loss: 7.3501\n",
            "Epoch 757/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2617 - val_loss: 7.1573\n",
            "Epoch 758/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2615 - val_loss: 6.8525\n",
            "Epoch 759/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2648 - val_loss: 6.6932\n",
            "Epoch 760/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2611 - val_loss: 6.7773\n",
            "Epoch 761/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2604 - val_loss: 6.7511\n",
            "Epoch 762/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2647 - val_loss: 6.4395\n",
            "Epoch 763/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2622 - val_loss: 6.3043\n",
            "Epoch 764/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2630 - val_loss: 6.2000\n",
            "Epoch 765/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2617 - val_loss: 6.3185\n",
            "Epoch 766/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2631 - val_loss: 6.5186\n",
            "Epoch 767/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2608 - val_loss: 6.3741\n",
            "Epoch 768/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2567 - val_loss: 6.5341\n",
            "Epoch 769/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2520 - val_loss: 6.7039\n",
            "Epoch 770/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2537 - val_loss: 6.9093\n",
            "Epoch 771/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2503 - val_loss: 7.0486\n",
            "Epoch 772/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2509 - val_loss: 7.1508\n",
            "Epoch 773/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2493 - val_loss: 7.1932\n",
            "Epoch 774/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2530 - val_loss: 7.2441\n",
            "Epoch 775/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2475 - val_loss: 6.8653\n",
            "Epoch 776/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2467 - val_loss: 6.5985\n",
            "Epoch 777/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2466 - val_loss: 6.4831\n",
            "Epoch 778/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2469 - val_loss: 6.5388\n",
            "Epoch 779/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2446 - val_loss: 6.4427\n",
            "Epoch 780/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2456 - val_loss: 6.4063\n",
            "Epoch 781/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2441 - val_loss: 6.4211\n",
            "Epoch 782/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2431 - val_loss: 6.4753\n",
            "Epoch 783/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2420 - val_loss: 6.5231\n",
            "Epoch 784/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2403 - val_loss: 6.5865\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 785/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2400 - val_loss: 6.7721\n",
            "Epoch 786/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2396 - val_loss: 6.9569\n",
            "Epoch 787/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2372 - val_loss: 6.9786\n",
            "Epoch 788/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2364 - val_loss: 6.9634\n",
            "Epoch 789/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2360 - val_loss: 6.9885\n",
            "Epoch 790/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2349 - val_loss: 6.9452\n",
            "Epoch 791/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2338 - val_loss: 6.8890\n",
            "Epoch 792/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2328 - val_loss: 6.9077\n",
            "Epoch 793/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2330 - val_loss: 6.9982\n",
            "Epoch 794/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2317 - val_loss: 7.0769\n",
            "Epoch 795/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2323 - val_loss: 7.2125\n",
            "Epoch 796/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2318 - val_loss: 7.3292\n",
            "Epoch 797/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2380 - val_loss: 7.4902\n",
            "Epoch 798/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2354 - val_loss: 7.4322\n",
            "Epoch 799/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2343 - val_loss: 7.2119\n",
            "Epoch 800/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2285 - val_loss: 6.9536\n",
            "Epoch 801/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2249 - val_loss: 6.4152\n",
            "Epoch 802/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2254 - val_loss: 6.1455\n",
            "Epoch 803/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2267 - val_loss: 6.0343\n",
            "Epoch 804/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2314 - val_loss: 5.7054\n",
            "Epoch 805/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2372 - val_loss: 5.5597\n",
            "Epoch 806/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2345 - val_loss: 5.6547\n",
            "Epoch 807/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2311 - val_loss: 5.6857\n",
            "Epoch 808/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2260 - val_loss: 5.9374\n",
            "Epoch 809/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2204 - val_loss: 6.2814\n",
            "Epoch 810/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2164 - val_loss: 6.5482\n",
            "Epoch 811/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2185 - val_loss: 6.9020\n",
            "Epoch 812/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2265 - val_loss: 7.1194\n",
            "Epoch 813/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2200 - val_loss: 6.8018\n",
            "Epoch 814/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2178 - val_loss: 6.5805\n",
            "Epoch 815/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2149 - val_loss: 6.4489\n",
            "Epoch 816/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2139 - val_loss: 6.4026\n",
            "Epoch 817/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2139 - val_loss: 6.3432\n",
            "Epoch 818/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2110 - val_loss: 5.9595\n",
            "Epoch 819/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2190 - val_loss: 5.5841\n",
            "Epoch 820/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2205 - val_loss: 5.4752\n",
            "Epoch 821/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2192 - val_loss: 5.5932\n",
            "Epoch 822/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2156 - val_loss: 5.7831\n",
            "Epoch 823/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2117 - val_loss: 5.9838\n",
            "Epoch 824/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2066 - val_loss: 6.2205\n",
            "Epoch 825/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2110 - val_loss: 6.5451\n",
            "Epoch 826/2000\n",
            "16/16 [==============================] - 0s 978us/step - loss: 0.2082 - val_loss: 6.6894\n",
            "Epoch 827/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2072 - val_loss: 6.7136\n",
            "Epoch 828/2000\n",
            "16/16 [==============================] - 0s 975us/step - loss: 0.2078 - val_loss: 6.7291\n",
            "Epoch 829/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2067 - val_loss: 6.7167\n",
            "Epoch 830/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2056 - val_loss: 6.6939\n",
            "Epoch 831/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2048 - val_loss: 6.6558\n",
            "Epoch 832/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2048 - val_loss: 6.5212\n",
            "Epoch 833/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2021 - val_loss: 6.3749\n",
            "Epoch 834/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2023 - val_loss: 6.2862\n",
            "Epoch 835/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.2032 - val_loss: 6.3079\n",
            "Epoch 836/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.2008 - val_loss: 5.9771\n",
            "Epoch 837/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1994 - val_loss: 5.9304\n",
            "Epoch 838/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1988 - val_loss: 6.0580\n",
            "Epoch 839/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1986 - val_loss: 6.2622\n",
            "Epoch 840/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1961 - val_loss: 6.3537\n",
            "Epoch 841/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1974 - val_loss: 6.4727\n",
            "Epoch 842/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1961 - val_loss: 6.4877\n",
            "Epoch 843/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1958 - val_loss: 6.5385\n",
            "Epoch 844/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1961 - val_loss: 6.5889\n",
            "Epoch 845/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1967 - val_loss: 6.5513\n",
            "Epoch 846/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1943 - val_loss: 6.5894\n",
            "Epoch 847/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1945 - val_loss: 6.6124\n",
            "Epoch 848/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1954 - val_loss: 6.6792\n",
            "Epoch 849/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1944 - val_loss: 6.5283\n",
            "Epoch 850/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1915 - val_loss: 6.0239\n",
            "Epoch 851/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1937 - val_loss: 5.6993\n",
            "Epoch 852/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1905 - val_loss: 5.6424\n",
            "Epoch 853/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1900 - val_loss: 5.6853\n",
            "Epoch 854/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1892 - val_loss: 5.8707\n",
            "Epoch 855/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1860 - val_loss: 5.9949\n",
            "Epoch 856/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1838 - val_loss: 6.2017\n",
            "Epoch 857/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1870 - val_loss: 6.5083\n",
            "Epoch 858/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1898 - val_loss: 6.7842\n",
            "Epoch 859/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1924 - val_loss: 6.9439\n",
            "Epoch 860/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1948 - val_loss: 6.9428\n",
            "Epoch 861/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1937 - val_loss: 6.8387\n",
            "Epoch 862/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1888 - val_loss: 6.6405\n",
            "Epoch 863/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1863 - val_loss: 6.3579\n",
            "Epoch 864/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 976us/step - loss: 0.1817 - val_loss: 6.0972\n",
            "Epoch 865/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1798 - val_loss: 5.9877\n",
            "Epoch 866/2000\n",
            "16/16 [==============================] - 0s 977us/step - loss: 0.1788 - val_loss: 5.7963\n",
            "Epoch 867/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1783 - val_loss: 5.3722\n",
            "Epoch 868/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1816 - val_loss: 5.1708\n",
            "Epoch 869/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1881 - val_loss: 5.0935\n",
            "Epoch 870/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1849 - val_loss: 5.2851\n",
            "Epoch 871/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1795 - val_loss: 5.4699\n",
            "Epoch 872/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1784 - val_loss: 5.6828\n",
            "Epoch 873/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1752 - val_loss: 5.7347\n",
            "Epoch 874/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1748 - val_loss: 5.6515\n",
            "Epoch 875/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1744 - val_loss: 5.6286\n",
            "Epoch 876/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1742 - val_loss: 5.6187\n",
            "Epoch 877/2000\n",
            "16/16 [==============================] - 0s 776us/step - loss: 0.1726 - val_loss: 5.7320\n",
            "Epoch 878/2000\n",
            "16/16 [==============================] - 0s 415us/step - loss: 0.1716 - val_loss: 5.7960\n",
            "Epoch 879/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1720 - val_loss: 5.8633\n",
            "Epoch 880/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1728 - val_loss: 5.6951\n",
            "Epoch 881/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1702 - val_loss: 5.6000\n",
            "Epoch 882/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1696 - val_loss: 5.4471\n",
            "Epoch 883/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1703 - val_loss: 5.4933\n",
            "Epoch 884/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1689 - val_loss: 5.6066\n",
            "Epoch 885/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1680 - val_loss: 5.7099\n",
            "Epoch 886/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1661 - val_loss: 5.8564\n",
            "Epoch 887/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1663 - val_loss: 6.0211\n",
            "Epoch 888/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1668 - val_loss: 6.1965\n",
            "Epoch 889/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1706 - val_loss: 6.2630\n",
            "Epoch 890/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1664 - val_loss: 6.0278\n",
            "Epoch 891/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1680 - val_loss: 5.7443\n",
            "Epoch 892/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1635 - val_loss: 5.6986\n",
            "Epoch 893/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1629 - val_loss: 5.6990\n",
            "Epoch 894/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1633 - val_loss: 5.7145\n",
            "Epoch 895/2000\n",
            "16/16 [==============================] - 0s 977us/step - loss: 0.1621 - val_loss: 5.3532\n",
            "Epoch 896/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1639 - val_loss: 5.0225\n",
            "Epoch 897/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1676 - val_loss: 4.9054\n",
            "Epoch 898/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1655 - val_loss: 5.0279\n",
            "Epoch 899/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1634 - val_loss: 5.2755\n",
            "Epoch 900/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1596 - val_loss: 5.5421\n",
            "Epoch 901/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1614 - val_loss: 5.8623\n",
            "Epoch 902/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1610 - val_loss: 6.0540\n",
            "Epoch 903/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1625 - val_loss: 6.1427\n",
            "Epoch 904/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1612 - val_loss: 6.1212\n",
            "Epoch 905/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1607 - val_loss: 6.1620\n",
            "Epoch 906/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1615 - val_loss: 6.1938\n",
            "Epoch 907/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1610 - val_loss: 6.1443\n",
            "Epoch 908/2000\n",
            "16/16 [==============================] - 0s 978us/step - loss: 0.1605 - val_loss: 6.0909\n",
            "Epoch 909/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1549 - val_loss: 5.8042\n",
            "Epoch 910/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1550 - val_loss: 5.4619\n",
            "Epoch 911/2000\n",
            "16/16 [==============================] - 0s 975us/step - loss: 0.1516 - val_loss: 5.2932\n",
            "Epoch 912/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1518 - val_loss: 5.1491\n",
            "Epoch 913/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1535 - val_loss: 4.9371\n",
            "Epoch 914/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1544 - val_loss: 4.8052\n",
            "Epoch 915/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1590 - val_loss: 4.5152\n",
            "Epoch 916/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1618 - val_loss: 4.4584\n",
            "Epoch 917/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1653 - val_loss: 4.3824\n",
            "Epoch 918/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1648 - val_loss: 4.4628\n",
            "Epoch 919/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1556 - val_loss: 4.7728\n",
            "Epoch 920/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1544 - val_loss: 5.1959\n",
            "Epoch 921/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1471 - val_loss: 5.5151\n",
            "Epoch 922/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1457 - val_loss: 5.7804\n",
            "Epoch 923/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1482 - val_loss: 5.8968\n",
            "Epoch 924/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1493 - val_loss: 5.9675\n",
            "Epoch 925/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1516 - val_loss: 6.0313\n",
            "Epoch 926/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1535 - val_loss: 5.8590\n",
            "Epoch 927/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1479 - val_loss: 5.7977\n",
            "Epoch 928/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1465 - val_loss: 5.7338\n",
            "Epoch 929/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1450 - val_loss: 5.6172\n",
            "Epoch 930/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1438 - val_loss: 5.4691\n",
            "Epoch 931/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1416 - val_loss: 5.3245\n",
            "Epoch 932/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1417 - val_loss: 5.0233\n",
            "Epoch 933/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1407 - val_loss: 4.8807\n",
            "Epoch 934/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1423 - val_loss: 4.7680\n",
            "Epoch 935/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1464 - val_loss: 4.6394\n",
            "Epoch 936/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1438 - val_loss: 4.7829\n",
            "Epoch 937/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1443 - val_loss: 5.0425\n",
            "Epoch 938/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1399 - val_loss: 5.0120\n",
            "Epoch 939/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1383 - val_loss: 4.7148\n",
            "Epoch 940/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1426 - val_loss: 4.5879\n",
            "Epoch 941/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1414 - val_loss: 4.6880\n",
            "Epoch 942/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1415 - val_loss: 4.7706\n",
            "Epoch 943/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1366 - val_loss: 4.5426\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 944/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1464 - val_loss: 4.2227\n",
            "Epoch 945/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1485 - val_loss: 4.2839\n",
            "Epoch 946/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1459 - val_loss: 4.3397\n",
            "Epoch 947/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1421 - val_loss: 4.4646\n",
            "Epoch 948/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1388 - val_loss: 4.6868\n",
            "Epoch 949/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1332 - val_loss: 4.8818\n",
            "Epoch 950/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1305 - val_loss: 5.1262\n",
            "Epoch 951/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1306 - val_loss: 5.2666\n",
            "Epoch 952/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1308 - val_loss: 5.3775\n",
            "Epoch 953/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1350 - val_loss: 5.5430\n",
            "Epoch 954/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1336 - val_loss: 5.6025\n",
            "Epoch 955/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1342 - val_loss: 5.5584\n",
            "Epoch 956/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1342 - val_loss: 5.5739\n",
            "Epoch 957/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1324 - val_loss: 5.3814\n",
            "Epoch 958/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1288 - val_loss: 4.8862\n",
            "Epoch 959/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1275 - val_loss: 4.6575\n",
            "Epoch 960/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1286 - val_loss: 4.6103\n",
            "Epoch 961/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1284 - val_loss: 4.6267\n",
            "Epoch 962/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1272 - val_loss: 4.7131\n",
            "Epoch 963/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1266 - val_loss: 4.8855\n",
            "Epoch 964/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1269 - val_loss: 5.1478\n",
            "Epoch 965/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1249 - val_loss: 5.3255\n",
            "Epoch 966/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1280 - val_loss: 5.4695\n",
            "Epoch 967/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1288 - val_loss: 5.4814\n",
            "Epoch 968/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1279 - val_loss: 5.5003\n",
            "Epoch 969/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1280 - val_loss: 5.4525\n",
            "Epoch 970/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1262 - val_loss: 5.3340\n",
            "Epoch 971/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1240 - val_loss: 5.1987\n",
            "Epoch 972/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1255 - val_loss: 5.0792\n",
            "Epoch 973/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1219 - val_loss: 5.1180\n",
            "Epoch 974/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1220 - val_loss: 5.1790\n",
            "Epoch 975/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1219 - val_loss: 5.1603\n",
            "Epoch 976/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1216 - val_loss: 5.1301\n",
            "Epoch 977/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1218 - val_loss: 5.2106\n",
            "Epoch 978/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1215 - val_loss: 5.1434\n",
            "Epoch 979/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1201 - val_loss: 5.0908\n",
            "Epoch 980/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1194 - val_loss: 5.0427\n",
            "Epoch 981/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1195 - val_loss: 5.0860\n",
            "Epoch 982/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1189 - val_loss: 5.0258\n",
            "Epoch 983/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1184 - val_loss: 4.9893\n",
            "Epoch 984/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1178 - val_loss: 4.9843\n",
            "Epoch 985/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1173 - val_loss: 5.0640\n",
            "Epoch 986/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1185 - val_loss: 5.1741\n",
            "Epoch 987/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1183 - val_loss: 5.1719\n",
            "Epoch 988/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1182 - val_loss: 5.0872\n",
            "Epoch 989/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1169 - val_loss: 4.9567\n",
            "Epoch 990/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1154 - val_loss: 4.8584\n",
            "Epoch 991/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1151 - val_loss: 4.8395\n",
            "Epoch 992/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1152 - val_loss: 4.9485\n",
            "Epoch 993/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1140 - val_loss: 4.9585\n",
            "Epoch 994/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1138 - val_loss: 4.9578\n",
            "Epoch 995/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1137 - val_loss: 4.9936\n",
            "Epoch 996/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1130 - val_loss: 5.1190\n",
            "Epoch 997/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1144 - val_loss: 5.1342\n",
            "Epoch 998/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1149 - val_loss: 5.1463\n",
            "Epoch 999/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1142 - val_loss: 5.1251\n",
            "Epoch 1000/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1136 - val_loss: 5.0152\n",
            "Epoch 1001/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1103 - val_loss: 4.7728\n",
            "Epoch 1002/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1106 - val_loss: 4.6726\n",
            "Epoch 1003/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1102 - val_loss: 4.7041\n",
            "Epoch 1004/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1095 - val_loss: 4.7126\n",
            "Epoch 1005/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1092 - val_loss: 4.7631\n",
            "Epoch 1006/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1087 - val_loss: 4.8793\n",
            "Epoch 1007/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1107 - val_loss: 4.8529\n",
            "Epoch 1008/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1140 - val_loss: 4.4395\n",
            "Epoch 1009/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1095 - val_loss: 4.2216\n",
            "Epoch 1010/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1100 - val_loss: 4.1974\n",
            "Epoch 1011/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1100 - val_loss: 4.2922\n",
            "Epoch 1012/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1084 - val_loss: 4.3367\n",
            "Epoch 1013/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1089 - val_loss: 4.4736\n",
            "Epoch 1014/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1060 - val_loss: 4.5185\n",
            "Epoch 1015/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1059 - val_loss: 4.5289\n",
            "Epoch 1016/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1055 - val_loss: 4.5556\n",
            "Epoch 1017/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1050 - val_loss: 4.5610\n",
            "Epoch 1018/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1054 - val_loss: 4.6304\n",
            "Epoch 1019/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1055 - val_loss: 4.7453\n",
            "Epoch 1020/2000\n",
            "16/16 [==============================] - 0s 561us/step - loss: 0.1053 - val_loss: 4.8382\n",
            "Epoch 1021/2000\n",
            "16/16 [==============================] - 0s 399us/step - loss: 0.1059 - val_loss: 4.8697\n",
            "Epoch 1022/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1065 - val_loss: 4.7860\n",
            "Epoch 1023/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 976us/step - loss: 0.0990 - val_loss: 4.2766\n",
            "Epoch 1024/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1009 - val_loss: 3.7352\n",
            "Epoch 1025/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1159 - val_loss: 3.5058\n",
            "Epoch 1026/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1211 - val_loss: 3.6122\n",
            "Epoch 1027/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1154 - val_loss: 3.7658\n",
            "Epoch 1028/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1107 - val_loss: 3.9058\n",
            "Epoch 1029/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1054 - val_loss: 4.0192\n",
            "Epoch 1030/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.1022 - val_loss: 4.1808\n",
            "Epoch 1031/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.1012 - val_loss: 4.3791\n",
            "Epoch 1032/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0992 - val_loss: 4.4887\n",
            "Epoch 1033/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0992 - val_loss: 4.5442\n",
            "Epoch 1034/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0992 - val_loss: 4.5508\n",
            "Epoch 1035/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0994 - val_loss: 4.4761\n",
            "Epoch 1036/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0978 - val_loss: 4.3134\n",
            "Epoch 1037/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0989 - val_loss: 4.2001\n",
            "Epoch 1038/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0986 - val_loss: 4.2254\n",
            "Epoch 1039/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0979 - val_loss: 4.2981\n",
            "Epoch 1040/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0969 - val_loss: 4.4733\n",
            "Epoch 1041/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0965 - val_loss: 4.5797\n",
            "Epoch 1042/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0996 - val_loss: 4.6242\n",
            "Epoch 1043/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0985 - val_loss: 4.3827\n",
            "Epoch 1044/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0952 - val_loss: 4.2537\n",
            "Epoch 1045/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0985 - val_loss: 4.1241\n",
            "Epoch 1046/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0963 - val_loss: 4.2110\n",
            "Epoch 1047/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0974 - val_loss: 4.3596\n",
            "Epoch 1048/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0948 - val_loss: 4.2798\n",
            "Epoch 1049/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0950 - val_loss: 4.2258\n",
            "Epoch 1050/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0938 - val_loss: 4.3079\n",
            "Epoch 1051/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0934 - val_loss: 4.3732\n",
            "Epoch 1052/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0937 - val_loss: 4.3976\n",
            "Epoch 1053/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0930 - val_loss: 4.4790\n",
            "Epoch 1054/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0941 - val_loss: 4.6147\n",
            "Epoch 1055/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0946 - val_loss: 4.6212\n",
            "Epoch 1056/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0946 - val_loss: 4.5919\n",
            "Epoch 1057/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0944 - val_loss: 4.4556\n",
            "Epoch 1058/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0930 - val_loss: 4.3423\n",
            "Epoch 1059/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0919 - val_loss: 4.3515\n",
            "Epoch 1060/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0917 - val_loss: 4.4320\n",
            "Epoch 1061/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0920 - val_loss: 4.4262\n",
            "Epoch 1062/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0916 - val_loss: 4.4649\n",
            "Epoch 1063/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0925 - val_loss: 4.3739\n",
            "Epoch 1064/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0864 - val_loss: 3.9937\n",
            "Epoch 1065/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0914 - val_loss: 3.7722\n",
            "Epoch 1066/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0934 - val_loss: 3.8404\n",
            "Epoch 1067/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0921 - val_loss: 4.0454\n",
            "Epoch 1068/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0918 - val_loss: 4.1933\n",
            "Epoch 1069/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0883 - val_loss: 4.0028\n",
            "Epoch 1070/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0890 - val_loss: 3.9059\n",
            "Epoch 1071/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0895 - val_loss: 3.9026\n",
            "Epoch 1072/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0894 - val_loss: 3.9641\n",
            "Epoch 1073/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0884 - val_loss: 3.9901\n",
            "Epoch 1074/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0883 - val_loss: 4.0994\n",
            "Epoch 1075/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0866 - val_loss: 4.2292\n",
            "Epoch 1076/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0881 - val_loss: 4.3266\n",
            "Epoch 1077/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0875 - val_loss: 4.3366\n",
            "Epoch 1078/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0872 - val_loss: 4.2980\n",
            "Epoch 1079/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0876 - val_loss: 4.2939\n",
            "Epoch 1080/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0884 - val_loss: 4.1489\n",
            "Epoch 1081/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0861 - val_loss: 4.0278\n",
            "Epoch 1082/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0833 - val_loss: 3.6652\n",
            "Epoch 1083/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0920 - val_loss: 3.4382\n",
            "Epoch 1084/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0944 - val_loss: 3.3883\n",
            "Epoch 1085/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0940 - val_loss: 3.4745\n",
            "Epoch 1086/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0907 - val_loss: 3.5951\n",
            "Epoch 1087/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0888 - val_loss: 3.8684\n",
            "Epoch 1088/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0837 - val_loss: 4.0436\n",
            "Epoch 1089/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0839 - val_loss: 4.1931\n",
            "Epoch 1090/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0844 - val_loss: 4.3900\n",
            "Epoch 1091/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0880 - val_loss: 4.5961\n",
            "Epoch 1092/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0889 - val_loss: 4.6720\n",
            "Epoch 1093/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0912 - val_loss: 4.6406\n",
            "Epoch 1094/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0885 - val_loss: 4.4352\n",
            "Epoch 1095/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0858 - val_loss: 4.1799\n",
            "Epoch 1096/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0814 - val_loss: 4.0140\n",
            "Epoch 1097/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0827 - val_loss: 3.8692\n",
            "Epoch 1098/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0835 - val_loss: 3.8353\n",
            "Epoch 1099/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0820 - val_loss: 3.9637\n",
            "Epoch 1100/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0812 - val_loss: 4.0075\n",
            "Epoch 1101/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0812 - val_loss: 4.0550\n",
            "Epoch 1102/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 976us/step - loss: 0.0803 - val_loss: 3.9213\n",
            "Epoch 1103/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0809 - val_loss: 3.6881\n",
            "Epoch 1104/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0821 - val_loss: 3.5840\n",
            "Epoch 1105/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0832 - val_loss: 3.6182\n",
            "Epoch 1106/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0820 - val_loss: 3.6518\n",
            "Epoch 1107/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0814 - val_loss: 3.7625\n",
            "Epoch 1108/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0798 - val_loss: 3.9942\n",
            "Epoch 1109/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0784 - val_loss: 4.1775\n",
            "Epoch 1110/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0812 - val_loss: 4.3127\n",
            "Epoch 1111/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0825 - val_loss: 4.3763\n",
            "Epoch 1112/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0829 - val_loss: 4.3169\n",
            "Epoch 1113/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0821 - val_loss: 4.1705\n",
            "Epoch 1114/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0799 - val_loss: 4.0924\n",
            "Epoch 1115/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0789 - val_loss: 4.0441\n",
            "Epoch 1116/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0778 - val_loss: 3.9621\n",
            "Epoch 1117/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0778 - val_loss: 3.8595\n",
            "Epoch 1118/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0814 - val_loss: 3.6283\n",
            "Epoch 1119/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0785 - val_loss: 3.5826\n",
            "Epoch 1120/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0785 - val_loss: 3.6005\n",
            "Epoch 1121/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0781 - val_loss: 3.6543\n",
            "Epoch 1122/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0800 - val_loss: 3.8639\n",
            "Epoch 1123/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0774 - val_loss: 3.9527\n",
            "Epoch 1124/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0765 - val_loss: 3.9572\n",
            "Epoch 1125/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0765 - val_loss: 3.9060\n",
            "Epoch 1126/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0794 - val_loss: 3.6904\n",
            "Epoch 1127/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0765 - val_loss: 3.7426\n",
            "Epoch 1128/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0756 - val_loss: 3.7705\n",
            "Epoch 1129/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0756 - val_loss: 3.8660\n",
            "Epoch 1130/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0756 - val_loss: 3.9238\n",
            "Epoch 1131/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0753 - val_loss: 3.8711\n",
            "Epoch 1132/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0753 - val_loss: 3.8552\n",
            "Epoch 1133/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0743 - val_loss: 3.9309\n",
            "Epoch 1134/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0749 - val_loss: 4.0441\n",
            "Epoch 1135/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0765 - val_loss: 4.1370\n",
            "Epoch 1136/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0767 - val_loss: 4.0984\n",
            "Epoch 1137/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0763 - val_loss: 4.0035\n",
            "Epoch 1138/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0754 - val_loss: 3.8509\n",
            "Epoch 1139/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0737 - val_loss: 3.7444\n",
            "Epoch 1140/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0733 - val_loss: 3.7020\n",
            "Epoch 1141/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0733 - val_loss: 3.6861\n",
            "Epoch 1142/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0732 - val_loss: 3.7035\n",
            "Epoch 1143/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0729 - val_loss: 3.7269\n",
            "Epoch 1144/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0733 - val_loss: 3.8129\n",
            "Epoch 1145/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0726 - val_loss: 3.8848\n",
            "Epoch 1146/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0757 - val_loss: 3.7209\n",
            "Epoch 1147/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0739 - val_loss: 3.6453\n",
            "Epoch 1148/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0725 - val_loss: 3.7440\n",
            "Epoch 1149/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0721 - val_loss: 3.7997\n",
            "Epoch 1150/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0719 - val_loss: 3.8429\n",
            "Epoch 1151/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0723 - val_loss: 3.9720\n",
            "Epoch 1152/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0757 - val_loss: 3.9695\n",
            "Epoch 1153/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0718 - val_loss: 3.5292\n",
            "Epoch 1154/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0747 - val_loss: 3.2862\n",
            "Epoch 1155/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0737 - val_loss: 3.3878\n",
            "Epoch 1156/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0765 - val_loss: 3.6811\n",
            "Epoch 1157/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0699 - val_loss: 3.7891\n",
            "Epoch 1158/2000\n",
            "16/16 [==============================] - 0s 483us/step - loss: 0.0703 - val_loss: 3.8178\n",
            "Epoch 1159/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0709 - val_loss: 3.8082\n",
            "Epoch 1160/2000\n",
            "16/16 [==============================] - 0s 438us/step - loss: 0.0701 - val_loss: 3.8707\n",
            "Epoch 1161/2000\n",
            "16/16 [==============================] - 0s 235us/step - loss: 0.0713 - val_loss: 3.9222\n",
            "Epoch 1162/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0713 - val_loss: 3.8894\n",
            "Epoch 1163/2000\n",
            "16/16 [==============================] - 0s 910us/step - loss: 0.0713 - val_loss: 3.8964\n",
            "Epoch 1164/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0707 - val_loss: 3.8682\n",
            "Epoch 1165/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0702 - val_loss: 3.8588\n",
            "Epoch 1166/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0710 - val_loss: 3.8805\n",
            "Epoch 1167/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0704 - val_loss: 3.8758\n",
            "Epoch 1168/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0698 - val_loss: 3.9003\n",
            "Epoch 1169/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0706 - val_loss: 3.8384\n",
            "Epoch 1170/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0695 - val_loss: 3.8118\n",
            "Epoch 1171/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0687 - val_loss: 3.6831\n",
            "Epoch 1172/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0692 - val_loss: 3.4524\n",
            "Epoch 1173/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0685 - val_loss: 3.4569\n",
            "Epoch 1174/2000\n",
            "16/16 [==============================] - 0s 737us/step - loss: 0.0673 - val_loss: 3.6004\n",
            "Epoch 1175/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0677 - val_loss: 3.6896\n",
            "Epoch 1176/2000\n",
            "16/16 [==============================] - 0s 109us/step - loss: 0.0681 - val_loss: 3.5358\n",
            "Epoch 1177/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0693 - val_loss: 3.3571\n",
            "Epoch 1178/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0675 - val_loss: 3.4391\n",
            "Epoch 1179/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0678 - val_loss: 3.6950\n",
            "Epoch 1180/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0701 - val_loss: 3.9054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1181/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0700 - val_loss: 3.9247\n",
            "Epoch 1182/2000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0696 - val_loss: 3.8452\n",
            "Epoch 1183/2000\n",
            "16/16 [==============================] - 0s 373us/step - loss: 0.0686 - val_loss: 3.8042\n",
            "Epoch 1184/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0678 - val_loss: 3.8247\n",
            "Epoch 1185/2000\n",
            "16/16 [==============================] - 0s 379us/step - loss: 0.0682 - val_loss: 3.8054\n",
            "Epoch 1186/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0675 - val_loss: 3.7046\n",
            "Epoch 1187/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0662 - val_loss: 3.5934\n",
            "Epoch 1188/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0658 - val_loss: 3.4734\n",
            "Epoch 1189/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0661 - val_loss: 3.3671\n",
            "Epoch 1190/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0664 - val_loss: 3.3140\n",
            "Epoch 1191/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0667 - val_loss: 3.3283\n",
            "Epoch 1192/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0654 - val_loss: 3.4124\n",
            "Epoch 1193/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0654 - val_loss: 3.4473\n",
            "Epoch 1194/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0657 - val_loss: 3.3590\n",
            "Epoch 1195/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0654 - val_loss: 3.3636\n",
            "Epoch 1196/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0647 - val_loss: 3.5188\n",
            "Epoch 1197/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0657 - val_loss: 3.6273\n",
            "Epoch 1198/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0656 - val_loss: 3.6182\n",
            "Epoch 1199/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0652 - val_loss: 3.7043\n",
            "Epoch 1200/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0660 - val_loss: 3.6880\n",
            "Epoch 1201/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0651 - val_loss: 3.6248\n",
            "Epoch 1202/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0658 - val_loss: 3.5299\n",
            "Epoch 1203/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0631 - val_loss: 3.1004\n",
            "Epoch 1204/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0665 - val_loss: 2.8925\n",
            "Epoch 1205/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0702 - val_loss: 2.8792\n",
            "Epoch 1206/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0696 - val_loss: 3.0598\n",
            "Epoch 1207/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0630 - val_loss: 3.3647\n",
            "Epoch 1208/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0613 - val_loss: 3.6485\n",
            "Epoch 1209/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0647 - val_loss: 3.9076\n",
            "Epoch 1210/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0694 - val_loss: 3.9561\n",
            "Epoch 1211/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0703 - val_loss: 3.7579\n",
            "Epoch 1212/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0644 - val_loss: 3.5877\n",
            "Epoch 1213/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0621 - val_loss: 3.3272\n",
            "Epoch 1214/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0627 - val_loss: 2.8972\n",
            "Epoch 1215/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0688 - val_loss: 2.7380\n",
            "Epoch 1216/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0719 - val_loss: 2.8255\n",
            "Epoch 1217/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0671 - val_loss: 3.1598\n",
            "Epoch 1218/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0650 - val_loss: 3.5643\n",
            "Epoch 1219/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0619 - val_loss: 3.7088\n",
            "Epoch 1220/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0670 - val_loss: 3.8249\n",
            "Epoch 1221/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0666 - val_loss: 3.7362\n",
            "Epoch 1222/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0635 - val_loss: 3.5518\n",
            "Epoch 1223/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0627 - val_loss: 3.3726\n",
            "Epoch 1224/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0614 - val_loss: 3.3535\n",
            "Epoch 1225/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0611 - val_loss: 3.3335\n",
            "Epoch 1226/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0610 - val_loss: 3.3372\n",
            "Epoch 1227/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0607 - val_loss: 3.3661\n",
            "Epoch 1228/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0607 - val_loss: 3.4611\n",
            "Epoch 1229/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0607 - val_loss: 3.5666\n",
            "Epoch 1230/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0621 - val_loss: 3.5454\n",
            "Epoch 1231/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0617 - val_loss: 3.4749\n",
            "Epoch 1232/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0607 - val_loss: 3.4006\n",
            "Epoch 1233/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0604 - val_loss: 3.4000\n",
            "Epoch 1234/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0606 - val_loss: 3.3506\n",
            "Epoch 1235/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0601 - val_loss: 3.0749\n",
            "Epoch 1236/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0683 - val_loss: 2.6573\n",
            "Epoch 1237/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0698 - val_loss: 2.5740\n",
            "Epoch 1238/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0721 - val_loss: 2.6217\n",
            "Epoch 1239/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0692 - val_loss: 2.7816\n",
            "Epoch 1240/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0662 - val_loss: 2.9256\n",
            "Epoch 1241/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0616 - val_loss: 2.8630\n",
            "Epoch 1242/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0625 - val_loss: 2.8709\n",
            "Epoch 1243/2000\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.142 - 0s 976us/step - loss: 0.0617 - val_loss: 2.9725\n",
            "Epoch 1244/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0610 - val_loss: 3.1841\n",
            "Epoch 1245/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0580 - val_loss: 3.3087\n",
            "Epoch 1246/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0579 - val_loss: 3.4842\n",
            "Epoch 1247/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0606 - val_loss: 3.5018\n",
            "Epoch 1248/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0607 - val_loss: 3.2759\n",
            "Epoch 1249/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0589 - val_loss: 3.1608\n",
            "Epoch 1250/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0617 - val_loss: 3.2715\n",
            "Epoch 1251/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0581 - val_loss: 3.2097\n",
            "Epoch 1252/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0580 - val_loss: 3.1491\n",
            "Epoch 1253/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0584 - val_loss: 3.0546\n",
            "Epoch 1254/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0589 - val_loss: 2.9980\n",
            "Epoch 1255/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0589 - val_loss: 3.0164\n",
            "Epoch 1256/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0580 - val_loss: 3.1134\n",
            "Epoch 1257/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0570 - val_loss: 3.3503\n",
            "Epoch 1258/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0579 - val_loss: 3.4998\n",
            "Epoch 1259/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 976us/step - loss: 0.0601 - val_loss: 3.4997\n",
            "Epoch 1260/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0604 - val_loss: 3.3465\n",
            "Epoch 1261/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0592 - val_loss: 3.1971\n",
            "Epoch 1262/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0578 - val_loss: 3.0969\n",
            "Epoch 1263/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0576 - val_loss: 3.0715\n",
            "Epoch 1264/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0586 - val_loss: 3.2369\n",
            "Epoch 1265/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0575 - val_loss: 3.2780\n",
            "Epoch 1266/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0572 - val_loss: 3.2929\n",
            "Epoch 1267/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0576 - val_loss: 3.3598\n",
            "Epoch 1268/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0579 - val_loss: 3.3140\n",
            "Epoch 1269/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0573 - val_loss: 3.2646\n",
            "Epoch 1270/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0574 - val_loss: 3.1527\n",
            "Epoch 1271/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0605 - val_loss: 2.7101\n",
            "Epoch 1272/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0642 - val_loss: 2.5285\n",
            "Epoch 1273/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0658 - val_loss: 2.6006\n",
            "Epoch 1274/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0620 - val_loss: 2.8145\n",
            "Epoch 1275/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0584 - val_loss: 3.0121\n",
            "Epoch 1276/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0575 - val_loss: 3.1577\n",
            "Epoch 1277/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0558 - val_loss: 3.2230\n",
            "Epoch 1278/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0562 - val_loss: 3.4003\n",
            "Epoch 1279/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0596 - val_loss: 3.5424\n",
            "Epoch 1280/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0603 - val_loss: 3.4862\n",
            "Epoch 1281/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0590 - val_loss: 3.4441\n",
            "Epoch 1282/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0583 - val_loss: 3.3763\n",
            "Epoch 1283/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0565 - val_loss: 3.2090\n",
            "Epoch 1284/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0544 - val_loss: 2.9024\n",
            "Epoch 1285/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0579 - val_loss: 2.6768\n",
            "Epoch 1286/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0595 - val_loss: 2.7542\n",
            "Epoch 1287/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0557 - val_loss: 2.9983\n",
            "Epoch 1288/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0532 - val_loss: 3.3129\n",
            "Epoch 1289/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0590 - val_loss: 3.5041\n",
            "Epoch 1290/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0601 - val_loss: 3.4788\n",
            "Epoch 1291/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0613 - val_loss: 3.3137\n",
            "Epoch 1292/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0571 - val_loss: 3.2767\n",
            "Epoch 1293/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0573 - val_loss: 3.3498\n",
            "Epoch 1294/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0582 - val_loss: 3.4491\n",
            "Epoch 1295/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0590 - val_loss: 3.3900\n",
            "Epoch 1296/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0553 - val_loss: 2.9870\n",
            "Epoch 1297/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0570 - val_loss: 2.4464\n",
            "Epoch 1298/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0644 - val_loss: 2.2637\n",
            "Epoch 1299/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0713 - val_loss: 2.3022\n",
            "Epoch 1300/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0659 - val_loss: 2.5329\n",
            "Epoch 1301/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0609 - val_loss: 3.0210\n",
            "Epoch 1302/2000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0554 - val_loss: 3.3552\n",
            "Epoch 1303/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0570 - val_loss: 3.4218\n",
            "Epoch 1304/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0583 - val_loss: 3.3028\n",
            "Epoch 1305/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0557 - val_loss: 3.1011\n",
            "Epoch 1306/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0537 - val_loss: 2.9155\n",
            "Epoch 1307/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0552 - val_loss: 2.7839\n",
            "Epoch 1308/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0546 - val_loss: 2.7806\n",
            "Epoch 1309/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0559 - val_loss: 2.8602\n",
            "Epoch 1310/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0535 - val_loss: 2.8800\n",
            "Epoch 1311/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0529 - val_loss: 2.9819\n",
            "Epoch 1312/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0547 - val_loss: 3.0474\n",
            "Epoch 1313/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0554 - val_loss: 2.7154\n",
            "Epoch 1314/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0556 - val_loss: 2.6148\n",
            "Epoch 1315/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0550 - val_loss: 2.8081\n",
            "Epoch 1316/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0503 - val_loss: 3.0855\n",
            "Epoch 1317/2000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0532 - val_loss: 3.3591\n",
            "Epoch 1318/2000\n",
            "16/16 [==============================] - 0s 393us/step - loss: 0.0597 - val_loss: 3.5276\n",
            "Epoch 1319/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0619 - val_loss: 3.4188\n",
            "Epoch 1320/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0561 - val_loss: 3.0423\n",
            "Epoch 1321/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0518 - val_loss: 2.8962\n",
            "Epoch 1322/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0528 - val_loss: 2.7719\n",
            "Epoch 1323/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0532 - val_loss: 2.7546\n",
            "Epoch 1324/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0533 - val_loss: 2.9773\n",
            "Epoch 1325/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0529 - val_loss: 3.2057\n",
            "Epoch 1326/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0547 - val_loss: 3.2328\n",
            "Epoch 1327/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0551 - val_loss: 3.1111\n",
            "Epoch 1328/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0530 - val_loss: 3.0922\n",
            "Epoch 1329/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0554 - val_loss: 3.1884\n",
            "Epoch 1330/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0535 - val_loss: 3.0707\n",
            "Epoch 1331/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0499 - val_loss: 2.7987\n",
            "Epoch 1332/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0515 - val_loss: 2.5976\n",
            "Epoch 1333/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0536 - val_loss: 2.5358\n",
            "Epoch 1334/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0561 - val_loss: 2.7010\n",
            "Epoch 1335/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0532 - val_loss: 2.8406\n",
            "Epoch 1336/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0514 - val_loss: 2.8470\n",
            "Epoch 1337/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0510 - val_loss: 2.8769\n",
            "Epoch 1338/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 0us/step - loss: 0.0512 - val_loss: 2.8663\n",
            "Epoch 1339/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0510 - val_loss: 2.8018\n",
            "Epoch 1340/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0511 - val_loss: 2.7412\n",
            "Epoch 1341/2000\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.018 - 0s 0us/step - loss: 0.0520 - val_loss: 2.6886\n",
            "Epoch 1342/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0521 - val_loss: 2.4975\n",
            "Epoch 1343/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0537 - val_loss: 2.6404\n",
            "Epoch 1344/2000\n",
            "16/16 [==============================] - 0s 975us/step - loss: 0.0510 - val_loss: 2.9398\n",
            "Epoch 1345/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0516 - val_loss: 3.1626\n",
            "Epoch 1346/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0549 - val_loss: 3.3184\n",
            "Epoch 1347/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0564 - val_loss: 3.2371\n",
            "Epoch 1348/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0551 - val_loss: 3.1553\n",
            "Epoch 1349/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0528 - val_loss: 3.0747\n",
            "Epoch 1350/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0514 - val_loss: 2.9033\n",
            "Epoch 1351/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0524 - val_loss: 2.6106\n",
            "Epoch 1352/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0514 - val_loss: 2.4901\n",
            "Epoch 1353/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0532 - val_loss: 2.5029\n",
            "Epoch 1354/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0537 - val_loss: 2.7523\n",
            "Epoch 1355/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0496 - val_loss: 2.8978\n",
            "Epoch 1356/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0499 - val_loss: 2.9213\n",
            "Epoch 1357/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0519 - val_loss: 2.7520\n",
            "Epoch 1358/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0496 - val_loss: 2.6804\n",
            "Epoch 1359/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0500 - val_loss: 2.5652\n",
            "Epoch 1360/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0511 - val_loss: 2.2937\n",
            "Epoch 1361/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0579 - val_loss: 2.2162\n",
            "Epoch 1362/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0643 - val_loss: 2.1313\n",
            "Epoch 1363/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0588 - val_loss: 2.4341\n",
            "Epoch 1364/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0504 - val_loss: 2.6831\n",
            "Epoch 1365/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0484 - val_loss: 2.8614\n",
            "Epoch 1366/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0493 - val_loss: 2.9132\n",
            "Epoch 1367/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0503 - val_loss: 2.9148\n",
            "Epoch 1368/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0494 - val_loss: 2.8030\n",
            "Epoch 1369/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0487 - val_loss: 2.6920\n",
            "Epoch 1370/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0494 - val_loss: 2.5883\n",
            "Epoch 1371/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0499 - val_loss: 2.6196\n",
            "Epoch 1372/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0496 - val_loss: 2.6665\n",
            "Epoch 1373/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0488 - val_loss: 2.6789\n",
            "Epoch 1374/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0489 - val_loss: 2.6965\n",
            "Epoch 1375/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0487 - val_loss: 2.8523\n",
            "Epoch 1376/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0486 - val_loss: 2.9392\n",
            "Epoch 1377/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0494 - val_loss: 2.8934\n",
            "Epoch 1378/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0499 - val_loss: 2.8853\n",
            "Epoch 1379/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0486 - val_loss: 2.7730\n",
            "Epoch 1380/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0492 - val_loss: 2.5990\n",
            "Epoch 1381/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0494 - val_loss: 2.5184\n",
            "Epoch 1382/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0489 - val_loss: 2.6863\n",
            "Epoch 1383/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0471 - val_loss: 2.8830\n",
            "Epoch 1384/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0480 - val_loss: 3.0203\n",
            "Epoch 1385/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0502 - val_loss: 3.0449\n",
            "Epoch 1386/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0505 - val_loss: 2.9445\n",
            "Epoch 1387/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0483 - val_loss: 2.7251\n",
            "Epoch 1388/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0504 - val_loss: 2.2283\n",
            "Epoch 1389/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0576 - val_loss: 2.0578\n",
            "Epoch 1390/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0626 - val_loss: 2.1291\n",
            "Epoch 1391/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0564 - val_loss: 2.3719\n",
            "Epoch 1392/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0494 - val_loss: 2.7356\n",
            "Epoch 1393/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0462 - val_loss: 2.9490\n",
            "Epoch 1394/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0501 - val_loss: 3.0337\n",
            "Epoch 1395/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0512 - val_loss: 2.9788\n",
            "Epoch 1396/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0505 - val_loss: 2.8132\n",
            "Epoch 1397/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0480 - val_loss: 2.8134\n",
            "Epoch 1398/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0472 - val_loss: 2.7092\n",
            "Epoch 1399/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0468 - val_loss: 2.5665\n",
            "Epoch 1400/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0476 - val_loss: 2.4738\n",
            "Epoch 1401/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0491 - val_loss: 2.5197\n",
            "Epoch 1402/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0475 - val_loss: 2.7631\n",
            "Epoch 1403/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0467 - val_loss: 2.9011\n",
            "Epoch 1404/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0520 - val_loss: 3.0544\n",
            "Epoch 1405/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0501 - val_loss: 2.9699\n",
            "Epoch 1406/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0487 - val_loss: 2.7882\n",
            "Epoch 1407/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0473 - val_loss: 2.6190\n",
            "Epoch 1408/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0474 - val_loss: 2.5522\n",
            "Epoch 1409/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0473 - val_loss: 2.5684\n",
            "Epoch 1410/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0469 - val_loss: 2.6431\n",
            "Epoch 1411/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0468 - val_loss: 2.7674\n",
            "Epoch 1412/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0477 - val_loss: 2.9861\n",
            "Epoch 1413/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0506 - val_loss: 3.1115\n",
            "Epoch 1414/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0516 - val_loss: 3.0357\n",
            "Epoch 1415/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0503 - val_loss: 2.8166\n",
            "Epoch 1416/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0494 - val_loss: 2.6672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1417/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0462 - val_loss: 2.7099\n",
            "Epoch 1418/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0468 - val_loss: 2.7508\n",
            "Epoch 1419/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0491 - val_loss: 2.9062\n",
            "Epoch 1420/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0482 - val_loss: 2.8720\n",
            "Epoch 1421/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0464 - val_loss: 2.6227\n",
            "Epoch 1422/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0437 - val_loss: 2.1722\n",
            "Epoch 1423/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0575 - val_loss: 1.9880\n",
            "Epoch 1424/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0608 - val_loss: 2.0605\n",
            "Epoch 1425/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0554 - val_loss: 2.2611\n",
            "Epoch 1426/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0481 - val_loss: 2.5865\n",
            "Epoch 1427/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0489 - val_loss: 2.8565\n",
            "Epoch 1428/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0477 - val_loss: 2.8196\n",
            "Epoch 1429/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0468 - val_loss: 2.7719\n",
            "Epoch 1430/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0460 - val_loss: 2.6886\n",
            "Epoch 1431/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0456 - val_loss: 2.5570\n",
            "Epoch 1432/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0473 - val_loss: 2.4629\n",
            "Epoch 1433/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0452 - val_loss: 2.6322\n",
            "Epoch 1434/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0465 - val_loss: 2.9235\n",
            "Epoch 1435/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0489 - val_loss: 2.9801\n",
            "Epoch 1436/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0492 - val_loss: 2.9476\n",
            "Epoch 1437/2000\n",
            "16/16 [==============================] - 0s 918us/step - loss: 0.0495 - val_loss: 2.9190\n",
            "Epoch 1438/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0473 - val_loss: 2.7723\n",
            "Epoch 1439/2000\n",
            "16/16 [==============================] - 0s 142us/step - loss: 0.0455 - val_loss: 2.6214\n",
            "Epoch 1440/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0450 - val_loss: 2.5159\n",
            "Epoch 1441/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0457 - val_loss: 2.4267\n",
            "Epoch 1442/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0460 - val_loss: 2.4798\n",
            "Epoch 1443/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0444 - val_loss: 2.7124\n",
            "Epoch 1444/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0503 - val_loss: 2.8837\n",
            "Epoch 1445/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0439 - val_loss: 2.4651\n",
            "Epoch 1446/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0503 - val_loss: 2.1613\n",
            "Epoch 1447/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0497 - val_loss: 2.3105\n",
            "Epoch 1448/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0459 - val_loss: 2.4799\n",
            "Epoch 1449/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0468 - val_loss: 2.7413\n",
            "Epoch 1450/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0472 - val_loss: 2.8679\n",
            "Epoch 1451/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0474 - val_loss: 2.9201\n",
            "Epoch 1452/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0484 - val_loss: 2.9516\n",
            "Epoch 1453/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0490 - val_loss: 2.8653\n",
            "Epoch 1454/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0473 - val_loss: 2.7445\n",
            "Epoch 1455/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0448 - val_loss: 2.4900\n",
            "Epoch 1456/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0417 - val_loss: 2.0016\n",
            "Epoch 1457/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0601 - val_loss: 1.6283\n",
            "Epoch 1458/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0796 - val_loss: 1.6176\n",
            "Epoch 1459/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0646 - val_loss: 2.1033\n",
            "Epoch 1460/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0431 - val_loss: 2.5845\n",
            "Epoch 1461/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0476 - val_loss: 2.9346\n",
            "Epoch 1462/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0500 - val_loss: 2.9282\n",
            "Epoch 1463/2000\n",
            "16/16 [==============================] - 0s 977us/step - loss: 0.0497 - val_loss: 2.7446\n",
            "Epoch 1464/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0474 - val_loss: 2.4535\n",
            "Epoch 1465/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0444 - val_loss: 2.3182\n",
            "Epoch 1466/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0445 - val_loss: 2.2922\n",
            "Epoch 1467/2000\n",
            "16/16 [==============================] - 0s 977us/step - loss: 0.0456 - val_loss: 2.3650\n",
            "Epoch 1468/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0441 - val_loss: 2.3353\n",
            "Epoch 1469/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0444 - val_loss: 2.4150\n",
            "Epoch 1470/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0432 - val_loss: 2.5559\n",
            "Epoch 1471/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0444 - val_loss: 2.6102\n",
            "Epoch 1472/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0443 - val_loss: 2.5164\n",
            "Epoch 1473/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0435 - val_loss: 2.4749\n",
            "Epoch 1474/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0440 - val_loss: 2.3943\n",
            "Epoch 1475/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0449 - val_loss: 2.3697\n",
            "Epoch 1476/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0435 - val_loss: 2.5314\n",
            "Epoch 1477/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0440 - val_loss: 2.7195\n",
            "Epoch 1478/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0458 - val_loss: 2.7627\n",
            "Epoch 1479/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0457 - val_loss: 2.6641\n",
            "Epoch 1480/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0445 - val_loss: 2.5983\n",
            "Epoch 1481/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0447 - val_loss: 2.5124\n",
            "Epoch 1482/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0433 - val_loss: 2.6038\n",
            "Epoch 1483/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0434 - val_loss: 2.5955\n",
            "Epoch 1484/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0432 - val_loss: 2.5308\n",
            "Epoch 1485/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0432 - val_loss: 2.3962\n",
            "Epoch 1486/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0429 - val_loss: 2.3818\n",
            "Epoch 1487/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0442 - val_loss: 2.5795\n",
            "Epoch 1488/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0448 - val_loss: 2.7718\n",
            "Epoch 1489/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0453 - val_loss: 2.7350\n",
            "Epoch 1490/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0444 - val_loss: 2.5679\n",
            "Epoch 1491/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0421 - val_loss: 2.4322\n",
            "Epoch 1492/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0431 - val_loss: 2.2772\n",
            "Epoch 1493/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0433 - val_loss: 2.3846\n",
            "Epoch 1494/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0423 - val_loss: 2.5063\n",
            "Epoch 1495/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0428 - val_loss: 2.5915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1496/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0432 - val_loss: 2.5917\n",
            "Epoch 1497/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0430 - val_loss: 2.5838\n",
            "Epoch 1498/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0440 - val_loss: 2.6744\n",
            "Epoch 1499/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0438 - val_loss: 2.5734\n",
            "Epoch 1500/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0438 - val_loss: 2.4379\n",
            "Epoch 1501/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0418 - val_loss: 2.5672\n",
            "Epoch 1502/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0447 - val_loss: 2.7466\n",
            "Epoch 1503/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0446 - val_loss: 2.6687\n",
            "Epoch 1504/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0417 - val_loss: 2.4866\n",
            "Epoch 1505/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0433 - val_loss: 2.3055\n",
            "Epoch 1506/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0440 - val_loss: 2.2766\n",
            "Epoch 1507/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0423 - val_loss: 2.5545\n",
            "Epoch 1508/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0409 - val_loss: 2.8514\n",
            "Epoch 1509/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0460 - val_loss: 3.0007\n",
            "Epoch 1510/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0505 - val_loss: 2.8965\n",
            "Epoch 1511/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0463 - val_loss: 2.6488\n",
            "Epoch 1512/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0464 - val_loss: 1.9872\n",
            "Epoch 1513/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0575 - val_loss: 1.8076\n",
            "Epoch 1514/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0539 - val_loss: 2.1630\n",
            "Epoch 1515/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0457 - val_loss: 2.4005\n",
            "Epoch 1516/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0426 - val_loss: 2.2135\n",
            "Epoch 1517/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0440 - val_loss: 2.1463\n",
            "Epoch 1518/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0439 - val_loss: 2.2218\n",
            "Epoch 1519/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0428 - val_loss: 2.4376\n",
            "Epoch 1520/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0408 - val_loss: 2.5598\n",
            "Epoch 1521/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0430 - val_loss: 2.6179\n",
            "Epoch 1522/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0420 - val_loss: 2.5016\n",
            "Epoch 1523/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0427 - val_loss: 2.3137\n",
            "Epoch 1524/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0414 - val_loss: 2.3038\n",
            "Epoch 1525/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0416 - val_loss: 2.3129\n",
            "Epoch 1526/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0407 - val_loss: 2.4260\n",
            "Epoch 1527/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0405 - val_loss: 2.5351\n",
            "Epoch 1528/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0412 - val_loss: 2.7470\n",
            "Epoch 1529/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0441 - val_loss: 2.8857\n",
            "Epoch 1530/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0477 - val_loss: 2.8859\n",
            "Epoch 1531/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0478 - val_loss: 2.7270\n",
            "Epoch 1532/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0444 - val_loss: 2.5973\n",
            "Epoch 1533/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0416 - val_loss: 2.4908\n",
            "Epoch 1534/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0410 - val_loss: 2.3296\n",
            "Epoch 1535/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0437 - val_loss: 2.1610\n",
            "Epoch 1536/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0422 - val_loss: 2.3450\n",
            "Epoch 1537/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0416 - val_loss: 2.4261\n",
            "Epoch 1538/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0403 - val_loss: 2.3419\n",
            "Epoch 1539/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0404 - val_loss: 2.2864\n",
            "Epoch 1540/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0406 - val_loss: 2.2674\n",
            "Epoch 1541/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0408 - val_loss: 2.2878\n",
            "Epoch 1542/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0416 - val_loss: 2.3402\n",
            "Epoch 1543/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0408 - val_loss: 2.2892\n",
            "Epoch 1544/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0403 - val_loss: 2.3229\n",
            "Epoch 1545/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0398 - val_loss: 2.4185\n",
            "Epoch 1546/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0401 - val_loss: 2.4441\n",
            "Epoch 1547/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0410 - val_loss: 2.4318\n",
            "Epoch 1548/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0399 - val_loss: 2.5065\n",
            "Epoch 1549/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0408 - val_loss: 2.5327\n",
            "Epoch 1550/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0414 - val_loss: 2.5868\n",
            "Epoch 1551/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0412 - val_loss: 2.4989\n",
            "Epoch 1552/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0420 - val_loss: 2.3081\n",
            "Epoch 1553/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0395 - val_loss: 2.1927\n",
            "Epoch 1554/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0409 - val_loss: 2.0876\n",
            "Epoch 1555/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0419 - val_loss: 2.1443\n",
            "Epoch 1556/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0405 - val_loss: 2.2763\n",
            "Epoch 1557/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0388 - val_loss: 2.4941\n",
            "Epoch 1558/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0398 - val_loss: 2.6744\n",
            "Epoch 1559/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0436 - val_loss: 2.7185\n",
            "Epoch 1560/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0456 - val_loss: 2.7216\n",
            "Epoch 1561/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0422 - val_loss: 2.4897\n",
            "Epoch 1562/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0411 - val_loss: 2.2131\n",
            "Epoch 1563/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0417 - val_loss: 2.2116\n",
            "Epoch 1564/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0387 - val_loss: 1.9085\n",
            "Epoch 1565/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0497 - val_loss: 1.7899\n",
            "Epoch 1566/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0485 - val_loss: 2.0129\n",
            "Epoch 1567/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0389 - val_loss: 2.5662\n",
            "Epoch 1568/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0502 - val_loss: 2.9760\n",
            "Epoch 1569/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0489 - val_loss: 2.6667\n",
            "Epoch 1570/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0397 - val_loss: 2.1870\n",
            "Epoch 1571/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0402 - val_loss: 1.7217\n",
            "Epoch 1572/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0522 - val_loss: 1.8712\n",
            "Epoch 1573/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0435 - val_loss: 2.1024\n",
            "Epoch 1574/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0360 - val_loss: 2.6243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1575/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0429 - val_loss: 2.9442\n",
            "Epoch 1576/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0520 - val_loss: 2.8505\n",
            "Epoch 1577/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0462 - val_loss: 2.5221\n",
            "Epoch 1578/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0415 - val_loss: 2.2911\n",
            "Epoch 1579/2000\n",
            "16/16 [==============================] - 0s 293us/step - loss: 0.0390 - val_loss: 2.3438\n",
            "Epoch 1580/2000\n",
            "16/16 [==============================] - 0s 429us/step - loss: 0.0386 - val_loss: 2.3768\n",
            "Epoch 1581/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0387 - val_loss: 2.3802\n",
            "Epoch 1582/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0388 - val_loss: 2.4166\n",
            "Epoch 1583/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0388 - val_loss: 2.3462\n",
            "Epoch 1584/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0378 - val_loss: 2.1758\n",
            "Epoch 1585/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0406 - val_loss: 2.0055\n",
            "Epoch 1586/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0408 - val_loss: 2.0746\n",
            "Epoch 1587/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0399 - val_loss: 2.3340\n",
            "Epoch 1588/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0372 - val_loss: 2.5333\n",
            "Epoch 1589/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0403 - val_loss: 2.5804\n",
            "Epoch 1590/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0408 - val_loss: 2.6208\n",
            "Epoch 1591/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0433 - val_loss: 2.4810\n",
            "Epoch 1592/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0396 - val_loss: 2.4938\n",
            "Epoch 1593/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0399 - val_loss: 2.5345\n",
            "Epoch 1594/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0403 - val_loss: 2.5614\n",
            "Epoch 1595/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0406 - val_loss: 2.5296\n",
            "Epoch 1596/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0405 - val_loss: 2.4703\n",
            "Epoch 1597/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0396 - val_loss: 2.3248\n",
            "Epoch 1598/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0380 - val_loss: 2.2903\n",
            "Epoch 1599/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0375 - val_loss: 2.3198\n",
            "Epoch 1600/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0381 - val_loss: 2.3248\n",
            "Epoch 1601/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0377 - val_loss: 2.3986\n",
            "Epoch 1602/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0422 - val_loss: 2.6219\n",
            "Epoch 1603/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0419 - val_loss: 2.6536\n",
            "Epoch 1604/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0432 - val_loss: 2.6168\n",
            "Epoch 1605/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0407 - val_loss: 2.3913\n",
            "Epoch 1606/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0377 - val_loss: 2.2806\n",
            "Epoch 1607/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0369 - val_loss: 2.1611\n",
            "Epoch 1608/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0376 - val_loss: 2.1204\n",
            "Epoch 1609/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0377 - val_loss: 2.0881\n",
            "Epoch 1610/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0385 - val_loss: 2.0174\n",
            "Epoch 1611/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0403 - val_loss: 1.7736\n",
            "Epoch 1612/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0471 - val_loss: 1.8129\n",
            "Epoch 1613/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0423 - val_loss: 2.1746\n",
            "Epoch 1614/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0377 - val_loss: 2.3798\n",
            "Epoch 1615/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0375 - val_loss: 2.4415\n",
            "Epoch 1616/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0403 - val_loss: 2.5085\n",
            "Epoch 1617/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0385 - val_loss: 2.3210\n",
            "Epoch 1618/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0368 - val_loss: 2.0654\n",
            "Epoch 1619/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0391 - val_loss: 1.6953\n",
            "Epoch 1620/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0480 - val_loss: 1.7113\n",
            "Epoch 1621/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0456 - val_loss: 1.8927\n",
            "Epoch 1622/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0421 - val_loss: 2.2324\n",
            "Epoch 1623/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0371 - val_loss: 2.3706\n",
            "Epoch 1624/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0374 - val_loss: 2.4006\n",
            "Epoch 1625/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0378 - val_loss: 2.4398\n",
            "Epoch 1626/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0382 - val_loss: 2.3253\n",
            "Epoch 1627/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0344 - val_loss: 1.9578\n",
            "Epoch 1628/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0407 - val_loss: 1.5395\n",
            "Epoch 1629/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0556 - val_loss: 1.5971\n",
            "Epoch 1630/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0466 - val_loss: 1.9495\n",
            "Epoch 1631/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0366 - val_loss: 2.5509\n",
            "Epoch 1632/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0449 - val_loss: 2.7269\n",
            "Epoch 1633/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0457 - val_loss: 2.3916\n",
            "Epoch 1634/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0365 - val_loss: 2.1091\n",
            "Epoch 1635/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0364 - val_loss: 2.0557\n",
            "Epoch 1636/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0362 - val_loss: 2.2137\n",
            "Epoch 1637/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0386 - val_loss: 2.2972\n",
            "Epoch 1638/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0364 - val_loss: 2.2580\n",
            "Epoch 1639/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0371 - val_loss: 2.1946\n",
            "Epoch 1640/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0371 - val_loss: 1.7559\n",
            "Epoch 1641/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0426 - val_loss: 1.6963\n",
            "Epoch 1642/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0446 - val_loss: 1.7634\n",
            "Epoch 1643/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0424 - val_loss: 1.7946\n",
            "Epoch 1644/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0404 - val_loss: 1.9086\n",
            "Epoch 1645/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0366 - val_loss: 2.1027\n",
            "Epoch 1646/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0389 - val_loss: 2.5174\n",
            "Epoch 1647/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0429 - val_loss: 2.5604\n",
            "Epoch 1648/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0368 - val_loss: 2.1306\n",
            "Epoch 1649/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0412 - val_loss: 1.6921\n",
            "Epoch 1650/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0449 - val_loss: 1.7656\n",
            "Epoch 1651/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0362 - val_loss: 2.2466\n",
            "Epoch 1652/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0387 - val_loss: 2.6163\n",
            "Epoch 1653/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0497 - val_loss: 2.6282\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1654/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0441 - val_loss: 2.0771\n",
            "Epoch 1655/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0353 - val_loss: 1.9372\n",
            "Epoch 1656/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0362 - val_loss: 1.9584\n",
            "Epoch 1657/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0365 - val_loss: 1.9625\n",
            "Epoch 1658/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0356 - val_loss: 2.0569\n",
            "Epoch 1659/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0354 - val_loss: 1.9437\n",
            "Epoch 1660/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0430 - val_loss: 1.6738\n",
            "Epoch 1661/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0409 - val_loss: 1.9761\n",
            "Epoch 1662/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0351 - val_loss: 2.1445\n",
            "Epoch 1663/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0351 - val_loss: 2.1970\n",
            "Epoch 1664/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0355 - val_loss: 2.2112\n",
            "Epoch 1665/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0357 - val_loss: 2.0981\n",
            "Epoch 1666/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0343 - val_loss: 1.9977\n",
            "Epoch 1667/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0344 - val_loss: 1.8785\n",
            "Epoch 1668/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0372 - val_loss: 1.8826\n",
            "Epoch 1669/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0360 - val_loss: 2.1013\n",
            "Epoch 1670/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0344 - val_loss: 2.1451\n",
            "Epoch 1671/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0346 - val_loss: 2.1106\n",
            "Epoch 1672/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0357 - val_loss: 2.1596\n",
            "Epoch 1673/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0343 - val_loss: 1.9523\n",
            "Epoch 1674/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0349 - val_loss: 1.6039\n",
            "Epoch 1675/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0469 - val_loss: 1.5895\n",
            "Epoch 1676/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0437 - val_loss: 1.8564\n",
            "Epoch 1677/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0374 - val_loss: 2.4489\n",
            "Epoch 1678/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0398 - val_loss: 2.5577\n",
            "Epoch 1679/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0413 - val_loss: 2.4177\n",
            "Epoch 1680/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0381 - val_loss: 2.1102\n",
            "Epoch 1681/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0364 - val_loss: 1.8898\n",
            "Epoch 1682/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0358 - val_loss: 1.9811\n",
            "Epoch 1683/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0349 - val_loss: 2.2257\n",
            "Epoch 1684/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0346 - val_loss: 2.2734\n",
            "Epoch 1685/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0348 - val_loss: 2.2688\n",
            "Epoch 1686/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0350 - val_loss: 2.2493\n",
            "Epoch 1687/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0351 - val_loss: 2.2778\n",
            "Epoch 1688/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0348 - val_loss: 2.0683\n",
            "Epoch 1689/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0417 - val_loss: 1.5375\n",
            "Epoch 1690/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0513 - val_loss: 1.5881\n",
            "Epoch 1691/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0391 - val_loss: 2.1912\n",
            "Epoch 1692/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0423 - val_loss: 2.5857\n",
            "Epoch 1693/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0423 - val_loss: 2.4345\n",
            "Epoch 1694/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0406 - val_loss: 2.1057\n",
            "Epoch 1695/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0352 - val_loss: 2.0676\n",
            "Epoch 1696/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0322 - val_loss: 2.2822\n",
            "Epoch 1697/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0348 - val_loss: 2.5193\n",
            "Epoch 1698/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0438 - val_loss: 2.4044\n",
            "Epoch 1699/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0394 - val_loss: 1.6238\n",
            "Epoch 1700/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0406 - val_loss: 1.5636\n",
            "Epoch 1701/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0425 - val_loss: 1.6862\n",
            "Epoch 1702/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0339 - val_loss: 2.0327\n",
            "Epoch 1703/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0331 - val_loss: 2.5102\n",
            "Epoch 1704/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0424 - val_loss: 2.4259\n",
            "Epoch 1705/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0389 - val_loss: 2.0499\n",
            "Epoch 1706/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0339 - val_loss: 1.9203\n",
            "Epoch 1707/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0352 - val_loss: 2.1153\n",
            "Epoch 1708/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0335 - val_loss: 2.0531\n",
            "Epoch 1709/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0340 - val_loss: 1.9649\n",
            "Epoch 1710/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0329 - val_loss: 1.9182\n",
            "Epoch 1711/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0334 - val_loss: 1.9154\n",
            "Epoch 1712/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0339 - val_loss: 1.9471\n",
            "Epoch 1713/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0336 - val_loss: 2.0009\n",
            "Epoch 1714/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0328 - val_loss: 2.1696\n",
            "Epoch 1715/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0368 - val_loss: 2.4098\n",
            "Epoch 1716/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0378 - val_loss: 2.3027\n",
            "Epoch 1717/2000\n",
            "16/16 [==============================] - 0s 978us/step - loss: 0.0360 - val_loss: 1.8329\n",
            "Epoch 1718/2000\n",
            "16/16 [==============================] - 0s 558us/step - loss: 0.0392 - val_loss: 1.5820\n",
            "Epoch 1719/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0415 - val_loss: 1.6988\n",
            "Epoch 1720/2000\n",
            "16/16 [==============================] - 0s 182us/step - loss: 0.0382 - val_loss: 1.9022\n",
            "Epoch 1721/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0363 - val_loss: 1.7215\n",
            "Epoch 1722/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0352 - val_loss: 1.8315\n",
            "Epoch 1723/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0327 - val_loss: 1.9746\n",
            "Epoch 1724/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0355 - val_loss: 2.0618\n",
            "Epoch 1725/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0326 - val_loss: 1.9014\n",
            "Epoch 1726/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0332 - val_loss: 1.8130\n",
            "Epoch 1727/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0342 - val_loss: 1.7690\n",
            "Epoch 1728/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0343 - val_loss: 1.9032\n",
            "Epoch 1729/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0326 - val_loss: 2.3105\n",
            "Epoch 1730/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0387 - val_loss: 2.4348\n",
            "Epoch 1731/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0394 - val_loss: 2.3326\n",
            "Epoch 1732/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0348 - val_loss: 2.0143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1733/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0337 - val_loss: 1.8117\n",
            "Epoch 1734/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0330 - val_loss: 1.8803\n",
            "Epoch 1735/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0323 - val_loss: 1.9288\n",
            "Epoch 1736/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0322 - val_loss: 1.9393\n",
            "Epoch 1737/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0318 - val_loss: 1.9138\n",
            "Epoch 1738/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0335 - val_loss: 2.0182\n",
            "Epoch 1739/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0317 - val_loss: 2.2017\n",
            "Epoch 1740/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0368 - val_loss: 2.3468\n",
            "Epoch 1741/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0367 - val_loss: 2.1317\n",
            "Epoch 1742/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0318 - val_loss: 1.9641\n",
            "Epoch 1743/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0315 - val_loss: 1.9052\n",
            "Epoch 1744/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0321 - val_loss: 1.8832\n",
            "Epoch 1745/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0320 - val_loss: 2.0429\n",
            "Epoch 1746/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0327 - val_loss: 2.0761\n",
            "Epoch 1747/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0313 - val_loss: 2.2139\n",
            "Epoch 1748/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0331 - val_loss: 2.1479\n",
            "Epoch 1749/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0326 - val_loss: 2.0115\n",
            "Epoch 1750/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0317 - val_loss: 1.8911\n",
            "Epoch 1751/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0321 - val_loss: 1.8766\n",
            "Epoch 1752/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0322 - val_loss: 1.9576\n",
            "Epoch 1753/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0312 - val_loss: 2.0241\n",
            "Epoch 1754/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0314 - val_loss: 2.0552\n",
            "Epoch 1755/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0314 - val_loss: 2.1278\n",
            "Epoch 1756/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0313 - val_loss: 2.2950\n",
            "Epoch 1757/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0348 - val_loss: 2.3985\n",
            "Epoch 1758/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0388 - val_loss: 2.3604\n",
            "Epoch 1759/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0364 - val_loss: 2.0222\n",
            "Epoch 1760/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0373 - val_loss: 1.6336\n",
            "Epoch 1761/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0365 - val_loss: 1.5787\n",
            "Epoch 1762/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0384 - val_loss: 1.5005\n",
            "Epoch 1763/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0417 - val_loss: 1.6546\n",
            "Epoch 1764/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0348 - val_loss: 1.8813\n",
            "Epoch 1765/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0306 - val_loss: 1.7070\n",
            "Epoch 1766/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0332 - val_loss: 1.6396\n",
            "Epoch 1767/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0344 - val_loss: 1.7039\n",
            "Epoch 1768/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0320 - val_loss: 1.8688\n",
            "Epoch 1769/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0337 - val_loss: 1.9362\n",
            "Epoch 1770/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0339 - val_loss: 1.4848\n",
            "Epoch 1771/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0398 - val_loss: 1.4515\n",
            "Epoch 1772/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0390 - val_loss: 1.7013\n",
            "Epoch 1773/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0352 - val_loss: 1.8857\n",
            "Epoch 1774/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0316 - val_loss: 1.6122\n",
            "Epoch 1775/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0346 - val_loss: 1.6376\n",
            "Epoch 1776/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0337 - val_loss: 1.6956\n",
            "Epoch 1777/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0336 - val_loss: 1.5327\n",
            "Epoch 1778/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0351 - val_loss: 1.6864\n",
            "Epoch 1779/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0289 - val_loss: 2.1842\n",
            "Epoch 1780/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0384 - val_loss: 2.3411\n",
            "Epoch 1781/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0387 - val_loss: 2.0143\n",
            "Epoch 1782/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0296 - val_loss: 1.7117\n",
            "Epoch 1783/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0352 - val_loss: 1.4953\n",
            "Epoch 1784/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0374 - val_loss: 1.6315\n",
            "Epoch 1785/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0293 - val_loss: 2.1812\n",
            "Epoch 1786/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0392 - val_loss: 2.3376\n",
            "Epoch 1787/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0375 - val_loss: 1.9469\n",
            "Epoch 1788/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0297 - val_loss: 1.6754\n",
            "Epoch 1789/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0314 - val_loss: 1.7087\n",
            "Epoch 1790/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0335 - val_loss: 1.6973\n",
            "Epoch 1791/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0303 - val_loss: 1.8797\n",
            "Epoch 1792/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0317 - val_loss: 2.0587\n",
            "Epoch 1793/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0320 - val_loss: 2.0330\n",
            "Epoch 1794/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0326 - val_loss: 1.8742\n",
            "Epoch 1795/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0316 - val_loss: 2.0329\n",
            "Epoch 1796/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0311 - val_loss: 2.0390\n",
            "Epoch 1797/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0308 - val_loss: 1.9674\n",
            "Epoch 1798/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0325 - val_loss: 1.6895\n",
            "Epoch 1799/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0312 - val_loss: 1.6129\n",
            "Epoch 1800/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0366 - val_loss: 1.6054\n",
            "Epoch 1801/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0312 - val_loss: 1.8798\n",
            "Epoch 1802/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0312 - val_loss: 2.2536\n",
            "Epoch 1803/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0336 - val_loss: 2.0562\n",
            "Epoch 1804/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0298 - val_loss: 1.7706\n",
            "Epoch 1805/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0399 - val_loss: 1.2569\n",
            "Epoch 1806/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0484 - val_loss: 1.4662\n",
            "Epoch 1807/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0332 - val_loss: 1.9473\n",
            "Epoch 1808/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0310 - val_loss: 1.7368\n",
            "Epoch 1809/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0315 - val_loss: 1.5996\n",
            "Epoch 1810/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0320 - val_loss: 1.6851\n",
            "Epoch 1811/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0288 - val_loss: 1.9322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1812/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0343 - val_loss: 2.2615\n",
            "Epoch 1813/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0365 - val_loss: 2.0797\n",
            "Epoch 1814/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0313 - val_loss: 1.7630\n",
            "Epoch 1815/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0296 - val_loss: 1.7344\n",
            "Epoch 1816/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0293 - val_loss: 1.7420\n",
            "Epoch 1817/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0294 - val_loss: 1.7398\n",
            "Epoch 1818/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0295 - val_loss: 1.7296\n",
            "Epoch 1819/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0299 - val_loss: 1.7924\n",
            "Epoch 1820/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0281 - val_loss: 1.9884\n",
            "Epoch 1821/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0303 - val_loss: 1.9999\n",
            "Epoch 1822/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0296 - val_loss: 1.8568\n",
            "Epoch 1823/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0292 - val_loss: 1.6965\n",
            "Epoch 1824/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0291 - val_loss: 1.8301\n",
            "Epoch 1825/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0282 - val_loss: 2.0432\n",
            "Epoch 1826/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0329 - val_loss: 2.1431\n",
            "Epoch 1827/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0325 - val_loss: 1.9974\n",
            "Epoch 1828/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0296 - val_loss: 1.7624\n",
            "Epoch 1829/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0286 - val_loss: 1.6624\n",
            "Epoch 1830/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0304 - val_loss: 1.7490\n",
            "Epoch 1831/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0284 - val_loss: 1.8489\n",
            "Epoch 1832/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0297 - val_loss: 1.9305\n",
            "Epoch 1833/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0288 - val_loss: 1.8051\n",
            "Epoch 1834/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0296 - val_loss: 1.7483\n",
            "Epoch 1835/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0289 - val_loss: 1.8066\n",
            "Epoch 1836/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0281 - val_loss: 1.9627\n",
            "Epoch 1837/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0290 - val_loss: 2.0550\n",
            "Epoch 1838/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0324 - val_loss: 2.1165\n",
            "Epoch 1839/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0321 - val_loss: 1.6945\n",
            "Epoch 1840/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0310 - val_loss: 1.6218\n",
            "Epoch 1841/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0306 - val_loss: 1.8233\n",
            "Epoch 1842/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0285 - val_loss: 1.9242\n",
            "Epoch 1843/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0289 - val_loss: 1.7662\n",
            "Epoch 1844/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0285 - val_loss: 1.6845\n",
            "Epoch 1845/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0289 - val_loss: 1.7358\n",
            "Epoch 1846/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0307 - val_loss: 1.7303\n",
            "Epoch 1847/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0385 - val_loss: 1.4006\n",
            "Epoch 1848/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0309 - val_loss: 1.8220\n",
            "Epoch 1849/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0318 - val_loss: 2.2252\n",
            "Epoch 1850/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0417 - val_loss: 2.2470\n",
            "Epoch 1851/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0329 - val_loss: 1.7945\n",
            "Epoch 1852/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0274 - val_loss: 1.5687\n",
            "Epoch 1853/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0310 - val_loss: 1.5001\n",
            "Epoch 1854/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0313 - val_loss: 1.6464\n",
            "Epoch 1855/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0282 - val_loss: 1.8604\n",
            "Epoch 1856/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0277 - val_loss: 1.9629\n",
            "Epoch 1857/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0293 - val_loss: 1.9463\n",
            "Epoch 1858/2000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0300 - val_loss: 1.9036\n",
            "Epoch 1859/2000\n",
            "16/16 [==============================] - 0s 373us/step - loss: 0.0274 - val_loss: 1.7148\n",
            "Epoch 1860/2000\n",
            "16/16 [==============================] - 0s 115us/step - loss: 0.0281 - val_loss: 1.6809\n",
            "Epoch 1861/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0290 - val_loss: 1.5891\n",
            "Epoch 1862/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0291 - val_loss: 1.3481\n",
            "Epoch 1863/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0383 - val_loss: 1.4556\n",
            "Epoch 1864/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0304 - val_loss: 1.7560\n",
            "Epoch 1865/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0264 - val_loss: 1.9993\n",
            "Epoch 1866/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0322 - val_loss: 2.2742\n",
            "Epoch 1867/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0355 - val_loss: 2.0164\n",
            "Epoch 1868/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0301 - val_loss: 1.7396\n",
            "Epoch 1869/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0285 - val_loss: 1.7686\n",
            "Epoch 1870/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0305 - val_loss: 1.8844\n",
            "Epoch 1871/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0292 - val_loss: 1.3874\n",
            "Epoch 1872/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0361 - val_loss: 1.3961\n",
            "Epoch 1873/2000\n",
            "16/16 [==============================] - 0s 622us/step - loss: 0.0307 - val_loss: 1.7469\n",
            "Epoch 1874/2000\n",
            "16/16 [==============================] - 0s 561us/step - loss: 0.0292 - val_loss: 1.9911\n",
            "Epoch 1875/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0311 - val_loss: 1.9600\n",
            "Epoch 1876/2000\n",
            "16/16 [==============================] - 0s 561us/step - loss: 0.0285 - val_loss: 1.8011\n",
            "Epoch 1877/2000\n",
            "16/16 [==============================] - 0s 509us/step - loss: 0.0266 - val_loss: 1.5500\n",
            "Epoch 1878/2000\n",
            "16/16 [==============================] - 0s 498us/step - loss: 0.0311 - val_loss: 1.1693\n",
            "Epoch 1879/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0440 - val_loss: 1.3427\n",
            "Epoch 1880/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0318 - val_loss: 1.7118\n",
            "Epoch 1881/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0264 - val_loss: 1.8584\n",
            "Epoch 1882/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0285 - val_loss: 1.6714\n",
            "Epoch 1883/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0344 - val_loss: 1.1740\n",
            "Epoch 1884/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0425 - val_loss: 1.3095\n",
            "Epoch 1885/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0328 - val_loss: 1.7512\n",
            "Epoch 1886/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0274 - val_loss: 1.9901\n",
            "Epoch 1887/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0327 - val_loss: 1.9522\n",
            "Epoch 1888/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0321 - val_loss: 1.6021\n",
            "Epoch 1889/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0273 - val_loss: 1.6756\n",
            "Epoch 1890/2000\n",
            "16/16 [==============================] - 0s 435us/step - loss: 0.0268 - val_loss: 1.9669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1891/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0305 - val_loss: 2.0408\n",
            "Epoch 1892/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0314 - val_loss: 1.7252\n",
            "Epoch 1893/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0263 - val_loss: 1.5207\n",
            "Epoch 1894/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0271 - val_loss: 1.7209\n",
            "Epoch 1895/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0290 - val_loss: 1.8695\n",
            "Epoch 1896/2000\n",
            "16/16 [==============================] - 0s 686us/step - loss: 0.0283 - val_loss: 1.7299\n",
            "Epoch 1897/2000\n",
            "16/16 [==============================] - 0s 561us/step - loss: 0.0272 - val_loss: 1.7959\n",
            "Epoch 1898/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0268 - val_loss: 1.9106\n",
            "Epoch 1899/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0284 - val_loss: 1.8858\n",
            "Epoch 1900/2000\n",
            "16/16 [==============================] - 0s 500us/step - loss: 0.0286 - val_loss: 1.9102\n",
            "Epoch 1901/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0309 - val_loss: 1.9990\n",
            "Epoch 1902/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0271 - val_loss: 1.6647\n",
            "Epoch 1903/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0297 - val_loss: 1.3154\n",
            "Epoch 1904/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0366 - val_loss: 1.2057\n",
            "Epoch 1905/2000\n",
            "16/16 [==============================] - 0s 562us/step - loss: 0.0416 - val_loss: 1.4124\n",
            "Epoch 1906/2000\n",
            "16/16 [==============================] - 0s 500us/step - loss: 0.0252 - val_loss: 2.1125\n",
            "Epoch 1907/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0410 - val_loss: 2.2016\n",
            "Epoch 1908/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0307 - val_loss: 1.5738\n",
            "Epoch 1909/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0276 - val_loss: 1.5017\n",
            "Epoch 1910/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0278 - val_loss: 1.6490\n",
            "Epoch 1911/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0257 - val_loss: 1.7590\n",
            "Epoch 1912/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0267 - val_loss: 1.8272\n",
            "Epoch 1913/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0268 - val_loss: 1.9474\n",
            "Epoch 1914/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0288 - val_loss: 1.8934\n",
            "Epoch 1915/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0271 - val_loss: 1.6082\n",
            "Epoch 1916/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0269 - val_loss: 1.6518\n",
            "Epoch 1917/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0263 - val_loss: 1.6950\n",
            "Epoch 1918/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0291 - val_loss: 1.9104\n",
            "Epoch 1919/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0282 - val_loss: 1.7266\n",
            "Epoch 1920/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0276 - val_loss: 1.4918\n",
            "Epoch 1921/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0282 - val_loss: 1.5448\n",
            "Epoch 1922/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0276 - val_loss: 1.7610\n",
            "Epoch 1923/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0258 - val_loss: 1.8884\n",
            "Epoch 1924/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0279 - val_loss: 1.9156\n",
            "Epoch 1925/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0283 - val_loss: 1.9276\n",
            "Epoch 1926/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0278 - val_loss: 1.8825\n",
            "Epoch 1927/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0277 - val_loss: 1.6927\n",
            "Epoch 1928/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0261 - val_loss: 1.7241\n",
            "Epoch 1929/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0268 - val_loss: 1.9233\n",
            "Epoch 1930/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0285 - val_loss: 1.7988\n",
            "Epoch 1931/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0264 - val_loss: 1.6767\n",
            "Epoch 1932/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0260 - val_loss: 1.5576\n",
            "Epoch 1933/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0268 - val_loss: 1.6716\n",
            "Epoch 1934/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0268 - val_loss: 1.6384\n",
            "Epoch 1935/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0234 - val_loss: 1.2313\n",
            "Epoch 1936/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0394 - val_loss: 1.2314\n",
            "Epoch 1937/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0356 - val_loss: 1.6403\n",
            "Epoch 1938/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0250 - val_loss: 1.8091\n",
            "Epoch 1939/2000\n",
            "16/16 [==============================] - 0s 59us/step - loss: 0.0261 - val_loss: 1.6630\n",
            "Epoch 1940/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0265 - val_loss: 1.5781\n",
            "Epoch 1941/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0247 - val_loss: 1.8150\n",
            "Epoch 1942/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0269 - val_loss: 1.7814\n",
            "Epoch 1943/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0261 - val_loss: 1.6297\n",
            "Epoch 1944/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0276 - val_loss: 1.3512\n",
            "Epoch 1945/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0304 - val_loss: 1.4781\n",
            "Epoch 1946/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0257 - val_loss: 1.6518\n",
            "Epoch 1947/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0252 - val_loss: 1.7244\n",
            "Epoch 1948/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0254 - val_loss: 1.6468\n",
            "Epoch 1949/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0254 - val_loss: 1.6257\n",
            "Epoch 1950/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0243 - val_loss: 1.8193\n",
            "Epoch 1951/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0266 - val_loss: 1.6821\n",
            "Epoch 1952/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0244 - val_loss: 1.4984\n",
            "Epoch 1953/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0265 - val_loss: 1.5392\n",
            "Epoch 1954/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0285 - val_loss: 1.8319\n",
            "Epoch 1955/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0269 - val_loss: 1.7984\n",
            "Epoch 1956/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0270 - val_loss: 1.9578\n",
            "Epoch 1957/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0278 - val_loss: 1.7652\n",
            "Epoch 1958/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0275 - val_loss: 1.5621\n",
            "Epoch 1959/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0257 - val_loss: 1.5646\n",
            "Epoch 1960/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0252 - val_loss: 1.5713\n",
            "Epoch 1961/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0276 - val_loss: 1.4910\n",
            "Epoch 1962/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0268 - val_loss: 1.5806\n",
            "Epoch 1963/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0242 - val_loss: 1.7685\n",
            "Epoch 1964/2000\n",
            "16/16 [==============================] - 0s 975us/step - loss: 0.0258 - val_loss: 1.9496\n",
            "Epoch 1965/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0285 - val_loss: 1.9165\n",
            "Epoch 1966/2000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0266 - val_loss: 1.6774\n",
            "Epoch 1967/2000\n",
            "16/16 [==============================] - 0s 411us/step - loss: 0.0268 - val_loss: 1.4661\n",
            "Epoch 1968/2000\n",
            "16/16 [==============================] - 0s 526us/step - loss: 0.0300 - val_loss: 1.5307\n",
            "Epoch 1969/2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 436us/step - loss: 0.0240 - val_loss: 2.0649\n",
            "Epoch 1970/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0344 - val_loss: 2.1198\n",
            "Epoch 1971/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0323 - val_loss: 1.6232\n",
            "Epoch 1972/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0255 - val_loss: 1.6049\n",
            "Epoch 1973/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0264 - val_loss: 1.7543\n",
            "Epoch 1974/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0268 - val_loss: 1.5372\n",
            "Epoch 1975/2000\n",
            "16/16 [==============================] - 0s 499us/step - loss: 0.0258 - val_loss: 1.6020\n",
            "Epoch 1976/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0245 - val_loss: 1.6094\n",
            "Epoch 1977/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0244 - val_loss: 1.5688\n",
            "Epoch 1978/2000\n",
            "16/16 [==============================] - 0s 73us/step - loss: 0.0247 - val_loss: 1.6208\n",
            "Epoch 1979/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0247 - val_loss: 1.6367\n",
            "Epoch 1980/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0245 - val_loss: 1.5669\n",
            "Epoch 1981/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0235 - val_loss: 1.7904\n",
            "Epoch 1982/2000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 0.0295 - val_loss: 1.8392\n",
            "Epoch 1983/2000\n",
            "16/16 [==============================] - 0s 98us/step - loss: 0.0256 - val_loss: 1.5789\n",
            "Epoch 1984/2000\n",
            "16/16 [==============================] - 0s 828us/step - loss: 0.0247 - val_loss: 1.4047\n",
            "Epoch 1985/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0259 - val_loss: 1.4631\n",
            "Epoch 1986/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0249 - val_loss: 1.6793\n",
            "Epoch 1987/2000\n",
            "16/16 [==============================] - 0s 65us/step - loss: 0.0298 - val_loss: 1.8591\n",
            "Epoch 1988/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0232 - val_loss: 1.4197\n",
            "Epoch 1989/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0343 - val_loss: 1.2387\n",
            "Epoch 1990/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0262 - val_loss: 1.8211\n",
            "Epoch 1991/2000\n",
            "16/16 [==============================] - 0s 976us/step - loss: 0.0264 - val_loss: 1.9515\n",
            "Epoch 1992/2000\n",
            "16/16 [==============================] - 0s 0us/step - loss: 0.0298 - val_loss: 1.8905\n",
            "Epoch 1993/2000\n",
            "16/16 [==============================] - 0s 983us/step - loss: 0.0272 - val_loss: 1.7146\n",
            "Epoch 1994/2000\n",
            "16/16 [==============================] - 0s 436us/step - loss: 0.0244 - val_loss: 1.4572\n",
            "Epoch 1995/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0258 - val_loss: 1.5200\n",
            "Epoch 1996/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0309 - val_loss: 1.9856\n",
            "Epoch 1997/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0310 - val_loss: 1.8848\n",
            "Epoch 1998/2000\n",
            "16/16 [==============================] - 0s 375us/step - loss: 0.0256 - val_loss: 1.5507\n",
            "Epoch 1999/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0234 - val_loss: 1.3916\n",
            "Epoch 2000/2000\n",
            "16/16 [==============================] - 0s 374us/step - loss: 0.0256 - val_loss: 1.3736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x28856de9cc8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc0e9w_MeRmo",
        "outputId": "bd5a1d89-3d8c-4cba-a641-aaabe934aa18"
      },
      "source": [
        "test_input = array([8])\n",
        "test_input = test_input.reshape((1, 1, 1))\n",
        "test_output = model.predict(test_input, verbose=0)\n",
        "print(test_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[119.94444]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC7pvN91eRmo"
      },
      "source": [
        "## Sequence Estimation for 2 Dimensional Input and One Dimensional Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgyH5fp7eRmo",
        "outputId": "d7def5de-9b3c-4e80-f74c-18885253acd7"
      },
      "source": [
        "nums = 1000\n",
        "\n",
        "X1 = list()\n",
        "X2 = list()\n",
        "X = list()\n",
        "Y = list()\n",
        "X1 = np.random.uniform(0,20,1000)\n",
        "X2 = np.random.uniform(0,20,1000)\n",
        "\n",
        "Y = [2*x1+x2 for x1,x2 in zip(X1,X2)]\n",
        "\n",
        "\n",
        "print(X1)\n",
        "print(X2)\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[13.61404994  2.12815014  9.24732253 17.91914729 13.26118389 11.49314885\n",
            " 16.72234031  1.46413553 17.39630046 16.93812883  3.99676761  0.75689572\n",
            " 18.25427115 13.63264856  5.09913165  0.50306863  9.28400176 13.28322428\n",
            " 15.43880086 11.16611436 19.74414053 18.65657158 12.52040837 12.55943378\n",
            "  3.85082828 14.51637533 13.52169277 15.81952356  8.68059616 10.8111324\n",
            " 18.99174379  4.02466257  8.91020745  0.24242416  4.08500018  5.75809102\n",
            "  3.28929346 12.8474433  17.2158367   1.37356573  1.97819735  4.99683447\n",
            "  7.62434365  5.55535834 13.61296424  7.27322231  7.812845   19.00727621\n",
            "  3.19937102  9.39071788 19.1756287   8.50550845 18.89848427 19.87583603\n",
            "  1.68961912 13.58819552 10.52566415 19.21899621  7.0140757   7.16732439\n",
            " 14.58721817  4.49875237 13.66610343  8.0406147  15.60718035 18.58234117\n",
            " 10.86120395 18.50410043  8.34555847  5.97971113 17.59931923 18.54629951\n",
            " 17.37660421  0.66070822 13.02188785  1.94368899 17.40364809 18.61628073\n",
            "  6.22123413  5.61405047  9.70092833 13.14089891 15.35931145 14.89885161\n",
            " 15.68436226 17.57414464  2.1385493   3.11992854 17.21413937  6.84747014\n",
            " 12.659823    4.69456422  8.13389125  3.69486026  5.76668506  0.52671391\n",
            "  8.70860684 19.747082    0.04933642 11.51178172  1.63460706 11.86756005\n",
            " 14.95217496  6.77943091 10.69254836 13.97378888  6.33476481  3.2194814\n",
            " 18.3311086   9.84344653 19.14339841 15.18716057 12.70178173 16.13020665\n",
            " 10.28424483 17.52076222  1.26484299  6.64860536  3.01119228  7.41873167\n",
            " 15.17563369  3.05632043  8.33447639 11.27362056 14.66342633 17.53778001\n",
            " 10.00291489  2.96905307  7.76329946  9.60539537 11.69215186  0.02263546\n",
            "  2.77030608 16.98629002  5.49317156  7.67727164  9.10133821  4.93491617\n",
            " 15.29070993  5.65638295  7.09935091  9.41368584 12.23242168 15.87256243\n",
            "  0.31060938 15.43013036 10.95735943 12.61527155 16.91587839  2.13486918\n",
            " 14.71248507 17.32440222 17.28268255  9.5213968  15.6256513   5.31976997\n",
            "  7.52713226 17.53815253  9.04540483  0.48194346 15.83119926 17.90975531\n",
            "  4.08638588 16.64799939  0.57041106  9.96126323  0.08130281  5.02896802\n",
            " 12.3174803  18.14116392  3.02120398 16.59563135 16.96076655  3.03174138\n",
            "  0.51150285  3.58073389 16.65823077 16.42816058 11.90121867  5.85391939\n",
            "  1.57731083  6.01884043 16.61354686  2.95705314 19.12938442 12.09812314\n",
            " 10.39602273 17.70613774  6.08449815 11.22136427  4.53804751  5.62102095\n",
            " 11.0483611  17.59405563  2.19360613  4.62347278  5.42377803 11.08366718\n",
            "  5.80440052  3.55052644  2.6815551   2.47440076 19.18219906  8.77134514\n",
            " 10.77016696 17.37913349 16.67851773 12.90162347  1.84898873  9.96351256\n",
            "  0.26207621  6.45644872  2.85863257 11.03818259 19.7153385  10.58571238\n",
            "  9.84770052 11.41025775 18.24532186  9.24502514  8.18796748  7.69352812\n",
            "  8.18374808  4.50679511 10.54698363  9.95688515 17.2924587  13.20148487\n",
            " 19.31461709  8.42008255 16.60612994 10.49116529 17.74139039 15.3256475\n",
            "  8.52098508 18.57317708  1.59493293  6.30007357  7.93427889 15.672875\n",
            " 15.5405273   9.7988224  10.63673654 15.07706881  0.19507205 15.89469138\n",
            "  7.18320602 12.41590059 13.73325453 17.54715055 17.11318257  0.11208252\n",
            "  0.82369042 16.11693152 12.80013668  0.2336301  18.51171093 11.70937618\n",
            " 17.39372067 18.35724951  0.91994126 19.83543053  2.29863804  1.06997252\n",
            "  7.07415982 14.36517541  0.30561183 16.74271237 16.0358411  13.65606565\n",
            " 17.29994306  2.04427011  0.41490125 18.53391996 18.77045891 12.85922591\n",
            "  6.75128994  7.87048566 17.6261411   9.31971649  4.78177178 14.08297663\n",
            "  4.84606284  5.63537625  6.16928415  9.11212899 10.07889958  2.3506585\n",
            " 18.80192965 10.25923159 18.62932017 18.00414083 13.0855788  16.57925038\n",
            " 17.29035014  8.55718153 18.53568105 10.33251859 18.63673225  7.92277085\n",
            "  3.79740166  3.71421934  3.69577478  3.41164934 15.24506486  9.72536637\n",
            " 10.07737259  4.49817918 11.55742878  3.99281256  2.43917845  4.61430023\n",
            " 17.3550887   7.88675166 18.55767969 14.20777481 10.90152174 18.87673133\n",
            "  4.04757679 12.9235802  10.53574284 13.10312098 12.21054238 19.01531981\n",
            "  2.78747402  1.31720289 19.46849467  3.28486781 16.45760978 19.94231579\n",
            "  1.29917174 18.22821974 14.8939045   9.3265901  13.04866983  6.10452573\n",
            "  6.6374758   3.92027962  6.39403861 10.94427219  0.98202088 13.53543114\n",
            "  0.93350822 19.34313779  3.4121653   2.39859408 17.78989304 13.32781478\n",
            " 19.443814   14.27358478 15.85464958  2.40999747 18.30865977  5.87726485\n",
            "  8.15317456 19.12730698  3.36023614 10.70320807  8.08832365  3.70899695\n",
            " 11.31133166 11.35015137  0.24681356  6.75344474  8.55941209  1.15253382\n",
            "  4.39693538  2.55351158  9.81231723  5.34769337  3.80755121  4.63191885\n",
            "  8.51686148  2.82717813  2.86767669 19.53259839  0.1283621   2.16989609\n",
            "  6.15911637 18.69273786  4.13427078 16.36845461  4.85497726  7.91371564\n",
            " 13.45094875 15.94135411 16.11994159  7.32823847  9.87475036  7.56840772\n",
            " 14.56780837 17.43312535 18.27662178  3.22454961 14.09851693  0.94118639\n",
            "  8.07555994 18.99354437 19.30310131  8.80976753  8.45144737  6.97402043\n",
            "  5.34803301  4.72689192 12.50093656  3.51145447 12.47168283 15.85829782\n",
            " 18.67920894 14.6671149   2.52086052  0.92080966 12.48878012  1.47698309\n",
            "  3.24435745  7.90714896 17.6399645  11.93551027 19.61476273  1.2728614\n",
            " 12.89973894  5.1843371   5.41203685 16.93977938 11.62228956  1.66811538\n",
            "  1.92766135 19.97893245  5.68765874 11.98237243 14.52815282 15.19466561\n",
            "  8.30322165  5.31620742 15.93382711  7.72329026  4.31829945 16.19849604\n",
            " 16.96642755 14.64403493  0.50563626 14.42855337 17.57708279 15.30911734\n",
            "  1.98801476 15.3407827   1.74544697  2.93014817 18.61869992  2.66829793\n",
            " 12.42005008  9.9161118  13.77545587 19.21289686 13.14106165 14.50787728\n",
            "  8.16425324 13.09678175 11.74068135  0.31719073  0.45712179 13.9384059\n",
            "  9.52282405  1.00599241 16.66570196 16.73960641 19.0414506   3.3035191\n",
            "  4.97272753  8.42053878  7.32304099  3.53206592 17.67565034 10.70600227\n",
            " 11.19421952  0.2945973  19.41410701  0.57100303  3.0159208  12.43483094\n",
            "  3.62827635 11.51074794  4.95710388  8.07628533  9.42376423 13.39537209\n",
            " 18.31995141  7.13793072  1.79037995 14.3193151   8.88321363  1.78744158\n",
            " 12.06100225  6.15559664  4.44569182 15.28490567  8.67377788  1.63420915\n",
            " 17.62645116 14.51214064 15.09221577  1.95256794  6.36590891 14.73198626\n",
            "  2.4612504   1.27123207 19.2013018  16.96107599  8.37561654 14.26935227\n",
            " 14.22116836 19.91540572  0.8512922  12.41753602 17.16438977 10.22646464\n",
            " 19.45452025 11.0125068  19.57066132 19.44130898 10.62124816  7.72555066\n",
            " 19.21889993 18.26914694  4.48522861 10.78564778  7.04714056 14.81062669\n",
            " 12.97331229  8.75496329 18.06717159  3.75503121 11.88040725  0.71228308\n",
            " 10.09191272 17.27693006  8.38082     7.0501952   6.9911849  18.53096333\n",
            " 12.79341605  7.22814444 13.51271421 13.35019843 11.41657448  8.79976582\n",
            " 13.36582729 15.22675325 18.36755535  9.71729474  8.70740893 16.50226187\n",
            "  7.54139284 18.96593131 15.60276378  4.3887225  18.99434147  3.583571\n",
            " 13.47588487 15.38733718 15.93834407 12.38719645 16.74772022  1.45978004\n",
            " 18.20145296 19.94369475 11.98720497 19.20238904  1.55414481 14.19217047\n",
            "  1.94905106  7.44374057  4.80826711 18.47280328 18.45990973  0.29415826\n",
            "  9.48082648  6.45279687 12.98671227  6.84773872 15.6704014  18.74768354\n",
            " 19.77313638 16.01391035 12.61077464  5.33819418 19.32224853  7.67661348\n",
            "  7.952951    5.44897728  2.56401645 11.42947641 11.26467364 16.07221734\n",
            "  0.30071993 13.91330418 19.98614539 16.59537718 15.14867482  9.69109183\n",
            " 19.2104592   4.89822648  9.27789993  4.75984371  0.12952102 15.14919998\n",
            " 17.24685623  5.59998351 18.40012977 13.90605095 14.59289481 10.79928167\n",
            " 14.92124656 19.02069106 12.89408324 14.96733452  6.85733212  7.7323634\n",
            " 12.9954408   1.41574698  4.36071032 15.37072725 16.26528829  7.36322219\n",
            " 18.47004216  4.02433295  0.69812173  2.44109654 17.28651655 10.07643743\n",
            "  1.09837932  1.73260653  0.71027892 18.59336854 19.57027499  4.7595388\n",
            " 10.28813589 18.72758728  9.06977229  7.88640146 18.83436272  7.72313619\n",
            " 17.15274595  0.83404117  7.39770571  6.72421801  7.20192024 10.63058624\n",
            " 13.43365529 18.93101029  4.29137727  6.95616994  3.91561043  5.44153234\n",
            "  3.1367767   1.20969377  1.47681196 16.59079517 11.57752385 17.05654107\n",
            " 16.33785799  4.99867845 17.0704537   7.41905106 10.01917601  5.53189382\n",
            " 12.0493993   8.09575165  1.94738062  1.93966811 18.580848    6.31537087\n",
            "  7.1970138  10.91429163 17.74080032 12.2183191  11.17302371  5.35385883\n",
            " 12.60911429  8.84379786 11.73369847  6.96193775  6.2086927   0.08939089\n",
            " 12.47359812 15.53681574 13.08181073 17.98751166 13.34066727 15.17284615\n",
            "  9.44096648  6.60488918  0.80718943  4.02702027  5.00451821  4.03023771\n",
            "  3.04480853 16.50376039  6.83673552  8.6659946  18.13342502  6.13371904\n",
            "  1.890043    9.63435484  6.92841286  1.31418551 17.99942399  8.11724131\n",
            " 18.40395344 10.33339464 11.43489916 16.962581    0.55990538 18.82481171\n",
            "  0.37971924 17.2583851   7.74395589  5.22347932  6.62121684 10.42431744\n",
            " 13.4727602   7.73202533  6.60500585 17.95715434 13.45424983  9.18981676\n",
            " 13.88482311  9.57002649  7.36363166  8.82679596  5.77235943 17.66074418\n",
            "  3.67402884  3.61663123 16.57380354 16.24237498  8.59005302  0.74970095\n",
            " 13.11640422 16.07478345  2.27261186 18.7983767  13.44580805  8.23503278\n",
            "  2.51923991  7.93368886  3.39835532 10.89573216 17.48102388  8.25100675\n",
            "  4.31153667  6.65361661 16.22836287 18.26708413  5.93698167 19.3328913\n",
            " 13.52550249  2.06639998  0.25546247  2.73133973 16.82597608  8.72649698\n",
            "  1.70566294  7.61493555 17.14256369  1.84744722 13.97462796  7.4693858\n",
            " 15.20173823 10.59834645 14.12271202  3.81326225  4.85235988 15.51521604\n",
            "  3.72706792  6.08223443  7.65085205  8.10965853  7.4553979   2.75232888\n",
            "  6.4254405   0.21954698 14.07737751 14.22189716 18.74120228 11.10651345\n",
            " 14.39863977 11.80529815 19.42276564  3.63367258 17.7468829  16.21754438\n",
            " 13.10016144  4.12319761 12.9011884  10.60888899  3.57615745 13.26867855\n",
            " 11.63385031  5.20050556 13.61109322 13.49618477 16.88272774 17.16737447\n",
            "  8.31180614 10.31007724  0.62019519  3.16534887 18.84856564  4.7413964\n",
            " 12.16977473 14.70553518 19.69812779  0.96210411  2.84455364 15.86458447\n",
            "  7.21516445  8.38897957 16.10467802  5.35966867 18.24907271 12.61542927\n",
            " 10.69901573  4.16772181  2.79449487  6.60738008 12.58966782  1.12198632\n",
            "  1.53675094 16.37203848  0.33494436  5.25467564 11.0682603   5.36925987\n",
            " 18.45668841  5.69166454  1.87156774 13.91667182 19.05421895 13.61480811\n",
            " 14.69211107 17.41304296  7.05493238  9.49603416 10.66345719 12.88247524\n",
            " 17.35231988 17.82800096 18.71044401 14.15616837  4.91376413  4.63502047\n",
            " 13.0359278   4.71058401  0.31287559  8.22883173 11.82433177 18.53042241\n",
            " 14.63971082 14.40058271 17.18727115 10.36047595 19.34245885  3.0346212\n",
            " 12.83872948  7.69165538  7.93710895 15.73674018 17.96482281  6.43168608\n",
            " 16.7864503   1.98594054 19.57981522  9.80745829 11.94285674  6.8284806\n",
            " 17.09466865  7.50691112 13.68496543  9.151054    0.03013947 10.6041804\n",
            " 13.97210739 11.46587351  5.20719727 12.49342386 15.12727809  7.09019556\n",
            "  1.51330825 14.01953371  2.89474525 16.33532211  0.64483526 18.4944098\n",
            "  0.19983801  5.84992983  6.73806286  2.98730975 16.48857166  9.05498275\n",
            " 10.99714827 15.38277325 16.84694635 15.85500028 17.42496613 17.01217293\n",
            "  8.07243766 12.4348513  13.96032282 10.58623992  2.69989845 16.61193573\n",
            " 16.27680895  2.52959493  2.84351795 13.3101203  17.82013663  5.49110011\n",
            "  3.70359122 15.59119384 13.53072414 17.72075213  3.67813899  6.86920008\n",
            "  3.88063964 12.39553057 10.79598222 17.04790916  5.24832829 10.32343354\n",
            " 14.2759992   4.1572811   5.03330405  9.94126494 17.86932791 16.93870906\n",
            " 17.47374252  7.34327211 18.68323677  8.25179904 12.19450548  2.03668222\n",
            "  1.23963661 13.13603178 18.16364848 13.78048017  6.93375189  5.15512294\n",
            "  2.62628021  7.70052045 10.02277641 17.00733468  0.57536935  9.64314357\n",
            "  4.5215887   3.38796635  6.69028962  1.57558754  9.79250163 15.04585651\n",
            "  1.8675367   8.73665358  3.23354754 14.17448774 13.10280381  4.02704572\n",
            "  0.79918032 18.04252196 17.63417751  8.05409139 17.34368919  1.46754118\n",
            "  2.65395417 17.90425506  8.81036384 14.96000656  3.41568074 15.91648962\n",
            "  5.60380694 15.28110603 14.2383577   9.00160211 12.89511372  3.9540415\n",
            "  9.76546593 14.5392665   3.82732774 11.98137566  8.24073946 11.12801043\n",
            " 11.15913345 12.9928876  19.41941017 14.33288557  1.16462749  7.2070201\n",
            "  0.84063796 15.94832169 13.5844915  12.34366353]\n",
            "[1.73170939e-01 3.40479077e+00 1.39827665e+01 5.42767611e-01\n",
            " 9.12558253e+00 1.94884050e+01 1.61890700e+01 1.15925206e+01\n",
            " 1.71146190e+01 9.15918420e+00 1.36961935e+00 1.22510686e+01\n",
            " 1.96945455e+01 1.57714467e+00 5.22064486e+00 3.82677644e+00\n",
            " 5.75755713e+00 1.50451355e+01 5.80376358e-01 6.16464270e+00\n",
            " 2.47219186e+00 1.34496648e+01 1.20938127e+01 8.41701970e+00\n",
            " 5.76926415e+00 1.87920450e+01 1.56929731e+01 1.04290085e+01\n",
            " 8.28015600e-01 1.41449272e+01 1.75912056e+01 1.66546056e+01\n",
            " 1.16057138e+01 6.55619781e+00 3.81620265e+00 1.88224750e+01\n",
            " 1.85229076e+01 7.39748202e+00 9.38764738e+00 7.40967787e+00\n",
            " 6.38794808e+00 5.47667860e+00 9.16139038e+00 5.58200534e+00\n",
            " 1.93116568e+01 1.28858339e+01 1.00161403e+01 2.58142123e+00\n",
            " 6.47057853e-01 1.17431521e+01 7.90208818e-02 8.79047368e+00\n",
            " 1.21798057e+01 7.00539805e+00 1.05319265e+01 1.40611699e+01\n",
            " 3.29994584e+00 8.10965082e+00 1.89219892e+01 1.33667470e+01\n",
            " 1.88056283e+01 1.26790952e+01 1.68754204e+01 1.84307939e+01\n",
            " 8.80504044e+00 5.58259779e+00 1.95023748e+01 4.05239591e+00\n",
            " 1.12885195e+01 2.67297373e+00 5.31377740e+00 5.64515309e+00\n",
            " 1.15096429e+01 1.42114483e+01 3.71631698e+00 1.86368531e+01\n",
            " 1.16865011e+00 4.29056782e+00 2.47537740e+00 5.59217106e-01\n",
            " 4.51998331e-01 1.13225963e+01 6.28058933e+00 1.66607361e+00\n",
            " 1.24921244e+01 1.25526395e+01 1.32965204e+01 1.16250104e+01\n",
            " 1.47681855e+01 1.37800146e+01 4.10534135e+00 6.74267536e+00\n",
            " 1.62724830e+01 1.36356303e+01 2.71865465e+00 9.41158772e+00\n",
            " 1.09909508e+01 1.55619075e+01 9.23808873e+00 9.93401815e+00\n",
            " 1.89634698e+01 1.10584899e+01 1.07416262e+01 3.84778452e+00\n",
            " 1.85379355e+01 1.57380534e+01 1.18032978e+01 4.74811580e+00\n",
            " 3.59202251e+00 5.92363768e+00 1.34468803e+01 4.15475543e+00\n",
            " 3.97520145e+00 7.66258873e+00 1.81516722e+00 1.64325704e+01\n",
            " 4.83405373e+00 3.59433414e+00 1.85497041e+01 3.88785032e+00\n",
            " 1.53494625e+00 2.62432075e+00 1.24491086e+01 6.87721480e+00\n",
            " 1.15518831e+01 1.64476413e+01 3.50788979e+00 6.98641538e+00\n",
            " 1.91143669e+01 1.56122391e+01 1.27246560e+01 8.61290794e+00\n",
            " 6.76221462e+00 9.15213807e+00 4.96895737e+00 1.43632053e+01\n",
            " 1.72294116e+00 8.91749353e+00 1.49795971e+01 7.07666450e+00\n",
            " 6.94026700e+00 1.83615256e+01 1.86638813e+01 1.20939339e+01\n",
            " 6.54899070e-01 1.60122892e+01 1.67217701e+01 1.33318117e+01\n",
            " 1.47885163e+01 1.07986224e+01 7.48204332e+00 1.49845312e+01\n",
            " 7.72477363e+00 6.81237116e+00 6.63922315e+00 1.59229737e+01\n",
            " 7.56162914e+00 8.81512568e+00 1.92859288e+00 3.82367022e+00\n",
            " 5.61398218e-01 9.65917473e+00 1.52717767e+01 1.42603290e+00\n",
            " 9.91041204e+00 1.63155320e+00 4.87460675e-01 3.97787985e+00\n",
            " 7.88753075e+00 9.75799876e+00 1.89532289e+01 1.65117191e+01\n",
            " 1.26419427e+01 1.13012498e+01 1.26922206e+01 4.15312857e+00\n",
            " 1.50136168e+01 1.32123950e+01 1.40210323e+01 5.30922001e-01\n",
            " 1.78241432e+01 1.98889268e+01 1.85933224e+01 2.25844867e-01\n",
            " 4.72406907e-01 1.18568032e+01 1.80459941e+01 6.87216349e+00\n",
            " 1.55820947e+01 1.01655905e+00 1.88165879e+01 1.64257786e+01\n",
            " 1.25932545e+01 8.42512401e+00 1.60974380e+01 7.10654788e-01\n",
            " 7.14571497e+00 1.56785686e+01 6.29194705e+00 1.97539443e+01\n",
            " 5.11724514e+00 1.62578752e+01 6.66319068e+00 1.11012444e+01\n",
            " 1.20415207e+01 1.08159913e+01 4.81502256e+00 3.43003352e+00\n",
            " 1.24817070e+01 1.93102570e-01 1.70479788e+01 3.57808659e-01\n",
            " 9.69352512e+00 1.23503533e+01 1.26411563e+01 1.51405935e+01\n",
            " 1.13999195e+01 2.66078158e+00 6.13410262e+00 1.67629735e+01\n",
            " 1.71628161e+01 1.09515133e+01 1.89149971e+01 1.78933994e+01\n",
            " 1.27555106e+01 1.82093162e+01 1.53858719e+01 3.12872670e+00\n",
            " 6.41025617e+00 1.12314459e+01 1.56671747e+01 1.48460988e+01\n",
            " 1.28986420e+01 8.29148712e+00 1.52741452e+01 6.40599947e+00\n",
            " 1.86975003e+01 7.93646758e+00 1.34990552e+01 9.74486295e+00\n",
            " 1.90598619e+01 2.07895021e+00 1.03547483e+00 8.41831089e+00\n",
            " 1.38458368e+01 9.81172050e+00 1.46169598e+00 1.67796713e+01\n",
            " 2.74499992e+00 1.04743065e+01 5.41875999e-01 1.14161767e+01\n",
            " 9.12139823e+00 1.98300743e+01 7.53715448e+00 1.28817070e+01\n",
            " 1.59012399e+01 5.43533618e+00 1.95424706e+01 1.18270147e+01\n",
            " 7.42200480e+00 1.62287839e+01 1.92548885e+01 2.12140381e+00\n",
            " 1.38238216e+01 6.55188043e+00 6.14971958e+00 1.09466161e+01\n",
            " 5.81467695e+00 1.98977335e+01 2.99683313e+00 3.03728495e-01\n",
            " 1.99606398e+01 1.22616625e+01 2.01849531e+00 2.09935646e+00\n",
            " 6.90828880e-01 1.08348573e+01 1.19018127e+01 9.03091245e+00\n",
            " 1.79064382e+01 1.19784609e+01 1.62945007e+01 7.85572713e+00\n",
            " 1.87123823e+01 1.22567766e+01 7.63984579e+00 6.93636228e+00\n",
            " 1.91023711e+01 2.00195959e+00 1.39875116e+01 1.20924890e+01\n",
            " 1.06823945e+00 9.25786889e+00 1.06477375e+01 4.73804272e+00\n",
            " 1.79676137e+01 1.12553670e+00 6.25800013e+00 1.20513088e+01\n",
            " 1.26122309e+00 9.75893388e+00 1.70995200e+01 4.53036025e+00\n",
            " 4.89995320e+00 2.97666857e+00 7.42038560e+00 1.66408430e+01\n",
            " 1.98521189e+01 1.62307158e+01 5.70218323e+00 1.43938509e+01\n",
            " 7.23289865e+00 7.80820462e+00 2.74827026e+00 1.41359292e+01\n",
            " 1.96723782e+01 1.38690600e+00 1.51346027e+01 3.22267796e+00\n",
            " 3.04495938e+00 1.95480857e+01 1.02315315e+01 8.74933295e-01\n",
            " 3.19473640e+00 2.57792058e+00 1.75335908e+01 1.68151049e+01\n",
            " 1.51139526e+01 1.45814122e+01 4.15609824e-01 2.24249683e+00\n",
            " 7.47236373e-01 4.28348188e+00 1.01743146e+01 1.04452152e+00\n",
            " 2.43333437e+00 1.49176469e-01 5.28604467e+00 1.21473076e+00\n",
            " 2.38288996e+00 1.79272069e+01 1.53784900e+01 7.25168685e+00\n",
            " 3.56434355e+00 1.98051403e+01 3.34235871e+00 6.84199253e+00\n",
            " 1.31522270e+01 9.28472588e+00 1.76083173e+01 4.01909735e+00\n",
            " 4.19516095e+00 3.51562863e+00 1.71834857e+01 1.91657945e+01\n",
            " 1.01360419e+00 1.30480020e+01 2.92384288e+00 1.24451353e+01\n",
            " 1.05206066e+01 6.00613687e+00 1.25877068e+01 1.97229034e+00\n",
            " 1.69164417e+01 8.95059955e+00 1.45044975e+01 3.33315899e+00\n",
            " 2.08608654e+00 1.48136810e+01 1.92654859e+01 1.56877890e+01\n",
            " 1.27650356e+00 6.69711802e+00 1.20331583e+01 8.68215817e+00\n",
            " 1.65590905e+01 3.65688367e+00 1.46940144e+01 1.22858732e-01\n",
            " 1.36569695e+01 1.04179406e+01 1.39219557e+01 2.47641456e+00\n",
            " 1.58182521e+01 1.58895673e+01 7.64801579e+00 6.55461565e+00\n",
            " 3.54942380e+00 1.22135537e+01 1.59818331e+01 1.41602509e+01\n",
            " 1.76991450e+01 9.99993438e+00 6.23674460e-01 1.41415863e+01\n",
            " 1.88174295e+01 1.89751526e+01 1.69744876e+00 7.76534066e+00\n",
            " 8.60617607e+00 1.75148482e+01 8.71246409e+00 1.41019293e+01\n",
            " 1.06512176e+01 1.98026817e+01 1.07168024e+01 9.37783635e+00\n",
            " 4.49123070e+00 3.61571872e+00 1.63578968e+01 1.77690521e+00\n",
            " 4.13764955e+00 1.19695732e+01 9.55416995e+00 1.91248659e+01\n",
            " 3.89980345e+00 5.70096158e+00 3.08128923e+00 1.22805390e+01\n",
            " 9.00967917e-01 1.03415524e+01 4.84334208e+00 1.39813333e+01\n",
            " 9.14259264e+00 5.46018360e+00 4.61588000e+00 1.91893486e-02\n",
            " 7.95852513e+00 5.76847567e-01 1.49944345e+01 3.94779788e+00\n",
            " 7.92019160e-01 1.17282480e+01 1.52860700e+01 6.10963512e+00\n",
            " 7.48067211e+00 1.44832203e+01 1.98373448e+01 1.64662831e+01\n",
            " 1.24521473e+01 1.58196065e+01 1.46173307e+01 1.58241910e+01\n",
            " 1.63047428e+01 1.91676150e+01 3.73601376e+00 5.17856841e+00\n",
            " 2.36543291e+00 2.02864954e+00 3.74326161e-01 3.36801025e+00\n",
            " 8.94716936e+00 8.73403648e+00 4.81524015e+00 9.00755399e+00\n",
            " 3.39048292e+00 1.37718209e+01 1.57262328e+01 2.86837781e+00\n",
            " 1.11701775e+01 1.16685965e+01 1.55413864e+01 4.18617414e+00\n",
            " 7.69092756e+00 4.15951655e+00 8.73205230e-01 7.77342448e-02\n",
            " 1.41586138e+01 1.10361399e+01 7.45624569e+00 5.57859008e+00\n",
            " 2.15323587e+00 6.19504821e-01 1.64901379e+01 1.46140982e+01\n",
            " 9.57663499e+00 5.20816206e+00 1.73183561e+01 1.17715198e+01\n",
            " 1.77231354e+01 1.43185451e+01 6.21508243e+00 1.08601801e+01\n",
            " 5.69382873e+00 4.57885126e+00 1.95902160e+01 1.20971879e+00\n",
            " 3.75857534e+00 1.84229793e+01 1.25669425e+01 7.13844557e+00\n",
            " 1.58243011e+01 2.32965719e+00 1.88323859e+01 2.14439557e+00\n",
            " 1.29177685e+01 1.49057439e+01 1.48609352e+01 1.72254682e+01\n",
            " 1.29780997e+01 1.33356493e+00 3.45768535e+00 1.73000840e+01\n",
            " 9.63904469e+00 1.52506084e+01 1.25732460e+01 1.71208042e+01\n",
            " 1.05893087e+01 1.93471421e+00 1.57790637e+01 8.78630005e+00\n",
            " 2.21509284e+00 1.82562691e+01 1.64977797e+00 1.99604331e+01\n",
            " 1.40831745e+01 1.00407998e+01 8.88246124e+00 1.37975652e+01\n",
            " 1.26335007e+01 9.64014899e+00 6.66154785e+00 1.94558755e+01\n",
            " 2.02544793e+00 5.55239588e-02 1.41503145e+01 1.09059098e+01\n",
            " 1.99416300e+01 1.01808478e+01 1.52254520e+01 6.32435938e+00\n",
            " 9.43675698e+00 1.22383034e+01 6.05208524e+00 1.76803945e+01\n",
            " 1.46274452e+01 1.62341963e+01 2.14342950e+00 6.56918748e-01\n",
            " 8.73168750e+00 1.73145555e+01 1.03220717e+01 5.24637502e+00\n",
            " 1.23646996e+01 5.76967543e+00 1.49932895e+01 1.54397683e+01\n",
            " 1.18573082e+01 9.11792695e+00 3.79922338e+00 1.60868199e+01\n",
            " 7.69247791e+00 1.04722097e+00 1.51785412e+01 1.64155213e+01\n",
            " 2.37093167e+00 1.10124061e+01 1.40726101e+01 7.36151693e+00\n",
            " 1.24652678e+01 6.85753948e+00 1.43635784e+01 1.46114792e+00\n",
            " 1.33279006e+01 8.68889422e+00 4.96847801e+00 1.35326446e+01\n",
            " 1.97822427e+01 3.25392460e+00 1.70672284e+01 2.11151318e+00\n",
            " 1.39880875e+01 4.27383879e+00 7.72391922e+00 1.90460093e+01\n",
            " 3.98521452e+00 3.87459013e+00 1.87067490e+01 1.35784842e+01\n",
            " 1.70842963e+01 1.95135070e+01 8.09341534e+00 8.56270295e+00\n",
            " 1.03510700e+01 4.28480188e+00 2.95087669e+00 5.38048317e+00\n",
            " 9.57767307e+00 3.95446591e+00 5.93915519e+00 3.03910326e+00\n",
            " 1.54931243e+01 1.02327488e+01 1.54957866e+01 1.93522933e+01\n",
            " 9.81474818e+00 9.35960144e-01 5.93349985e+00 1.26962606e+01\n",
            " 7.35271444e-01 1.10164582e+01 3.14719823e+00 1.32882963e+01\n",
            " 1.43604375e+00 9.63069203e+00 1.29790821e+01 3.47649451e+00\n",
            " 1.93465548e+01 1.90629538e+01 1.58700443e-01 6.71550024e+00\n",
            " 5.57481635e+00 1.06155260e+01 1.41198860e+01 3.95771967e+00\n",
            " 8.64390067e+00 1.41211282e+01 1.85339295e+01 1.29298830e+01\n",
            " 5.46136400e+00 4.55712715e+00 8.11881100e+00 7.62291633e+00\n",
            " 1.37155113e+01 1.07610286e+01 1.20898416e+00 1.95655483e+01\n",
            " 1.90735769e+01 1.26638613e+01 1.98433383e+01 7.73471189e+00\n",
            " 1.16964086e+01 1.83610934e+01 2.93620878e+00 3.34026269e+00\n",
            " 4.71902114e+00 3.21127922e+00 1.30769504e+01 1.77523528e+01\n",
            " 8.71855419e+00 1.64126614e+01 3.97711904e+00 1.94817786e+01\n",
            " 3.78490161e+00 7.86660006e+00 6.15611511e+00 1.61842460e+01\n",
            " 2.95203945e+00 1.26591909e+01 6.88599608e+00 1.67034299e+01\n",
            " 1.13765236e+01 1.25193030e+01 1.17638938e+01 3.60453435e+00\n",
            " 6.71367905e-01 3.61825087e+00 1.92451536e+01 1.44297248e+01\n",
            " 8.27559196e+00 3.14693357e+00 1.70374172e+01 1.29763258e+01\n",
            " 1.36154106e+01 1.43107637e+01 7.41175887e+00 1.26304381e+01\n",
            " 2.06095283e+00 1.02380025e+01 2.77229944e-01 6.12756680e+00\n",
            " 1.64406114e+01 2.26166965e+00 1.22379230e+01 1.28210743e+01\n",
            " 1.55306538e+01 4.01418856e-01 1.78679680e+01 1.01839399e+01\n",
            " 5.27787488e+00 1.38118196e+01 9.25480958e+00 9.61569116e+00\n",
            " 8.35185847e+00 1.06461738e+00 7.25574243e+00 1.36292117e+00\n",
            " 5.62631366e+00 1.37746531e+01 1.04247320e+01 6.36754069e-02\n",
            " 1.65965955e+01 1.42380349e+01 1.30518581e+01 1.23245427e+01\n",
            " 5.08914732e+00 1.55460231e+01 9.03899004e+00 3.48095325e+00\n",
            " 1.79163736e+01 7.73881309e-01 1.17439035e+01 1.37919879e+01\n",
            " 3.32806654e+00 2.50782685e+00 1.12082590e+01 1.61357638e+01\n",
            " 1.78672184e+01 1.68591727e+01 3.37027243e+00 1.19783844e+00\n",
            " 5.32789747e+00 1.38415966e+01 8.42377480e+00 1.97584697e+01\n",
            " 3.40180443e+00 1.32891958e+01 1.54777816e+01 5.79406603e+00\n",
            " 1.34752455e+01 1.51134920e+01 8.25253799e+00 8.95640209e+00\n",
            " 1.48933887e+01 1.02769486e+01 2.41434070e+00 3.85210884e+00\n",
            " 3.15905504e+00 1.66691181e+01 8.65408617e+00 1.80800308e+00\n",
            " 3.00733046e+00 1.01469319e+01 1.82117850e+01 2.72459906e-01\n",
            " 9.08295890e+00 1.61601086e+01 1.81370865e+01 1.75456424e+01\n",
            " 1.66130147e+01 1.21567956e+01 1.91066397e+01 5.48524893e-01\n",
            " 5.54590831e+00 1.87966672e+01 7.15970076e+00 8.30383416e+00\n",
            " 2.82011362e+00 6.02934374e+00 1.45932201e+01 1.54318644e+01\n",
            " 1.04874578e+01 1.69707490e+01 5.77428407e+00 1.16491414e+01\n",
            " 1.02471775e+01 1.26234022e+01 5.53961101e+00 6.02622904e+00\n",
            " 7.89068992e+00 1.23574110e+01 4.59534134e+00 9.54755683e+00\n",
            " 7.78336188e+00 7.98229366e-01 6.17495271e+00 2.41648114e+00\n",
            " 1.02692768e+01 5.72647437e+00 4.06231633e-01 1.24405329e+01\n",
            " 7.56048429e+00 1.95448517e+01 1.43038454e+01 8.98109549e+00\n",
            " 2.58360279e+00 1.36285134e+01 9.22474744e+00 1.39791030e+01\n",
            " 5.02115641e+00 1.14668809e+01 1.16138973e+01 4.92782261e+00\n",
            " 1.28960080e+01 1.04419199e+01 2.90460869e+00 7.70639035e+00\n",
            " 1.46275235e+01 1.83795474e+01 8.67814275e+00 1.17549759e+01\n",
            " 6.24405533e+00 7.65450597e+00 1.31795423e+01 1.26718909e+00\n",
            " 5.17443105e+00 4.85524749e+00 7.61792759e+00 3.55142736e-01\n",
            " 1.24971690e+01 3.44661966e+00 1.26493105e+01 1.84806784e+01\n",
            " 3.28720951e+00 7.54412707e+00 1.55251940e+01 1.61062457e+01\n",
            " 1.32715863e+01 1.10641294e+01 1.76604561e+01 7.33683434e+00\n",
            " 1.72606038e+01 1.79543421e+01 8.43960811e+00 5.56097810e-01\n",
            " 1.79402533e+01 2.94637929e+00 3.94767747e+00 1.50610175e+00\n",
            " 5.56454102e+00 5.41690994e+00 6.71067140e+00 2.75318706e+00\n",
            " 8.55119178e+00 7.99174205e+00 1.05567090e+01 1.38736982e+01\n",
            " 8.26423486e+00 1.64316495e+00 1.31589296e+01 6.07874911e-01\n",
            " 1.42566198e+01 1.26412837e+01 1.06495335e+01 8.86286360e+00\n",
            " 1.15473292e+01 1.55257786e+01 6.67777105e-01 1.19258013e+01\n",
            " 2.29955170e+00 1.41681830e+01 5.67957469e+00 9.46426595e+00\n",
            " 5.48326667e+00 6.79334000e+00 4.73876150e-01 1.88451330e+01\n",
            " 1.54671469e+01 8.28792679e+00 1.13401724e+01 1.48952372e+01\n",
            " 1.67394912e+01 1.03447347e+01 1.55076936e+01 1.00561842e+01\n",
            " 5.59757524e+00 9.29192950e+00 1.66116785e+00 1.83053571e+00\n",
            " 8.98552337e+00 1.34562856e+01 1.74866772e+01 9.42347203e+00\n",
            " 4.29724746e+00 9.92657126e-01 6.95277759e+00 1.02978642e+01\n",
            " 3.23874011e+00 1.40163378e+01 3.30916569e+00 6.80076236e+00\n",
            " 3.83471910e+00 1.59713108e+01 9.77023185e+00 6.18173055e+00\n",
            " 1.32069950e+01 1.58279339e+00 1.42067586e+01 3.45144325e+00\n",
            " 7.70349281e+00 1.92017461e+01 1.74754563e+01 1.61305587e+01\n",
            " 1.99496268e+01 3.47320206e+00 8.17938071e+00 1.41920142e+01\n",
            " 1.35454950e+01 1.61108950e+01 9.31231546e+00 1.00834056e+01\n",
            " 2.36672263e+00 4.51347617e+00 1.42197903e+01 4.03604592e+00\n",
            " 3.14370694e+00 1.09842082e+01 1.97559809e+01 6.12351805e+00\n",
            " 4.43120655e+00 1.54256579e+01 6.09191740e-01 9.90545880e-01\n",
            " 4.79103283e+00 3.28385595e+00 3.90815111e+00 4.90665623e+00\n",
            " 6.62176725e+00 2.79409245e+00 1.58162124e+01 5.56859484e+00\n",
            " 1.40322960e+00 1.59802196e+01 6.21075333e-01 3.89461070e+00\n",
            " 1.87607849e+01 1.48141296e+01 1.54024967e+01 1.86664331e+01\n",
            " 8.94318114e+00 8.04477985e+00 1.03098848e+01 1.69114816e+01\n",
            " 4.98614836e+00 5.98092799e+00 7.12751308e+00 3.22918669e+00\n",
            " 1.29989049e+01 3.71211252e+00 1.74867125e+00 1.65872451e+01\n",
            " 1.14213433e+01 6.67408610e+00 1.97439770e+01 1.38641234e+01\n",
            " 4.88333128e+00 7.12869852e+00 1.91490976e+01 1.26664805e+01\n",
            " 1.09136186e+00 7.35003025e+00 2.38172772e+00 2.43954532e+00\n",
            " 1.31858078e+01 1.40003797e+01 9.82676874e+00 1.03895021e+01\n",
            " 1.74284140e+01 1.54694431e+01 1.85932376e+01 1.00165238e+01\n",
            " 5.11419830e+00 1.21896392e+01 1.77483753e+01 1.55554214e+01\n",
            " 1.83471868e+01 1.05528615e+01 1.50028939e+01 1.03508410e+01\n",
            " 1.90149926e+01 5.26810733e+00 1.14440556e+01 1.57806314e+01\n",
            " 6.45343854e+00 1.07259517e+01 1.78569083e+01 1.68516927e+01\n",
            " 5.72741214e-01 1.17311664e+01 1.47829208e+01 2.93994518e+00\n",
            " 1.52761623e+01 1.65764437e+01 1.74785051e+01 1.50199747e+01\n",
            " 2.01917361e+00 1.44951315e+01 1.76556272e+01 1.55255498e+01\n",
            " 1.82587014e+00 1.81074484e+01 8.04247280e+00 1.31442336e+01\n",
            " 9.50610405e+00 1.08629180e+01 1.81871507e+01 1.61006069e+01\n",
            " 1.68110043e+01 1.24845830e+01 1.09388246e+01 1.61970070e+01\n",
            " 1.27418099e+01 3.68126535e+00 3.73196850e+00 1.37057376e+01\n",
            " 1.26653984e+01 1.98267484e+01 3.52955663e+00 1.62505155e+01]\n",
            "[27.40127082476204, 7.661091060297864, 32.477411532855726, 36.38106219096992, 35.647950317742385, 42.474702721670056, 49.63375062775417, 14.520791654965187, 51.90721994018404, 43.03544186836408, 9.363154577666112, 13.764860055249615, 56.203087789597106, 28.84244179530604, 15.418908161317933, 4.832913702077699, 24.32556064223811, 41.61158400728309, 31.45797808252227, 28.496871411465705, 41.96047291643413, 50.762807911195736, 37.13462949149677, 33.53588726128389, 13.470920717224281, 47.824795664370434, 42.736358587691704, 42.068055623173116, 18.189207914699555, 35.7671919903231, 55.57469316415688, 24.703930739698485, 29.42612868055511, 7.041046125791268, 11.986203019275056, 30.338657017960273, 25.101494561137557, 33.09236862166576, 43.81932078860886, 10.156809330664341, 10.344342782511116, 15.470347544797587, 24.410077684696617, 16.692722016577306, 46.53758525228532, 27.432278522341285, 25.641830285131157, 40.59597364727767, 7.045799882837446, 30.524587857347967, 38.430278273748584, 25.801490572857272, 49.97677423042391, 46.75707010230905, 13.91116470264435, 41.237560892462625, 24.35127413145649, 46.547643243264595, 32.95014061848566, 27.701395770407817, 47.98006463061343, 21.676599945827487, 44.20762727775678, 34.512023277697466, 40.019401144738936, 42.747280134839826, 41.224782665400255, 41.06059676614644, 27.979636472806682, 14.632395979601188, 40.51241585910143, 42.73775211392159, 46.2628513424561, 15.532864732912305, 29.76009268772861, 22.524231073905714, 35.9759462935605, 41.523129280767826, 14.917845672354687, 11.787318047684753, 19.85385499991537, 37.60439410966447, 36.999212233383474, 31.463776827906955, 43.86084891784232, 47.70092879174386, 17.57361901641729, 17.86486749495331, 49.196464279240686, 27.474954851895433, 29.424987359677683, 16.131803795556976, 32.54026544628835, 21.025350847262665, 14.252024769826619, 10.465015537966735, 28.40816451978938, 55.05607150539949, 9.336761558254283, 32.957581589195165, 22.23268387552232, 34.79360997799233, 40.64597610378187, 17.406646341383098, 39.92303224806449, 43.68563117495852, 24.472827427856906, 11.187078596232123, 40.25423970389311, 25.610530745852905, 51.73367710832521, 34.52907656602391, 29.378764904640562, 39.92300202707672, 22.38365687314577, 51.474094841237715, 7.363739714586375, 16.89154485910301, 24.57208863258702, 18.725313656706405, 31.886213617127297, 8.73696161454424, 29.118061341476665, 29.424455917876443, 40.878735757725764, 51.523201278766706, 23.513719567330003, 12.924521529576339, 34.640965805226855, 34.8230298822362, 36.108959695314894, 8.65817886617665, 12.302826785251511, 43.12471810454517, 15.955300489678226, 29.717748606168023, 19.925617580234594, 18.78732586284738, 45.561016942341865, 18.38943039949128, 21.13896882159817, 37.18889725754244, 43.12872465708044, 43.839058759185065, 1.2761178256726868, 46.872549967023346, 38.63648900459337, 38.56235477064507, 48.620273092895815, 15.068360753060613, 36.90701345542557, 49.633335630369146, 42.29013872111471, 25.85516475658397, 37.89052574032322, 26.56251367326357, 22.615893654105967, 43.891430743475624, 20.019402538560065, 4.787557140887621, 32.223796738991254, 45.4786853486262, 23.44454849767765, 34.72203168961949, 11.051234160839513, 21.554079664883908, 0.6500662909303423, 14.035815892211707, 32.52249134853457, 46.04032658762078, 24.995636820328198, 49.702981778731186, 46.56347584450173, 17.36473258935918, 13.715226259657374, 11.314596349395817, 48.330078310732745, 46.06871615319562, 37.823469607201645, 12.238760786379725, 20.978764814410027, 31.926607703134728, 51.82041613367248, 6.13995115048414, 38.73117574032908, 36.053049495633644, 38.838039609225035, 42.28443897322703, 27.75109104441467, 23.45928759289757, 27.892682881015666, 27.667820494653228, 34.68997665687842, 43.61323527655317, 20.48465025942351, 9.957600354504027, 17.993271032248984, 37.845902932579946, 17.90074809005558, 26.85499717641654, 10.480355331318805, 21.206676740908176, 45.02758880128883, 28.64393471689534, 33.58185464731594, 45.57425823370691, 38.172058012224696, 29.23328045226467, 16.1796844695696, 20.120127688146685, 17.572131228807418, 13.270706095394786, 15.41079025236057, 34.42671853274335, 52.07183330532831, 36.31201831031607, 31.095320529608077, 25.481297087122048, 42.62474634283892, 35.253023776349025, 33.53875109327478, 26.338569541916883, 35.28249322442748, 26.90698964747213, 33.84947790251836, 38.12308645322918, 49.97078929262187, 29.53169644657487, 45.03949034548439, 28.071610994411703, 48.87943459046311, 35.82842940372147, 48.381422792844916, 38.942782117762874, 32.316115373250774, 43.55235363761668, 21.88736615238502, 20.536614710711728, 29.367613012916244, 41.090612950627886, 50.14091651954034, 21.676595006988983, 22.3089479024725, 38.572448514995735, 14.235980921073443, 41.60110325918367, 15.828108021581413, 41.61147252118079, 30.211508984414877, 45.56860758408112, 34.768241137423146, 11.64034170503566, 10.768779066832938, 52.06393736815598, 33.13742784463963, 13.348967215625702, 52.92466177815126, 28.85408853374426, 54.329911909924846, 48.541513712738336, 9.261887318221625, 55.899644992542, 23.852164575006995, 4.261348839570607, 27.972141211307274, 35.28223124976092, 6.7609432340328475, 44.43204085861292, 37.88635914304292, 47.20986483735205, 37.596719250257195, 4.392268714563827, 20.790442321583992, 49.32950236734106, 39.5594131269446, 27.81780828297567, 14.193408757821024, 26.57582864517329, 47.15409494887873, 27.670345439651726, 27.469981751276006, 40.14441415140724, 25.986626394146946, 19.126479628872673, 31.05095062621889, 30.481034562555724, 27.797644940149333, 11.637679282215005, 56.70623037999383, 22.52042277729289, 51.24615190604839, 48.1007706871622, 27.239397045843514, 42.41636964353797, 45.228437817321534, 21.85240578212661, 55.038975804440234, 21.790573880978666, 43.531464639262566, 27.896850513673634, 8.85602641400062, 17.187372557141877, 24.491069565074113, 11.353658932059592, 35.39008292379451, 22.42740129893302, 27.575130783158464, 25.637201385388725, 42.96697646888622, 24.21634090736753, 10.580540142088061, 23.6224513916702, 41.94307604139704, 23.58170794482181, 39.86362963674355, 42.5514787861932, 41.475421713854686, 39.140368663409994, 23.229756290136898, 29.06983836442743, 24.116445053477822, 45.75432765372163, 34.65261623770721, 38.90557291497961, 8.769684439373766, 5.212326363477919, 56.470580136545074, 23.384840469108813, 48.02917217705664, 54.46604375672719, 3.013953302860508, 38.69893631393234, 30.535045377932008, 22.93666209185041, 36.271654268862534, 13.253572975865044, 15.70828597700372, 7.989735706029024, 18.074121895468522, 23.103275146105698, 4.346931725250204, 44.998069197823064, 17.245506438180435, 45.93796243313871, 10.388674155553165, 24.602328466105984, 38.922144798930205, 33.497622083008906, 52.03985495328861, 37.831895442332915, 49.31761645615475, 8.839092294669499, 40.812480501464236, 15.270158331916996, 33.48983485458504, 57.42040845531049, 7.734076456943502, 34.454418102645604, 19.10049017064279, 19.863129213876356, 33.14326995787477, 28.706439612035943, 13.081333927403254, 15.479179824621374, 34.03526592386447, 11.255667195204655, 23.29836822496688, 8.440182150546239, 21.710720994972824, 25.50906775097633, 26.880588350367184, 24.95162665559874, 18.310226528509556, 12.351474275443636, 17.768511713182136, 47.74735494212227, 16.81581468224625, 7.99667584931618, 27.01224716483089, 37.508334449690544, 21.92551108598127, 43.15484982391296, 23.631910263448816, 18.303845841121266, 42.72014959148662, 47.772275493055325, 39.88789896545314, 21.211092595214765, 23.298924528332197, 27.350369101897467, 45.11744981047043, 49.02650162099688, 54.252388603805514, 16.44903360335995, 28.82070831822025, 16.02395903217115, 34.968549344845485, 56.9622413504494, 40.30365138421264, 25.38487571561873, 25.509070803940507, 31.462889111028012, 19.40853010248434, 23.55571311220818, 35.65309073623307, 26.82559068562012, 35.660168062875655, 41.09443199766221, 41.84964858414625, 32.949948520990866, 21.399617824670997, 3.618524525440896, 29.115209790419996, 14.923539321270127, 16.042884856688268, 34.93916386705867, 39.179732448863156, 29.571982111130303, 42.31081469674031, 14.826261766902064, 26.70044578892602, 20.710226579724633, 15.667415787166455, 47.860892074113224, 32.38717175248622, 8.796414360922306, 8.471202706708347, 39.97705424045674, 19.333842609250272, 24.541592430514807, 44.05074008681147, 34.33712909889854, 17.398462455353084, 22.360662853847813, 47.15372418153672, 21.55621564539115, 16.117271016720284, 46.880212425550674, 53.77019990237419, 45.75435294780692, 13.463419838687571, 44.67671319401733, 49.77149623456084, 46.44242567380247, 20.28077233203075, 49.84918039035401, 7.226907698454863, 11.038864754899729, 39.60283274898926, 7.365245391612918, 25.214426319539754, 23.200233849987317, 36.4980811039953, 47.15983020600565, 31.097363444535265, 38.023308549155516, 19.718989397174003, 39.96538437229847, 39.2075955270242, 3.5027592711067546, 12.084421091135566, 39.54540829070292, 34.587034468531414, 6.198158957323283, 41.0223314853742, 37.638729366449745, 38.956106419841596, 6.6847724472838355, 24.104068878373834, 27.87721749662125, 22.102327663382788, 12.64272191192923, 37.50453655186216, 22.03150936597288, 38.8785769286476, 15.20329278859906, 48.404849014963915, 6.3501681143980955, 23.350197748575923, 36.64118165117484, 24.979688060379665, 37.340041011122395, 16.12929019289297, 27.012750756409467, 24.54135718570198, 31.369595453297418, 56.23011882470883, 15.485580241676464, 7.339335240298135, 47.06160946041801, 30.333369768062237, 10.71332873158556, 39.94630561129264, 14.640850465572399, 27.723769528652817, 32.714206917338466, 30.265324274473922, 18.174162164058224, 50.1138375000467, 46.24974946828381, 43.16253127958755, 5.23870080908644, 16.189503172410227, 46.764056523356416, 14.561545481338822, 17.7930725701611, 50.975849597610434, 51.0429562332164, 27.340541815250855, 30.47341874568904, 44.22140040264327, 48.6171114968225, 3.917677231982395, 43.09134118752765, 35.97855751532452, 40.413362417259904, 52.99221498154745, 32.065813404060954, 48.02378386871066, 52.68018314098997, 33.8759969891732, 25.09125031162484, 45.09934771323174, 55.99416933244058, 10.995905160173685, 21.626819512970233, 28.244595584180313, 40.52716317870195, 45.888254557664936, 27.690774387806194, 51.35979523118043, 13.83442180493277, 33.19757147866286, 13.662869535682953, 26.235910672633587, 52.234254585847076, 31.38908520438782, 30.334586732937154, 16.12579930027123, 37.718845401511246, 34.31851959103004, 31.770844409281935, 37.34750008526453, 31.946771884367987, 35.19784859721387, 23.369207067688023, 41.72494408864081, 45.89327478828325, 48.5924188997214, 28.552516439564783, 21.214041252126748, 49.09134361166109, 22.775263598738995, 38.979083587316225, 46.3840687613444, 25.192966262572188, 40.3596146146991, 18.179548101018906, 41.024379862789345, 38.13619128489704, 44.34195597588321, 31.631932384179287, 47.85901884157869, 4.380708006174567, 49.73080655985663, 48.57628371912001, 28.9428879605833, 51.93742272770791, 22.890532321241448, 31.638265531824587, 20.965330550256127, 16.998994316872235, 23.604621737583066, 41.219445359607334, 44.64373867209511, 19.63432586682914, 22.946867472779864, 16.78018386588721, 44.6801735795561, 27.273961685366878, 48.4250990525665, 57.0088741222964, 47.63968810035475, 40.59052365035254, 35.57261924768879, 14.9611902479378, 41.59537375093471, 20.733710142601467, 25.483575063573696, 14.852420465377701, 11.067188100343682, 25.898056084232053, 38.022471542824654, 42.3771834268026, 16.097226469688238, 47.17890168406883, 49.7870389598353, 34.12671450406778, 36.23084948231603, 32.078444269153316, 39.15618984964772, 20.812911190472512, 21.702998086420468, 22.807983711524777, 1.6950857805151953, 39.929091982834116, 47.472794570464615, 14.676461538335449, 56.14681431475992, 46.87505566105169, 29.344490054812336, 28.314063573770248, 35.417309461235774, 48.656908144209694, 39.908052441436716, 33.89238871729553, 22.35856491547007, 29.585854980122875, 44.52481113684511, 15.761376948197597, 14.182784643564547, 35.29858164851731, 40.64938757226728, 22.34936070944628, 50.65559561720933, 18.809694462978968, 2.605227616079697, 24.44774135684766, 53.64660996525808, 32.816736156028846, 22.040096919908624, 11.199924956160011, 13.116966453345148, 55.547830446594425, 42.07675877216484, 12.859340288659459, 25.295292917222188, 40.666453787187805, 31.216495014296203, 33.52515570399932, 46.387279638677064, 31.858933787415435, 38.28261093121849, 21.149860905624962, 18.580313034409453, 21.315036076837373, 20.55995559496301, 37.44541845342927, 29.81935002339129, 50.521211451611144, 15.468750619781312, 30.61576975513819, 19.207744494396078, 23.402367656183905, 18.03744722144519, 6.023921890803696, 3.6249918245046264, 36.799841210766374, 42.400201263498104, 48.54280691474604, 40.95130794053232, 13.144290477223102, 51.1783246492556, 27.814427918521794, 33.65376260159064, 25.374551303015, 31.510557470312236, 28.82194144627691, 5.955714076225194, 14.117338762937827, 37.438925939547765, 18.758308539537623, 30.834639005480064, 24.09025290510119, 47.71952366721191, 37.2577124850972, 37.876701201998486, 11.109136520894374, 43.08619660944307, 27.87153558277266, 28.74527182823152, 27.735695112314204, 21.67219497977438, 9.794472941003235, 33.29905469776219, 32.13824885438785, 33.41936388360148, 37.33794449704628, 32.30764820110919, 44.1203454077305, 29.306664993751273, 13.273453761429169, 18.210974376737937, 22.292075454117267, 23.0608944969828, 20.385018116854784, 11.178764386554164, 48.55354388485952, 22.71246107396947, 20.81294245309279, 54.18322362689723, 13.041319397842493, 15.523989480192546, 33.06069754371316, 17.18489226741454, 5.136197876016347, 47.20710699465604, 32.37024644497821, 54.67512527706461, 37.52596194004901, 26.240070743605063, 35.12300043786945, 6.4477082159787, 51.4912200524115, 9.183213282983996, 54.27523988923824, 18.88971621140683, 23.736154441402284, 28.720215233443323, 26.642700904328947, 40.420765876074825, 30.577542713889386, 21.46254969218746, 44.87071077849747, 41.801888362238834, 28.656582084980304, 30.183986909245675, 22.99216181318015, 17.88631835373945, 34.32271004328736, 20.198805031221013, 37.12949143981955, 10.355388135087086, 17.380194380404564, 51.35939205343823, 32.757209869643255, 26.263064941600604, 17.659510549780727, 44.36989493481008, 49.69520930931643, 21.15823846022844, 49.753549034795554, 45.998255789652035, 17.01859045803725, 10.584388133365113, 34.66404496646152, 13.956411388346305, 30.095298482337622, 37.78216137473099, 22.531357238757412, 23.216293466236877, 28.739097659662935, 42.944183503902146, 53.50491729192478, 17.64824741544491, 50.31492399371753, 37.298182459134395, 16.756202192064002, 6.050535947086257, 11.488908502254453, 41.54264207511323, 29.81040496546678, 8.006667224833208, 24.777427930273408, 42.06848926015782, 4.493123812406417, 34.12420863141286, 17.355252742326982, 40.67275320967569, 26.923167263188986, 28.651655668849184, 20.06705739052217, 17.265204048186334, 50.575283814844184, 21.75798120249454, 21.145564348875773, 17.885306898888185, 29.847830464690652, 24.135543240828287, 19.48376080232843, 17.872037408614954, 11.905974888645996, 39.768652324375026, 33.371616931595, 50.37841261637591, 32.6549467954882, 31.70188823264055, 31.316986653016293, 53.47305474294609, 25.646892578944005, 44.17190855706266, 44.190064654798505, 32.44437820434074, 15.90090118242834, 38.98191911550065, 22.484967072403787, 12.3267459476652, 31.3926045881702, 30.885628207831047, 10.756153857893711, 39.719355383430305, 30.438989202831365, 46.41476593083247, 52.81542733102262, 19.910821795866866, 28.16428154896539, 16.76558437598065, 22.436943462372632, 50.96871759708615, 20.546922207835802, 42.0000055627945, 36.747904693299624, 56.6568593719974, 19.878550302843045, 14.128715387881362, 32.28526674682358, 32.370582191738976, 19.72433843873571, 36.1570335141223, 12.225439078428971, 42.06268644651968, 30.647768481508436, 28.10870285501659, 11.088630685895827, 14.140181511859911, 21.206502214878096, 35.73604464127649, 16.117670804399523, 11.337736743528296, 34.38724190941556, 13.828818289130702, 11.117226200376775, 36.39314040323459, 23.379803435125552, 47.56291033090156, 20.24619268237685, 15.290464726482682, 43.359122275458695, 38.776215012463105, 39.1554175250847, 31.683773844616724, 48.99426891888078, 19.78943943999242, 28.456334268151508, 26.810181056736297, 32.558290475970594, 35.17851590186635, 54.501134936086956, 52.88803490371668, 36.60026353240575, 21.16770070202478, 24.16527815464504, 42.811346825527835, 19.765902728638718, 16.133444806347057, 26.51384767670012, 29.24623877720033, 46.352774328419684, 30.940589492503573, 30.631701121013272, 43.36006568062116, 34.17723751035489, 56.17159491733254, 15.492714434500726, 29.97470643312868, 16.375967880020163, 22.826995488767956, 41.77134452354751, 39.16838571995114, 26.879709907567587, 36.882066296709695, 10.772643443704544, 42.99434954581712, 35.58622739330788, 33.65594532899878, 19.838691739021677, 47.39633230765338, 16.596615631188396, 41.576689447089684, 21.753551245435553, 7.763771750104939, 40.41010691248615, 45.41967110049828, 39.06230576069643, 30.36402132791536, 28.460049775977655, 38.4339368971718, 28.37240531718414, 16.5721115116556, 44.149962380285345, 15.101805962463953, 42.75404979476219, 3.656393155237412, 41.50229576038509, 14.619466296204141, 15.735905576468621, 16.619832666336407, 16.95882766242025, 52.73312425563274, 24.233483554682373, 26.425503078090784, 46.19120443638281, 34.30308443951177, 32.700546443898894, 39.64096508428781, 37.3082018126984, 20.05302642656518, 29.77635883354958, 34.542412887634384, 23.96657229207855, 21.216009279127068, 38.79246630224717, 33.956847505886486, 21.039409425732956, 6.308111241151764, 30.514851296344787, 54.40105820669302, 25.79632980577, 22.809679163614266, 49.84882077706557, 36.00462941177439, 43.486284101955505, 17.66616273811634, 30.649881716904957, 12.74742762920235, 30.771989127200225, 28.71947751824119, 37.32500502289264, 23.495561460505293, 24.358979603281863, 30.300669654564405, 24.90180728941571, 21.487951445589488, 26.556615971535912, 55.482632804013065, 47.74154149733604, 39.830816318123425, 21.815242737457773, 56.515571125006645, 29.17007863191221, 25.480372821153285, 11.423394683793946, 4.861000948351064, 28.711608882406658, 49.513104765446485, 41.56134005546153, 23.69427251147488, 20.69974798136051, 22.680974416513543, 30.87048397845389, 38.63879042076023, 44.03119314320756, 6.264937001494966, 31.47592632423905, 26.791552722414885, 22.33135412465107, 31.72776606384157, 13.704036602865493, 34.58789714782088, 40.44255401738761, 22.750065999123116, 22.74141448989121, 17.911150644978186, 44.1296068216634, 32.65904615869268, 18.78004310299807, 19.455268956063367, 52.9367366480507, 35.84109622659745, 27.83934913653715, 49.47029920385208, 5.875027527909271, 20.584070682844384, 52.38495381607296, 35.09923278241568, 44.93998782845871, 8.850535085574613, 46.32811074039495, 28.86324105885225, 46.08776189919489, 30.302585529433927, 36.11065267651156, 33.832700237081085, 21.052316628620773, 29.03703591526084, 39.94145104049298, 25.841806157634803, 40.06335825972856, 33.292483264189286, 34.740603871133565, 33.25709152735325, 42.182782201521796, 51.58063026270777, 32.34703649085289, 6.061223471765562, 28.11977777221898, 14.346674271953827, 51.72339174265343, 30.698539626706186, 40.93784258260634]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFRWMtGQeRmp"
      },
      "source": [
        "X1= np.array(X1).reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVKdyOfYeRmp"
      },
      "source": [
        "X2=np.array(X2).reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NB3EPtHeRmp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7eLkSFoeRmp",
        "outputId": "6867055b-0e85-40c0-fa36-f241ecde195b"
      },
      "source": [
        "X1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVIi99FxeRmp"
      },
      "source": [
        "X=np.concatenate((X1, X2), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJZH4YMbeRmp",
        "outputId": "f245a32d-a0e9-45e5-ca41-629cf922bcd4"
      },
      "source": [
        "#X = np.column_stack((X1, X2))\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[13.61404994  0.17317094]\n",
            " [ 2.12815014  3.40479077]\n",
            " [ 9.24732253 13.98276647]\n",
            " ...\n",
            " [15.94832169 19.82674835]\n",
            " [13.5844915   3.52955663]\n",
            " [12.34366353 16.25051553]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oGLFXNseRmp"
      },
      "source": [
        "X = array(X).reshape(1000, 1, 2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwuvSZU9eRmp",
        "outputId": "f55fa0f3-12d2-45d6-ac48-23dc25794276"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0i6Xp3LeRmp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxesdEyseRmp",
        "outputId": "96ec6cb0-f5a6-4fb6-af94-2659cd070255"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(20, activation='relu', input_shape=(1, 2)))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (None, 20)                1840      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 2,061\n",
            "Trainable params: 2,061\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnBpww3eRmp",
        "outputId": "9e5e0747-3e41-4678-ab66-4cb0c91a1265"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn5cTv0-eRmp",
        "outputId": "24c835c3-d5cc-4d9a-bf36-08c1d39f3f94"
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-143-de29cf719ad1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5lHJp3FeRmp"
      },
      "source": [
        "model.compile(optimizer='adam', loss='mse')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m70VEGkXeRmp",
        "outputId": "406a91d8-55f1-416e-c36f-08c1a1aba25a"
      },
      "source": [
        "model.fit(X, Y, epochs=500, validation_split=0.2, shuffle=True, batch_size=5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/500\n",
            "800/800 [==============================] - 0s 559us/step - loss: 875.7121 - val_loss: 243.9020\n",
            "Epoch 2/500\n",
            "800/800 [==============================] - 0s 234us/step - loss: 25.1967 - val_loss: 3.4513\n",
            "Epoch 3/500\n",
            "800/800 [==============================] - 0s 197us/step - loss: 1.7343 - val_loss: 0.9421\n",
            "Epoch 4/500\n",
            "800/800 [==============================] - 0s 196us/step - loss: 0.6229 - val_loss: 0.4759\n",
            "Epoch 5/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 0.3165 - val_loss: 0.2364\n",
            "Epoch 6/500\n",
            "800/800 [==============================] - 0s 199us/step - loss: 0.1699 - val_loss: 0.1214\n",
            "Epoch 7/500\n",
            "800/800 [==============================] - 0s 189us/step - loss: 0.0992 - val_loss: 0.0747\n",
            "Epoch 8/500\n",
            "800/800 [==============================] - 0s 197us/step - loss: 0.0653 - val_loss: 0.0537\n",
            "Epoch 9/500\n",
            "800/800 [==============================] - 0s 208us/step - loss: 0.0492 - val_loss: 0.0432\n",
            "Epoch 10/500\n",
            "800/800 [==============================] - 0s 207us/step - loss: 0.0380 - val_loss: 0.0313\n",
            "Epoch 11/500\n",
            "800/800 [==============================] - 0s 235us/step - loss: 0.0294 - val_loss: 0.0254\n",
            "Epoch 12/500\n",
            "800/800 [==============================] - 0s 202us/step - loss: 0.0246 - val_loss: 0.0203\n",
            "Epoch 13/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 0.0208 - val_loss: 0.0178\n",
            "Epoch 14/500\n",
            "800/800 [==============================] - 0s 213us/step - loss: 0.0177 - val_loss: 0.0162\n",
            "Epoch 15/500\n",
            "800/800 [==============================] - 0s 211us/step - loss: 0.0154 - val_loss: 0.0138\n",
            "Epoch 16/500\n",
            "800/800 [==============================] - 0s 211us/step - loss: 0.0135 - val_loss: 0.0128\n",
            "Epoch 17/500\n",
            "800/800 [==============================] - 0s 211us/step - loss: 0.0124 - val_loss: 0.0118\n",
            "Epoch 18/500\n",
            "800/800 [==============================] - 0s 205us/step - loss: 0.0114 - val_loss: 0.0104\n",
            "Epoch 19/500\n",
            "800/800 [==============================] - 0s 216us/step - loss: 0.0103 - val_loss: 0.0099\n",
            "Epoch 20/500\n",
            "800/800 [==============================] - 0s 230us/step - loss: 0.0095 - val_loss: 0.0102\n",
            "Epoch 21/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0089 - val_loss: 0.0109\n",
            "Epoch 22/500\n",
            "800/800 [==============================] - 0s 216us/step - loss: 0.0084 - val_loss: 0.0085\n",
            "Epoch 23/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0077 - val_loss: 0.0082\n",
            "Epoch 24/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0074 - val_loss: 0.0070\n",
            "Epoch 25/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 0.0066 - val_loss: 0.0066\n",
            "Epoch 26/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 0.0063 - val_loss: 0.0063\n",
            "Epoch 27/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 0.0062 - val_loss: 0.0057\n",
            "Epoch 28/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0056 - val_loss: 0.0064\n",
            "Epoch 29/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0051 - val_loss: 0.0051\n",
            "Epoch 30/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0050 - val_loss: 0.0076\n",
            "Epoch 31/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0047 - val_loss: 0.0049\n",
            "Epoch 32/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0044 - val_loss: 0.0040\n",
            "Epoch 33/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0042 - val_loss: 0.0039\n",
            "Epoch 34/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 0.0039 - val_loss: 0.0038\n",
            "Epoch 35/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 0.0040 - val_loss: 0.0035\n",
            "Epoch 36/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0035 - val_loss: 0.0054\n",
            "Epoch 37/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 0.0033 - val_loss: 0.0033\n",
            "Epoch 38/500\n",
            "800/800 [==============================] - 0s 215us/step - loss: 0.0032 - val_loss: 0.0026\n",
            "Epoch 39/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 0.0034 - val_loss: 0.0024\n",
            "Epoch 40/500\n",
            "800/800 [==============================] - 0s 207us/step - loss: 0.0027 - val_loss: 0.0053\n",
            "Epoch 41/500\n",
            "800/800 [==============================] - 0s 237us/step - loss: 0.0030 - val_loss: 0.0023\n",
            "Epoch 42/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0024 - val_loss: 0.0020\n",
            "Epoch 43/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0024 - val_loss: 0.0029\n",
            "Epoch 44/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0024 - val_loss: 0.0034\n",
            "Epoch 45/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 46/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 47/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0019 - val_loss: 9.9600e-04\n",
            "Epoch 48/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0016 - val_loss: 0.0026\n",
            "Epoch 49/500\n",
            "800/800 [==============================] - 0s 220us/step - loss: 0.0014 - val_loss: 0.0019\n",
            "Epoch 50/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0020 - val_loss: 0.0011\n",
            "Epoch 51/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0041 - val_loss: 0.0024\n",
            "Epoch 52/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 0.0013 - val_loss: 4.6950e-04\n",
            "Epoch 53/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 54/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 0.0015 - val_loss: 8.5456e-04\n",
            "Epoch 55/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 0.0033 - val_loss: 4.6368e-04\n",
            "Epoch 56/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0021 - val_loss: 0.0045\n",
            "Epoch 57/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0022 - val_loss: 0.0021\n",
            "Epoch 58/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0017 - val_loss: 0.0131\n",
            "Epoch 59/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0044 - val_loss: 2.7735e-04\n",
            "Epoch 60/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 0.0011 - val_loss: 3.6872e-04\n",
            "Epoch 61/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0011 - val_loss: 2.0883e-04\n",
            "Epoch 62/500\n",
            "800/800 [==============================] - 0s 214us/step - loss: 0.0013 - val_loss: 3.6456e-04\n",
            "Epoch 63/500\n",
            "800/800 [==============================] - 0s 214us/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 64/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 0.0041 - val_loss: 0.0041\n",
            "Epoch 65/500\n",
            "800/800 [==============================] - 0s 216us/step - loss: 0.0022 - val_loss: 2.9427e-04\n",
            "Epoch 66/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 7.2720e-04 - val_loss: 3.1029e-04\n",
            "Epoch 67/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 9.0124e-04 - val_loss: 1.8854e-04\n",
            "Epoch 68/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 7.7364e-04 - val_loss: 4.5294e-04\n",
            "Epoch 69/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 0.0020 - val_loss: 3.1415e-04\n",
            "Epoch 70/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 0.0014 - val_loss: 0.0019\n",
            "Epoch 71/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 7.0235e-04 - val_loss: 1.9561e-04\n",
            "Epoch 72/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 0.0012 - val_loss: 2.1331e-04\n",
            "Epoch 73/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 0.0014 - val_loss: 0.0020\n",
            "Epoch 74/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 0.0020 - val_loss: 0.0012\n",
            "Epoch 75/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 76/500\n",
            "800/800 [==============================] - 0s 247us/step - loss: 0.0013 - val_loss: 2.7358e-04\n",
            "Epoch 77/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 0.0020 - val_loss: 0.0049\n",
            "Epoch 78/500\n",
            "800/800 [==============================] - 0s 214us/step - loss: 0.0013 - val_loss: 3.5601e-04\n",
            "Epoch 79/500\n",
            "800/800 [==============================] - 0s 234us/step - loss: 0.0013 - val_loss: 4.7361e-04\n",
            "Epoch 80/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 0.0016 - val_loss: 1.9617e-04\n",
            "Epoch 81/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 0.0015 - val_loss: 1.6416e-04\n",
            "Epoch 82/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 0.0013 - val_loss: 0.0048\n",
            "Epoch 83/500\n",
            "800/800 [==============================] - 0s 209us/step - loss: 0.0043 - val_loss: 1.3910e-04\n",
            "Epoch 84/500\n",
            "800/800 [==============================] - 0s 220us/step - loss: 0.0057 - val_loss: 0.0072\n",
            "Epoch 85/500\n",
            "800/800 [==============================] - 0s 209us/step - loss: 0.0027 - val_loss: 1.4748e-04\n",
            "Epoch 86/500\n",
            "800/800 [==============================] - 0s 258us/step - loss: 1.8904e-04 - val_loss: 2.4077e-04\n",
            "Epoch 87/500\n",
            "800/800 [==============================] - 0s 242us/step - loss: 2.7418e-04 - val_loss: 7.6145e-04\n",
            "Epoch 88/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 5.3235e-04 - val_loss: 4.3864e-04\n",
            "Epoch 89/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0018 - val_loss: 9.2251e-04\n",
            "Epoch 90/500\n",
            "800/800 [==============================] - 0s 214us/step - loss: 5.6390e-04 - val_loss: 1.3019e-04\n",
            "Epoch 91/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 9.2608e-04 - val_loss: 1.8659e-04\n",
            "Epoch 92/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 6.7691e-04 - val_loss: 0.0041\n",
            "Epoch 93/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 8.0176e-04 - val_loss: 2.1487e-04\n",
            "Epoch 94/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0118 - val_loss: 0.0024\n",
            "Epoch 95/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0018 - val_loss: 0.0024\n",
            "Epoch 96/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 5.7038e-04 - val_loss: 9.2513e-05\n",
            "Epoch 97/500\n",
            "800/800 [==============================] - 0s 207us/step - loss: 3.3883e-04 - val_loss: 2.8469e-04\n",
            "Epoch 98/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 2.7626e-04 - val_loss: 2.3276e-04\n",
            "Epoch 99/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 5.3725e-04 - val_loss: 0.0013\n",
            "Epoch 100/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0023 - val_loss: 3.9266e-04\n",
            "Epoch 101/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 8.4834e-04 - val_loss: 4.4101e-04\n",
            "Epoch 102/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 2.2700e-04 - val_loss: 1.7729e-04\n",
            "Epoch 103/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0026 - val_loss: 9.3392e-04\n",
            "Epoch 104/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 3.5260e-04 - val_loss: 3.2505e-04\n",
            "Epoch 105/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 3.9334e-04 - val_loss: 0.0012\n",
            "Epoch 106/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 0.0018 - val_loss: 2.8372e-04\n",
            "Epoch 107/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0044 - val_loss: 0.0017\n",
            "Epoch 108/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 0.0013 - val_loss: 4.5473e-04\n",
            "Epoch 109/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 6.2554e-04 - val_loss: 2.2996e-04\n",
            "Epoch 110/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 0.0013 - val_loss: 3.3206e-04\n",
            "Epoch 111/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 2.8221e-04 - val_loss: 1.7950e-04\n",
            "Epoch 112/500\n",
            "800/800 [==============================] - 0s 198us/step - loss: 0.0091 - val_loss: 0.0016\n",
            "Epoch 113/500\n",
            "800/800 [==============================] - 0s 209us/step - loss: 8.2669e-04 - val_loss: 6.8855e-04\n",
            "Epoch 114/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 3.6074e-04 - val_loss: 4.3707e-04\n",
            "Epoch 115/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0023 - val_loss: 5.4636e-04\n",
            "Epoch 116/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 2.4774e-04 - val_loss: 1.7586e-04\n",
            "Epoch 117/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 3.6753e-04 - val_loss: 4.2217e-04\n",
            "Epoch 118/500\n",
            "800/800 [==============================] - 0s 214us/step - loss: 5.5935e-04 - val_loss: 2.5327e-04\n",
            "Epoch 119/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 5.8164e-04 - val_loss: 0.0017\n",
            "Epoch 120/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 0.0157 - val_loss: 1.4164e-04\n",
            "Epoch 121/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 1.6039e-04 - val_loss: 1.4511e-04\n",
            "Epoch 122/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 1.7374e-04 - val_loss: 3.8562e-04\n",
            "Epoch 123/500\n",
            "800/800 [==============================] - 0s 213us/step - loss: 6.5196e-04 - val_loss: 9.6258e-05\n",
            "Epoch 124/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 3.2289e-04 - val_loss: 6.0107e-04\n",
            "Epoch 125/500\n",
            "800/800 [==============================] - 0s 216us/step - loss: 0.0018 - val_loss: 1.6737e-04\n",
            "Epoch 126/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0010 - val_loss: 7.0851e-04\n",
            "Epoch 127/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 2.6364e-04 - val_loss: 9.2468e-05\n",
            "Epoch 128/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 0.0019 - val_loss: 2.6148e-04\n",
            "Epoch 129/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 6.1147e-04 - val_loss: 6.2890e-04\n",
            "Epoch 130/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 8.0653e-04 - val_loss: 4.7863e-04\n",
            "Epoch 131/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 0.0016 - val_loss: 0.0020\n",
            "Epoch 132/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0014 - val_loss: 6.5430e-04\n",
            "Epoch 133/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 0.0024 - val_loss: 8.9270e-05\n",
            "Epoch 134/500\n",
            "800/800 [==============================] - 0s 216us/step - loss: 2.0257e-04 - val_loss: 9.7756e-05\n",
            "Epoch 135/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0014 - val_loss: 4.9118e-04\n",
            "Epoch 136/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 4.4536e-04 - val_loss: 3.5473e-04\n",
            "Epoch 137/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 3.1310e-04 - val_loss: 5.6844e-04\n",
            "Epoch 138/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 0.0032 - val_loss: 0.0024\n",
            "Epoch 139/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 5.5442e-04 - val_loss: 1.2344e-04\n",
            "Epoch 140/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 9.2439e-05 - val_loss: 3.2511e-05\n",
            "Epoch 141/500\n",
            "800/800 [==============================] - 0s 208us/step - loss: 3.0672e-04 - val_loss: 1.2912e-04\n",
            "Epoch 142/500\n",
            "800/800 [==============================] - 0s 245us/step - loss: 3.5146e-04 - val_loss: 3.7345e-04\n",
            "Epoch 143/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0050 - val_loss: 0.0077\n",
            "Epoch 144/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 0.0019 - val_loss: 2.0935e-04\n",
            "Epoch 145/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 4.6288e-04 - val_loss: 6.2035e-04\n",
            "Epoch 146/500\n",
            "800/800 [==============================] - 0s 216us/step - loss: 2.1423e-04 - val_loss: 3.9416e-04\n",
            "Epoch 147/500\n",
            "800/800 [==============================] - 0s 216us/step - loss: 2.8091e-04 - val_loss: 1.5746e-04\n",
            "Epoch 148/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 9.6770e-04 - val_loss: 9.3854e-05\n",
            "Epoch 149/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 4.0413e-04 - val_loss: 1.2903e-04\n",
            "Epoch 150/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 0.0027 - val_loss: 0.0088\n",
            "Epoch 151/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0014 - val_loss: 0.0030\n",
            "Epoch 152/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "800/800 [==============================] - 0s 228us/step - loss: 0.0024 - val_loss: 7.2569e-04\n",
            "Epoch 153/500\n",
            "800/800 [==============================] - 0s 214us/step - loss: 8.9968e-04 - val_loss: 1.8171e-04\n",
            "Epoch 154/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 5.1113e-04 - val_loss: 0.0011\n",
            "Epoch 155/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 2.8090e-04 - val_loss: 2.5547e-04\n",
            "Epoch 156/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 2.5611e-04 - val_loss: 2.2474e-04\n",
            "Epoch 157/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 3.9637e-04 - val_loss: 4.3266e-04\n",
            "Epoch 158/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0069 - val_loss: 4.8008e-04\n",
            "Epoch 159/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 1.4721e-04 - val_loss: 4.6268e-05\n",
            "Epoch 160/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 3.7780e-04 - val_loss: 6.4412e-05\n",
            "Epoch 161/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 2.2830e-04 - val_loss: 9.5053e-04\n",
            "Epoch 162/500\n",
            "800/800 [==============================] - 0s 220us/step - loss: 5.8572e-04 - val_loss: 1.0396e-04\n",
            "Epoch 163/500\n",
            "800/800 [==============================] - 0s 205us/step - loss: 1.3111e-04 - val_loss: 3.6427e-05\n",
            "Epoch 164/500\n",
            "800/800 [==============================] - 0s 241us/step - loss: 3.6566e-04 - val_loss: 1.6177e-04\n",
            "Epoch 165/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 4.1184e-04 - val_loss: 0.0031\n",
            "Epoch 166/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 0.0085 - val_loss: 2.6215e-04\n",
            "Epoch 167/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 1.7544e-04 - val_loss: 1.0712e-04\n",
            "Epoch 168/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 1.1677e-04 - val_loss: 3.5101e-05\n",
            "Epoch 169/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 2.5272e-04 - val_loss: 3.0420e-04\n",
            "Epoch 170/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 4.4666e-04 - val_loss: 3.0368e-04\n",
            "Epoch 171/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 8.4151e-04 - val_loss: 1.6193e-04\n",
            "Epoch 172/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 3.5603e-04 - val_loss: 5.7720e-05\n",
            "Epoch 173/500\n",
            "800/800 [==============================] - 0s 199us/step - loss: 1.0336e-04 - val_loss: 5.3583e-05\n",
            "Epoch 174/500\n",
            "800/800 [==============================] - 0s 211us/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 175/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 0.0081 - val_loss: 3.7624e-04\n",
            "Epoch 176/500\n",
            "800/800 [==============================] - 0s 225us/step - loss: 2.9638e-04 - val_loss: 2.8091e-05\n",
            "Epoch 177/500\n",
            "800/800 [==============================] - 0s 239us/step - loss: 1.2225e-04 - val_loss: 1.5202e-04\n",
            "Epoch 178/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 9.8538e-05 - val_loss: 1.5923e-04\n",
            "Epoch 179/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 6.2622e-05 - val_loss: 4.5087e-05\n",
            "Epoch 180/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 0.0010 - val_loss: 0.0015\n",
            "Epoch 181/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 5.8032e-04 - val_loss: 7.1837e-05\n",
            "Epoch 182/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 1.1161e-04 - val_loss: 8.2505e-05\n",
            "Epoch 183/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 2.8803e-04 - val_loss: 3.8425e-05\n",
            "Epoch 184/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 6.2356e-04 - val_loss: 0.0059\n",
            "Epoch 185/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 0.0055 - val_loss: 2.2211e-04\n",
            "Epoch 186/500\n",
            "800/800 [==============================] - 0s 200us/step - loss: 7.2967e-05 - val_loss: 4.9338e-05\n",
            "Epoch 187/500\n",
            "800/800 [==============================] - 0s 244us/step - loss: 6.5787e-05 - val_loss: 2.8508e-04\n",
            "Epoch 188/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 3.0862e-04 - val_loss: 1.7708e-04\n",
            "Epoch 189/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 1.7199e-04 - val_loss: 4.6806e-05\n",
            "Epoch 190/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 4.9821e-05 - val_loss: 2.0676e-05\n",
            "Epoch 191/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 0.0012 - val_loss: 7.6387e-04\n",
            "Epoch 192/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0020 - val_loss: 0.0011\n",
            "Epoch 193/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 0.0068 - val_loss: 0.0040\n",
            "Epoch 194/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 0.0020 - val_loss: 3.3281e-05\n",
            "Epoch 195/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 7.6742e-05 - val_loss: 2.6728e-05\n",
            "Epoch 196/500\n",
            "800/800 [==============================] - 0s 216us/step - loss: 1.0019e-04 - val_loss: 3.4333e-05\n",
            "Epoch 197/500\n",
            "800/800 [==============================] - 0s 233us/step - loss: 1.0448e-04 - val_loss: 1.1794e-04\n",
            "Epoch 198/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 5.6439e-05 - val_loss: 8.8754e-05\n",
            "Epoch 199/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 1.2504e-04 - val_loss: 6.2634e-05\n",
            "Epoch 200/500\n",
            "800/800 [==============================] - 0s 225us/step - loss: 4.6655e-04 - val_loss: 0.0076\n",
            "Epoch 201/500\n",
            "800/800 [==============================] - 0s 208us/step - loss: 0.0028 - val_loss: 7.0691e-05\n",
            "Epoch 202/500\n",
            "800/800 [==============================] - 0s 239us/step - loss: 2.5257e-04 - val_loss: 2.8840e-04\n",
            "Epoch 203/500\n",
            "800/800 [==============================] - 0s 225us/step - loss: 4.7048e-04 - val_loss: 3.0828e-04\n",
            "Epoch 204/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 5.8774e-04 - val_loss: 9.5470e-04\n",
            "Epoch 205/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 9.2698e-04 - val_loss: 5.8957e-05\n",
            "Epoch 206/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 8.5598e-05 - val_loss: 8.2926e-05\n",
            "Epoch 207/500\n",
            "800/800 [==============================] - 0s 205us/step - loss: 7.8278e-04 - val_loss: 9.6009e-04\n",
            "Epoch 208/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 0.0164 - val_loss: 0.0322\n",
            "Epoch 209/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 0.0053 - val_loss: 3.2876e-05\n",
            "Epoch 210/500\n",
            "800/800 [==============================] - 0s 242us/step - loss: 2.7479e-05 - val_loss: 1.8137e-05\n",
            "Epoch 211/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 3.8610e-05 - val_loss: 2.3298e-05\n",
            "Epoch 212/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 2.5554e-05 - val_loss: 1.4273e-04\n",
            "Epoch 213/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 2.5199e-05 - val_loss: 1.8196e-05\n",
            "Epoch 214/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 5.3223e-05 - val_loss: 1.0020e-04\n",
            "Epoch 215/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 4.8208e-05 - val_loss: 2.3428e-05\n",
            "Epoch 216/500\n",
            "800/800 [==============================] - 0s 230us/step - loss: 4.6278e-05 - val_loss: 2.8186e-05\n",
            "Epoch 217/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 1.5030e-04 - val_loss: 1.8266e-04\n",
            "Epoch 218/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 2.1512e-04 - val_loss: 1.1659e-04\n",
            "Epoch 219/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 2.3131e-04 - val_loss: 1.1655e-04\n",
            "Epoch 220/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0012 - val_loss: 1.9759e-04\n",
            "Epoch 221/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 1.1892e-04 - val_loss: 7.6684e-05\n",
            "Epoch 222/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 4.1580e-04 - val_loss: 0.0015\n",
            "Epoch 223/500\n",
            "800/800 [==============================] - 0s 210us/step - loss: 0.0075 - val_loss: 1.8086e-04\n",
            "Epoch 224/500\n",
            "800/800 [==============================] - 0s 245us/step - loss: 6.7032e-05 - val_loss: 1.8837e-05\n",
            "Epoch 225/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 3.8521e-05 - val_loss: 2.8850e-05\n",
            "Epoch 226/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 4.1495e-05 - val_loss: 2.0789e-05\n",
            "Epoch 227/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 6.5977e-05 - val_loss: 3.5590e-05\n",
            "Epoch 228/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0015 - val_loss: 2.8996e-04\n",
            "Epoch 229/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 7.2957e-04 - val_loss: 6.0464e-04\n",
            "Epoch 230/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 2.9850e-04 - val_loss: 6.1497e-05\n",
            "Epoch 231/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0036 - val_loss: 0.0034\n",
            "Epoch 232/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0020 - val_loss: 2.3920e-05\n",
            "Epoch 233/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 1.0797e-04 - val_loss: 1.2290e-04\n",
            "Epoch 234/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0031 - val_loss: 0.0023\n",
            "Epoch 235/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 0.0030 - val_loss: 4.3212e-05\n",
            "Epoch 236/500\n",
            "800/800 [==============================] - 0s 216us/step - loss: 2.0000e-04 - val_loss: 2.0018e-04\n",
            "Epoch 237/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 9.9448e-05 - val_loss: 2.1779e-05\n",
            "Epoch 238/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 1.8193e-04 - val_loss: 3.7059e-05\n",
            "Epoch 239/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 8.8827e-05 - val_loss: 2.1869e-05\n",
            "Epoch 240/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 1.6843e-04 - val_loss: 0.0011\n",
            "Epoch 241/500\n",
            "800/800 [==============================] - 0s 233us/step - loss: 7.6361e-04 - val_loss: 6.0299e-04\n",
            "Epoch 242/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 2.6519e-04 - val_loss: 5.6228e-05\n",
            "Epoch 243/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 0.0047 - val_loss: 0.0347\n",
            "Epoch 244/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 0.0068 - val_loss: 0.0054\n",
            "Epoch 245/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 8.1295e-04 - val_loss: 2.1420e-05\n",
            "Epoch 246/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 4.8201e-05 - val_loss: 2.7383e-05\n",
            "Epoch 247/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 1.3506e-04 - val_loss: 2.5378e-05\n",
            "Epoch 248/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 1.2020e-04 - val_loss: 3.8294e-04\n",
            "Epoch 249/500\n",
            "800/800 [==============================] - 0s 237us/step - loss: 6.1091e-04 - val_loss: 4.8943e-05\n",
            "Epoch 250/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 2.9774e-04 - val_loss: 2.0803e-04\n",
            "Epoch 251/500\n",
            "800/800 [==============================] - 0s 225us/step - loss: 1.6488e-04 - val_loss: 9.1912e-05\n",
            "Epoch 252/500\n",
            "800/800 [==============================] - 0s 213us/step - loss: 4.7204e-04 - val_loss: 2.8667e-04\n",
            "Epoch 253/500\n",
            "800/800 [==============================] - 0s 244us/step - loss: 4.9665e-04 - val_loss: 1.0630e-04\n",
            "Epoch 254/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 3.6506e-04 - val_loss: 2.7972e-04\n",
            "Epoch 255/500\n",
            "800/800 [==============================] - 0s 236us/step - loss: 0.0064 - val_loss: 7.8269e-04\n",
            "Epoch 256/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 1.8203e-04 - val_loss: 5.4994e-05\n",
            "Epoch 257/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 9.3055e-05 - val_loss: 3.4763e-05\n",
            "Epoch 258/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 1.2280e-04 - val_loss: 6.6034e-05\n",
            "Epoch 259/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 8.4684e-05 - val_loss: 4.3066e-05\n",
            "Epoch 260/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 6.1092e-04 - val_loss: 2.1599e-04\n",
            "Epoch 261/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 0.0039 - val_loss: 0.0011\n",
            "Epoch 262/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 5.8838e-04 - val_loss: 2.6489e-05\n",
            "Epoch 263/500\n",
            "800/800 [==============================] - 0s 208us/step - loss: 6.9533e-05 - val_loss: 1.1552e-04\n",
            "Epoch 264/500\n",
            "800/800 [==============================] - 0s 230us/step - loss: 7.1708e-05 - val_loss: 4.6067e-05\n",
            "Epoch 265/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 1.5999e-04 - val_loss: 2.6253e-04\n",
            "Epoch 266/500\n",
            "800/800 [==============================] - 0s 237us/step - loss: 0.0012 - val_loss: 0.0024\n",
            "Epoch 267/500\n",
            "800/800 [==============================] - 0s 216us/step - loss: 2.1639e-04 - val_loss: 2.3957e-04\n",
            "Epoch 268/500\n",
            "800/800 [==============================] - 0s 220us/step - loss: 2.0123e-04 - val_loss: 5.1275e-05\n",
            "Epoch 269/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 1.8047e-04 - val_loss: 2.4156e-04\n",
            "Epoch 270/500\n",
            "800/800 [==============================] - 0s 241us/step - loss: 0.0019 - val_loss: 0.0010\n",
            "Epoch 271/500\n",
            "800/800 [==============================] - 0s 225us/step - loss: 4.6739e-04 - val_loss: 5.9111e-05\n",
            "Epoch 272/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 9.5656e-05 - val_loss: 1.0706e-04\n",
            "Epoch 273/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 2.5935e-04 - val_loss: 4.2547e-05\n",
            "Epoch 274/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 8.7733e-04 - val_loss: 4.4555e-04\n",
            "Epoch 275/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0028 - val_loss: 1.3441e-04\n",
            "Epoch 276/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 1.8425e-04 - val_loss: 2.2751e-04\n",
            "Epoch 277/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 1.9609e-04 - val_loss: 3.2293e-05\n",
            "Epoch 278/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 0.0067 - val_loss: 0.0017\n",
            "Epoch 279/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 5.2769e-04 - val_loss: 9.7499e-05\n",
            "Epoch 280/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 6.0076e-05 - val_loss: 4.1552e-05\n",
            "Epoch 281/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 2.1465e-04 - val_loss: 2.2705e-05\n",
            "Epoch 282/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 1.2735e-04 - val_loss: 1.7422e-04\n",
            "Epoch 283/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 0.0014 - val_loss: 6.0292e-04\n",
            "Epoch 284/500\n",
            "800/800 [==============================] - 0s 237us/step - loss: 4.3360e-04 - val_loss: 6.2814e-04\n",
            "Epoch 285/500\n",
            "800/800 [==============================] - 0s 238us/step - loss: 4.5410e-04 - val_loss: 4.2175e-05\n",
            "Epoch 286/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 6.4564e-04 - val_loss: 3.4425e-04\n",
            "Epoch 287/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 2.1021e-04 - val_loss: 5.3663e-05\n",
            "Epoch 288/500\n",
            "800/800 [==============================] - 0s 248us/step - loss: 3.3802e-04 - val_loss: 1.7268e-04\n",
            "Epoch 289/500\n",
            "800/800 [==============================] - 0s 225us/step - loss: 9.2260e-04 - val_loss: 0.0025\n",
            "Epoch 290/500\n",
            "800/800 [==============================] - 0s 215us/step - loss: 2.0295e-04 - val_loss: 2.7765e-04\n",
            "Epoch 291/500\n",
            "800/800 [==============================] - 0s 230us/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 292/500\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.0015 - val_loss: 1.3270e-04\n",
            "Epoch 293/500\n",
            "800/800 [==============================] - 0s 295us/step - loss: 4.0817e-04 - val_loss: 5.0021e-05\n",
            "Epoch 294/500\n",
            "800/800 [==============================] - 0s 252us/step - loss: 2.0539e-04 - val_loss: 0.0015\n",
            "Epoch 295/500\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.0020 - val_loss: 1.6988e-04\n",
            "Epoch 296/500\n",
            "800/800 [==============================] - 0s 214us/step - loss: 0.0021 - val_loss: 3.1500e-04\n",
            "Epoch 297/500\n",
            "800/800 [==============================] - 0s 237us/step - loss: 6.7365e-04 - val_loss: 0.0013\n",
            "Epoch 298/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "800/800 [==============================] - 0s 225us/step - loss: 6.7438e-04 - val_loss: 4.8778e-04\n",
            "Epoch 299/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 3.5151e-04 - val_loss: 5.7211e-05\n",
            "Epoch 300/500\n",
            "800/800 [==============================] - 0s 209us/step - loss: 1.0838e-04 - val_loss: 1.6140e-04\n",
            "Epoch 301/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 4.8848e-04 - val_loss: 0.0027\n",
            "Epoch 302/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 6.3243e-04 - val_loss: 8.1385e-05\n",
            "Epoch 303/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 4.0586e-04 - val_loss: 3.2316e-04\n",
            "Epoch 304/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 2.2773e-04 - val_loss: 0.0012\n",
            "Epoch 305/500\n",
            "800/800 [==============================] - 0s 238us/step - loss: 3.9666e-04 - val_loss: 1.8998e-04\n",
            "Epoch 306/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 0.0054 - val_loss: 3.3357e-04\n",
            "Epoch 307/500\n",
            "800/800 [==============================] - 0s 214us/step - loss: 3.7514e-04 - val_loss: 7.7699e-04\n",
            "Epoch 308/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 8.8875e-05 - val_loss: 3.2631e-05\n",
            "Epoch 309/500\n",
            "800/800 [==============================] - 0s 244us/step - loss: 1.9085e-04 - val_loss: 2.1406e-05\n",
            "Epoch 310/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 1.5629e-04 - val_loss: 7.7611e-04\n",
            "Epoch 311/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 3.0553e-04 - val_loss: 3.3009e-05\n",
            "Epoch 312/500\n",
            "800/800 [==============================] - 0s 207us/step - loss: 2.3178e-04 - val_loss: 0.0020\n",
            "Epoch 313/500\n",
            "800/800 [==============================] - 0s 214us/step - loss: 0.0041 - val_loss: 6.6456e-05\n",
            "Epoch 314/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 6.6687e-05 - val_loss: 3.9743e-05\n",
            "Epoch 315/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 1.3546e-04 - val_loss: 2.2694e-05\n",
            "Epoch 316/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 1.4118e-04 - val_loss: 2.0015e-04\n",
            "Epoch 317/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 2.2085e-04 - val_loss: 3.9249e-04\n",
            "Epoch 318/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 8.3267e-04 - val_loss: 0.0039\n",
            "Epoch 319/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 0.0040 - val_loss: 8.2890e-04\n",
            "Epoch 320/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 1.5878e-04 - val_loss: 2.2058e-05\n",
            "Epoch 321/500\n",
            "800/800 [==============================] - 0s 233us/step - loss: 1.1977e-04 - val_loss: 1.3246e-04\n",
            "Epoch 322/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 1.2652e-04 - val_loss: 1.3352e-04\n",
            "Epoch 323/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 0.0013 - val_loss: 1.8003e-04\n",
            "Epoch 324/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 7.2437e-04 - val_loss: 2.0373e-04\n",
            "Epoch 325/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 3.2854e-04 - val_loss: 2.0311e-04\n",
            "Epoch 326/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 9.7334e-04 - val_loss: 2.5476e-04\n",
            "Epoch 327/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 0.0025 - val_loss: 0.0014\n",
            "Epoch 328/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 8.6283e-04 - val_loss: 4.6243e-04\n",
            "Epoch 329/500\n",
            "800/800 [==============================] - 0s 216us/step - loss: 1.0065e-04 - val_loss: 1.8838e-05\n",
            "Epoch 330/500\n",
            "800/800 [==============================] - 0s 212us/step - loss: 1.5128e-04 - val_loss: 2.3952e-05\n",
            "Epoch 331/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 1.4722e-04 - val_loss: 6.5002e-04\n",
            "Epoch 332/500\n",
            "800/800 [==============================] - 0s 276us/step - loss: 2.1742e-04 - val_loss: 4.9253e-05\n",
            "Epoch 333/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 1.2550e-04 - val_loss: 1.5414e-04\n",
            "Epoch 334/500\n",
            "800/800 [==============================] - 0s 237us/step - loss: 5.7608e-04 - val_loss: 0.0019\n",
            "Epoch 335/500\n",
            "800/800 [==============================] - 0s 236us/step - loss: 0.0030 - val_loss: 0.0197\n",
            "Epoch 336/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0032 - val_loss: 3.6807e-05\n",
            "Epoch 337/500\n",
            "800/800 [==============================] - 0s 235us/step - loss: 1.4265e-04 - val_loss: 2.6323e-05\n",
            "Epoch 338/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 6.8706e-05 - val_loss: 2.6233e-04\n",
            "Epoch 339/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 8.5560e-05 - val_loss: 7.0070e-05\n",
            "Epoch 340/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 6.1369e-04 - val_loss: 1.1457e-04\n",
            "Epoch 341/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 4.2815e-04 - val_loss: 0.0016\n",
            "Epoch 342/500\n",
            "800/800 [==============================] - 0s 212us/step - loss: 0.0011 - val_loss: 8.0072e-05\n",
            "Epoch 343/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 2.7176e-04 - val_loss: 3.0164e-04\n",
            "Epoch 344/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0016 - val_loss: 0.0025\n",
            "Epoch 345/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 7.5536e-04 - val_loss: 1.3090e-04\n",
            "Epoch 346/500\n",
            "800/800 [==============================] - 0s 239us/step - loss: 6.4085e-05 - val_loss: 3.2845e-05\n",
            "Epoch 347/500\n",
            "800/800 [==============================] - 0s 233us/step - loss: 6.4682e-04 - val_loss: 1.3231e-04\n",
            "Epoch 348/500\n",
            "800/800 [==============================] - 0s 254us/step - loss: 1.2795e-04 - val_loss: 3.6326e-05\n",
            "Epoch 349/500\n",
            "800/800 [==============================] - 0s 234us/step - loss: 5.9693e-04 - val_loss: 8.8340e-05\n",
            "Epoch 350/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 8.8730e-04 - val_loss: 1.3518e-04\n",
            "Epoch 351/500\n",
            "800/800 [==============================] - 0s 233us/step - loss: 8.8953e-04 - val_loss: 4.6459e-04\n",
            "Epoch 352/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 5.1883e-04 - val_loss: 2.1050e-04\n",
            "Epoch 353/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 7.7104e-04 - val_loss: 0.0069\n",
            "Epoch 354/500\n",
            "800/800 [==============================] - 0s 237us/step - loss: 5.0173e-04 - val_loss: 0.0019\n",
            "Epoch 355/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 0.0040 - val_loss: 2.5145e-04\n",
            "Epoch 356/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 1.2381e-04 - val_loss: 3.3353e-05\n",
            "Epoch 357/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 7.3439e-05 - val_loss: 1.2229e-04\n",
            "Epoch 358/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 8.0434e-05 - val_loss: 9.2566e-05\n",
            "Epoch 359/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 2.1919e-04 - val_loss: 7.5731e-05\n",
            "Epoch 360/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 4.3845e-04 - val_loss: 2.2042e-04\n",
            "Epoch 361/500\n",
            "800/800 [==============================] - 0s 215us/step - loss: 2.9563e-04 - val_loss: 1.2695e-04\n",
            "Epoch 362/500\n",
            "800/800 [==============================] - 0s 249us/step - loss: 0.0012 - val_loss: 1.1173e-04\n",
            "Epoch 363/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 0.0032 - val_loss: 0.0036\n",
            "Epoch 364/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 4.0092e-04 - val_loss: 7.9043e-05\n",
            "Epoch 365/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 1.1670e-04 - val_loss: 4.4033e-04\n",
            "Epoch 366/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 4.1148e-04 - val_loss: 3.1992e-04\n",
            "Epoch 367/500\n",
            "800/800 [==============================] - 0s 233us/step - loss: 4.1605e-04 - val_loss: 8.7386e-04\n",
            "Epoch 368/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 1.7218e-04 - val_loss: 9.9249e-04\n",
            "Epoch 369/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 0.0093 - val_loss: 8.4351e-04\n",
            "Epoch 370/500\n",
            "800/800 [==============================] - 0s 236us/step - loss: 2.2043e-04 - val_loss: 1.3928e-04\n",
            "Epoch 371/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 1.1052e-04 - val_loss: 5.6140e-05\n",
            "Epoch 372/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 6.4832e-05 - val_loss: 6.9772e-05\n",
            "Epoch 373/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 1.2229e-04 - val_loss: 1.1551e-04\n",
            "Epoch 374/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 1.2625e-04 - val_loss: 1.0777e-04\n",
            "Epoch 375/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 9.5008e-05 - val_loss: 3.9626e-05\n",
            "Epoch 376/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 1.3930e-04 - val_loss: 2.0091e-05\n",
            "Epoch 377/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 6.4936e-05 - val_loss: 5.1570e-05\n",
            "Epoch 378/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 8.2563e-05 - val_loss: 8.6324e-05\n",
            "Epoch 379/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 0.0047 - val_loss: 4.5281e-04\n",
            "Epoch 380/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 2.3233e-04 - val_loss: 8.0744e-05\n",
            "Epoch 381/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 1.1984e-04 - val_loss: 6.1392e-05\n",
            "Epoch 382/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 1.0615e-04 - val_loss: 7.0528e-05\n",
            "Epoch 383/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 2.4405e-04 - val_loss: 2.7659e-04\n",
            "Epoch 384/500\n",
            "800/800 [==============================] - 0s 219us/step - loss: 2.2307e-04 - val_loss: 7.8920e-04\n",
            "Epoch 385/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 8.2163e-04 - val_loss: 0.0051\n",
            "Epoch 386/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 8.8543e-04 - val_loss: 2.6116e-04\n",
            "Epoch 387/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0094 - val_loss: 0.0021\n",
            "Epoch 388/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 2.8978e-04 - val_loss: 3.9825e-05\n",
            "Epoch 389/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 5.4587e-05 - val_loss: 2.9219e-05\n",
            "Epoch 390/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 7.8530e-05 - val_loss: 6.4069e-05\n",
            "Epoch 391/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 5.2274e-05 - val_loss: 9.2305e-05\n",
            "Epoch 392/500\n",
            "800/800 [==============================] - 0s 233us/step - loss: 1.2912e-04 - val_loss: 3.5114e-04\n",
            "Epoch 393/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 1.6437e-04 - val_loss: 1.4775e-05\n",
            "Epoch 394/500\n",
            "800/800 [==============================] - 0s 230us/step - loss: 3.4342e-04 - val_loss: 4.5664e-05\n",
            "Epoch 395/500\n",
            "800/800 [==============================] - 0s 217us/step - loss: 3.3180e-04 - val_loss: 2.8139e-04\n",
            "Epoch 396/500\n",
            "800/800 [==============================] - 0s 243us/step - loss: 1.3035e-04 - val_loss: 2.1299e-05\n",
            "Epoch 397/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 0.0012 - val_loss: 0.0027\n",
            "Epoch 398/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 4.0795e-04 - val_loss: 2.0845e-04\n",
            "Epoch 399/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 1.7599e-04 - val_loss: 6.5261e-04\n",
            "Epoch 400/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 0.0029 - val_loss: 2.0373e-04\n",
            "Epoch 401/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 3.1178e-04 - val_loss: 2.7336e-04\n",
            "Epoch 402/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 8.2424e-05 - val_loss: 1.4742e-04\n",
            "Epoch 403/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 2.7257e-04 - val_loss: 9.8240e-05\n",
            "Epoch 404/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 4.5581e-04 - val_loss: 1.9835e-04\n",
            "Epoch 405/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 6.8147e-04 - val_loss: 9.5599e-04\n",
            "Epoch 406/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 3.1492e-04 - val_loss: 8.8094e-05\n",
            "Epoch 407/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 1.0431e-04 - val_loss: 2.5792e-04\n",
            "Epoch 408/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 1.7642e-04 - val_loss: 1.7838e-04\n",
            "Epoch 409/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 0.0060 - val_loss: 4.5540e-05\n",
            "Epoch 410/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 7.6673e-05 - val_loss: 6.0318e-05\n",
            "Epoch 411/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 9.5030e-05 - val_loss: 6.0997e-05\n",
            "Epoch 412/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 7.6573e-05 - val_loss: 0.0011\n",
            "Epoch 413/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 4.0250e-04 - val_loss: 3.6156e-04\n",
            "Epoch 414/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 1.2482e-04 - val_loss: 6.6052e-05\n",
            "Epoch 415/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 4.4293e-04 - val_loss: 3.6951e-04\n",
            "Epoch 416/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 1.3007e-04 - val_loss: 3.5558e-04\n",
            "Epoch 417/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 1.4707e-04 - val_loss: 1.5667e-04\n",
            "Epoch 418/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 0.0051 - val_loss: 2.4527e-04\n",
            "Epoch 419/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 3.2531e-04 - val_loss: 1.1086e-04\n",
            "Epoch 420/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 1.9650e-04 - val_loss: 1.4687e-04\n",
            "Epoch 421/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 4.1420e-04 - val_loss: 5.4607e-05\n",
            "Epoch 422/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 1.8323e-04 - val_loss: 7.4441e-05\n",
            "Epoch 423/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 2.7323e-04 - val_loss: 1.9782e-05\n",
            "Epoch 424/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 2.3253e-04 - val_loss: 7.5153e-04\n",
            "Epoch 425/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 2.2500e-04 - val_loss: 4.8145e-05\n",
            "Epoch 426/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 7.5435e-05 - val_loss: 1.8270e-04\n",
            "Epoch 427/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 7.7354e-04 - val_loss: 0.0046\n",
            "Epoch 428/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 0.0013 - val_loss: 3.6701e-05\n",
            "Epoch 429/500\n",
            "800/800 [==============================] - 0s 234us/step - loss: 1.0475e-04 - val_loss: 6.0377e-05\n",
            "Epoch 430/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 1.1512e-04 - val_loss: 3.1763e-05\n",
            "Epoch 431/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 5.4078e-05 - val_loss: 5.2178e-05\n",
            "Epoch 432/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 1.0704e-04 - val_loss: 2.1392e-04\n",
            "Epoch 433/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 2.4163e-04 - val_loss: 6.1959e-04\n",
            "Epoch 434/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 0.0013 - val_loss: 6.3599e-04\n",
            "Epoch 435/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 4.9745e-04 - val_loss: 1.2482e-04\n",
            "Epoch 436/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 8.5660e-05 - val_loss: 1.5806e-04\n",
            "Epoch 437/500\n",
            "800/800 [==============================] - 0s 234us/step - loss: 1.0005e-04 - val_loss: 6.4969e-04\n",
            "Epoch 438/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 8.5413e-04 - val_loss: 6.3540e-04\n",
            "Epoch 439/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 0.0011 - val_loss: 2.7003e-04\n",
            "Epoch 440/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 4.3485e-04 - val_loss: 3.6789e-05\n",
            "Epoch 441/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 442/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0017 - val_loss: 1.8968e-04\n",
            "Epoch 443/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 1.3353e-04 - val_loss: 2.3416e-05\n",
            "Epoch 444/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "800/800 [==============================] - 0s 225us/step - loss: 2.8711e-04 - val_loss: 0.0019\n",
            "Epoch 445/500\n",
            "800/800 [==============================] - 0s 220us/step - loss: 3.0955e-04 - val_loss: 1.9412e-05\n",
            "Epoch 446/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 6.0918e-05 - val_loss: 2.8945e-04\n",
            "Epoch 447/500\n",
            "800/800 [==============================] - 0s 244us/step - loss: 0.0032 - val_loss: 1.6849e-04\n",
            "Epoch 448/500\n",
            "800/800 [==============================] - 0s 236us/step - loss: 4.1696e-04 - val_loss: 2.9544e-04\n",
            "Epoch 449/500\n",
            "800/800 [==============================] - 0s 218us/step - loss: 5.9038e-05 - val_loss: 6.9821e-05\n",
            "Epoch 450/500\n",
            "800/800 [==============================] - 0s 242us/step - loss: 0.0010 - val_loss: 8.6912e-04\n",
            "Epoch 451/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 3.5977e-04 - val_loss: 4.7556e-05\n",
            "Epoch 452/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 1.2496e-04 - val_loss: 1.9231e-05\n",
            "Epoch 453/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 4.9293e-04 - val_loss: 5.2836e-05\n",
            "Epoch 454/500\n",
            "800/800 [==============================] - 0s 220us/step - loss: 2.6083e-04 - val_loss: 8.7660e-05\n",
            "Epoch 455/500\n",
            "800/800 [==============================] - 0s 216us/step - loss: 1.0325e-04 - val_loss: 3.3074e-05\n",
            "Epoch 456/500\n",
            "800/800 [==============================] - 0s 256us/step - loss: 2.6845e-04 - val_loss: 5.7476e-04\n",
            "Epoch 457/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 0.0012 - val_loss: 1.0704e-04\n",
            "Epoch 458/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 9.7542e-04 - val_loss: 6.3713e-04\n",
            "Epoch 459/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 5.9994e-04 - val_loss: 6.0496e-05\n",
            "Epoch 460/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 1.2045e-04 - val_loss: 1.1634e-04\n",
            "Epoch 461/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 4.0106e-04 - val_loss: 2.6672e-05\n",
            "Epoch 462/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 3.4053e-04 - val_loss: 9.0180e-05\n",
            "Epoch 463/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 3.2980e-04 - val_loss: 1.9651e-04\n",
            "Epoch 464/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 7.6767e-04 - val_loss: 9.6140e-04\n",
            "Epoch 465/500\n",
            "800/800 [==============================] - 0s 227us/step - loss: 8.4970e-04 - val_loss: 3.9944e-04\n",
            "Epoch 466/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 2.0466e-04 - val_loss: 1.2324e-04\n",
            "Epoch 467/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 1.9582e-04 - val_loss: 8.1147e-05\n",
            "Epoch 468/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 9.7304e-05 - val_loss: 3.0618e-05\n",
            "Epoch 469/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 8.7386e-05 - val_loss: 0.0012\n",
            "Epoch 470/500\n",
            "800/800 [==============================] - 0s 216us/step - loss: 0.0019 - val_loss: 3.2263e-04\n",
            "Epoch 471/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 0.0036 - val_loss: 7.3528e-05\n",
            "Epoch 472/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 8.2900e-05 - val_loss: 6.2778e-05\n",
            "Epoch 473/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 4.9212e-05 - val_loss: 1.0100e-05\n",
            "Epoch 474/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 3.3826e-05 - val_loss: 3.3584e-05\n",
            "Epoch 475/500\n",
            "800/800 [==============================] - 0s 231us/step - loss: 9.2746e-05 - val_loss: 4.9024e-04\n",
            "Epoch 476/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 3.0395e-04 - val_loss: 9.1689e-04\n",
            "Epoch 477/500\n",
            "800/800 [==============================] - 0s 234us/step - loss: 2.6085e-04 - val_loss: 2.5538e-04\n",
            "Epoch 478/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 7.5012e-05 - val_loss: 1.0668e-04\n",
            "Epoch 479/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 6.7661e-04 - val_loss: 8.6088e-04\n",
            "Epoch 480/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 0.0031 - val_loss: 0.0112\n",
            "Epoch 481/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 0.0010 - val_loss: 1.5970e-04\n",
            "Epoch 482/500\n",
            "800/800 [==============================] - 0s 221us/step - loss: 1.5045e-04 - val_loss: 3.8009e-05\n",
            "Epoch 483/500\n",
            "800/800 [==============================] - 0s 228us/step - loss: 8.5516e-05 - val_loss: 1.1952e-05\n",
            "Epoch 484/500\n",
            "800/800 [==============================] - 0s 220us/step - loss: 4.3624e-05 - val_loss: 7.7716e-05\n",
            "Epoch 485/500\n",
            "800/800 [==============================] - 0s 234us/step - loss: 1.1689e-04 - val_loss: 2.3927e-05\n",
            "Epoch 486/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 1.1228e-04 - val_loss: 3.2018e-04\n",
            "Epoch 487/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 0.0011 - val_loss: 0.0060\n",
            "Epoch 488/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 0.0028 - val_loss: 0.0047\n",
            "Epoch 489/500\n",
            "800/800 [==============================] - 0s 226us/step - loss: 0.0036 - val_loss: 4.1563e-04\n",
            "Epoch 490/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 2.4279e-04 - val_loss: 3.5305e-05\n",
            "Epoch 491/500\n",
            "800/800 [==============================] - 0s 236us/step - loss: 4.8243e-05 - val_loss: 4.1314e-05\n",
            "Epoch 492/500\n",
            "800/800 [==============================] - 0s 211us/step - loss: 3.9582e-05 - val_loss: 3.5158e-05\n",
            "Epoch 493/500\n",
            "800/800 [==============================] - 0s 232us/step - loss: 6.8966e-05 - val_loss: 3.9492e-05\n",
            "Epoch 494/500\n",
            "800/800 [==============================] - 0s 234us/step - loss: 3.8026e-05 - val_loss: 1.3519e-05\n",
            "Epoch 495/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 9.3347e-05 - val_loss: 5.9598e-05\n",
            "Epoch 496/500\n",
            "800/800 [==============================] - 0s 235us/step - loss: 2.8466e-04 - val_loss: 0.0015\n",
            "Epoch 497/500\n",
            "800/800 [==============================] - 0s 222us/step - loss: 0.0018 - val_loss: 5.1867e-05\n",
            "Epoch 498/500\n",
            "800/800 [==============================] - 0s 229us/step - loss: 1.4981e-04 - val_loss: 1.2752e-04\n",
            "Epoch 499/500\n",
            "800/800 [==============================] - 0s 224us/step - loss: 4.0881e-04 - val_loss: 1.3960e-04\n",
            "Epoch 500/500\n",
            "800/800 [==============================] - 0s 223us/step - loss: 9.6094e-05 - val_loss: 3.0402e-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x22c1fe0af88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoeMOrT0eRmp"
      },
      "source": [
        "x_test=np.ones((1,1,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlnvOTMjeRmp",
        "outputId": "0a13dbbd-5ee9-44f6-bcbf-b3a114b8ea31"
      },
      "source": [
        "x_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1., 1.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r-IE5veeRmp"
      },
      "source": [
        "X_test=np.array([[1,]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-MkJJDPeRmp",
        "outputId": "d082fef5-e0cc-46cd-dba4-cd862a024309"
      },
      "source": [
        "model.predict(np.array([[[10.,10.]]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[29.98069]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuRg96ELeRmq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yJ-9q8teRmq"
      },
      "source": [
        "for i in range(1000):\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9wXlF9QeRmq"
      },
      "source": [
        "## Many to One LSTM Problems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbXG7erieRmq",
        "outputId": "6db7e26e-62b4-43ab-e98d-3f6ff2ab52c3"
      },
      "source": [
        "X = np.array([x+1 for x in range(45)])\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
            " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnsso_BYeRmq",
        "outputId": "cbe82ccd-4462-456f-98d1-8f30dbfaa914"
      },
      "source": [
        "X = X.reshape(15,3,1)\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 1]\n",
            "  [ 2]\n",
            "  [ 3]]\n",
            "\n",
            " [[ 4]\n",
            "  [ 5]\n",
            "  [ 6]]\n",
            "\n",
            " [[ 7]\n",
            "  [ 8]\n",
            "  [ 9]]\n",
            "\n",
            " [[10]\n",
            "  [11]\n",
            "  [12]]\n",
            "\n",
            " [[13]\n",
            "  [14]\n",
            "  [15]]\n",
            "\n",
            " [[16]\n",
            "  [17]\n",
            "  [18]]\n",
            "\n",
            " [[19]\n",
            "  [20]\n",
            "  [21]]\n",
            "\n",
            " [[22]\n",
            "  [23]\n",
            "  [24]]\n",
            "\n",
            " [[25]\n",
            "  [26]\n",
            "  [27]]\n",
            "\n",
            " [[28]\n",
            "  [29]\n",
            "  [30]]\n",
            "\n",
            " [[31]\n",
            "  [32]\n",
            "  [33]]\n",
            "\n",
            " [[34]\n",
            "  [35]\n",
            "  [36]]\n",
            "\n",
            " [[37]\n",
            "  [38]\n",
            "  [39]]\n",
            "\n",
            " [[40]\n",
            "  [41]\n",
            "  [42]]\n",
            "\n",
            " [[43]\n",
            "  [44]\n",
            "  [45]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1lkCzaZeRmq",
        "outputId": "fde78e9f-a59b-45fe-bb20-5f62fcfdb9af"
      },
      "source": [
        "Y = list()\n",
        "for x in X:\n",
        "    Y.append(x.sum())\n",
        "\n",
        "Y = np.array(Y)\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  6  15  24  33  42  51  60  69  78  87  96 105 114 123 132]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqv9OAP1eRmq",
        "outputId": "ba618c62-9c15-4dac-9a18-002004e35fa5"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(20, activation='relu', input_shape=(3, 1)))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_15 (LSTM)               (None, 20)                1760      \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 1,981\n",
            "Trainable params: 1,981\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhY5MntYeRmq",
        "outputId": "19405fc8-6093-40ec-827d-167c638e7223"
      },
      "source": [
        "model.fit(X, Y, epochs=500, validation_split=0.2, shuffle=True, batch_size=5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12 samples, validate on 3 samples\n",
            "Epoch 1/500\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 4584.1268 - val_loss: 17253.9980\n",
            "Epoch 2/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 4546.2396 - val_loss: 17128.7969\n",
            "Epoch 3/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 4510.1804 - val_loss: 16996.3945\n",
            "Epoch 4/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 4477.9711 - val_loss: 16861.6699\n",
            "Epoch 5/500\n",
            "12/12 [==============================] - 0s 914us/step - loss: 4437.3061 - val_loss: 16731.6973\n",
            "Epoch 6/500\n",
            "12/12 [==============================] - 0s 831us/step - loss: 4397.9625 - val_loss: 16597.9531\n",
            "Epoch 7/500\n",
            "12/12 [==============================] - 0s 914us/step - loss: 4358.7086 - val_loss: 16457.8398\n",
            "Epoch 8/500\n",
            "12/12 [==============================] - 0s 831us/step - loss: 4315.6039 - val_loss: 16298.6484\n",
            "Epoch 9/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 4266.9693 - val_loss: 16102.4365\n",
            "Epoch 10/500\n",
            "12/12 [==============================] - 0s 831us/step - loss: 4219.1579 - val_loss: 15883.5146\n",
            "Epoch 11/500\n",
            "12/12 [==============================] - 0s 831us/step - loss: 4153.5510 - val_loss: 15621.3750\n",
            "Epoch 12/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 4093.1089 - val_loss: 15285.0967\n",
            "Epoch 13/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 4005.7481 - val_loss: 14870.0420\n",
            "Epoch 14/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 3922.3808 - val_loss: 14460.1299\n",
            "Epoch 15/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 3844.0825 - val_loss: 14265.7734\n",
            "Epoch 16/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 3801.6836 - val_loss: 14056.4307\n",
            "Epoch 17/500\n",
            "12/12 [==============================] - 0s 831us/step - loss: 3753.1298 - val_loss: 13793.3799\n",
            "Epoch 18/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 3669.9965 - val_loss: 13479.3252\n",
            "Epoch 19/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 3597.7046 - val_loss: 13173.2529\n",
            "Epoch 20/500\n",
            "12/12 [==============================] - 0s 749us/step - loss: 3514.0397 - val_loss: 12744.2539\n",
            "Epoch 21/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 3420.6612 - val_loss: 12304.1533\n",
            "Epoch 22/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 3303.0971 - val_loss: 11917.9814\n",
            "Epoch 23/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 3192.5979 - val_loss: 11496.8047\n",
            "Epoch 24/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 3066.3021 - val_loss: 11016.5420\n",
            "Epoch 25/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 2966.6079 - val_loss: 10436.8857\n",
            "Epoch 26/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 2812.0680 - val_loss: 9805.0869\n",
            "Epoch 27/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2652.1942 - val_loss: 9121.7197\n",
            "Epoch 28/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2509.1466 - val_loss: 8391.9795\n",
            "Epoch 29/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2314.7152 - val_loss: 7683.9126\n",
            "Epoch 30/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 2120.9244 - val_loss: 6967.4272\n",
            "Epoch 31/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 1924.7231 - val_loss: 6241.3521\n",
            "Epoch 32/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 1717.1157 - val_loss: 5507.8765\n",
            "Epoch 33/500\n",
            "12/12 [==============================] - 0s 831us/step - loss: 1512.3852 - val_loss: 4770.4634\n",
            "Epoch 34/500\n",
            "12/12 [==============================] - ETA: 0s - loss: 815.956 - 0s 665us/step - loss: 1297.5317 - val_loss: 4039.5410\n",
            "Epoch 35/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1121.0719 - val_loss: 3290.8728\n",
            "Epoch 36/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 905.0208 - val_loss: 2543.5925\n",
            "Epoch 37/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 704.5142 - val_loss: 1802.4987\n",
            "Epoch 38/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 503.7734 - val_loss: 1163.6727\n",
            "Epoch 39/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 335.4736 - val_loss: 659.8234\n",
            "Epoch 40/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 192.4524 - val_loss: 283.8615\n",
            "Epoch 41/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 87.6911 - val_loss: 63.2438\n",
            "Epoch 42/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 31.0853 - val_loss: 0.0204\n",
            "Epoch 43/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 10.9383 - val_loss: 42.0422\n",
            "Epoch 44/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 15.5799 - val_loss: 119.2345\n",
            "Epoch 45/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 32.6694 - val_loss: 175.7026\n",
            "Epoch 46/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 41.0752 - val_loss: 176.2015\n",
            "Epoch 47/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 38.2948 - val_loss: 134.0873\n",
            "Epoch 48/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 28.9940 - val_loss: 82.8071\n",
            "Epoch 49/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 17.9481 - val_loss: 43.1447\n",
            "Epoch 50/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 11.8206 - val_loss: 16.9474\n",
            "Epoch 51/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 9.3634 - val_loss: 4.1034\n",
            "Epoch 52/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 8.6292 - val_loss: 0.3136\n",
            "Epoch 53/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 9.5126 - val_loss: 0.1349\n",
            "Epoch 54/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 10.3660 - val_loss: 0.5545\n",
            "Epoch 55/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 10.6111 - val_loss: 0.4637\n",
            "Epoch 56/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 10.2263 - val_loss: 0.0915\n",
            "Epoch 57/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 9.5286 - val_loss: 0.0794\n",
            "Epoch 58/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 8.7622 - val_loss: 0.7816\n",
            "Epoch 59/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 8.1650 - val_loss: 2.1334\n",
            "Epoch 60/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 7.8074 - val_loss: 3.8553\n",
            "Epoch 61/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 7.6064 - val_loss: 5.6912\n",
            "Epoch 62/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 7.6124 - val_loss: 7.2912\n",
            "Epoch 63/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 7.5901 - val_loss: 8.0304\n",
            "Epoch 64/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 7.5687 - val_loss: 8.2258\n",
            "Epoch 65/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 7.4641 - val_loss: 7.3896\n",
            "Epoch 66/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 7.3397 - val_loss: 6.4084\n",
            "Epoch 67/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 7.1482 - val_loss: 5.8498\n",
            "Epoch 68/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 7.0098 - val_loss: 5.2827\n",
            "Epoch 69/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 6.8688 - val_loss: 4.5642\n",
            "Epoch 70/500\n",
            "12/12 [==============================] - 0s 667us/step - loss: 6.7595 - val_loss: 3.7044\n",
            "Epoch 71/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 6.6546 - val_loss: 3.0727\n",
            "Epoch 72/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 6.5618 - val_loss: 2.7969\n",
            "Epoch 73/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 6.4760 - val_loss: 2.3508\n",
            "Epoch 74/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 6.3805 - val_loss: 2.1335\n",
            "Epoch 75/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 6.2926 - val_loss: 2.1525\n",
            "Epoch 76/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 6.2010 - val_loss: 2.2084\n",
            "Epoch 77/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 6.1254 - val_loss: 2.2928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 78/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 6.0188 - val_loss: 2.5229\n",
            "Epoch 79/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 5.9561 - val_loss: 2.8692\n",
            "Epoch 80/500\n",
            "12/12 [==============================] - 0s 831us/step - loss: 5.8973 - val_loss: 3.0396\n",
            "Epoch 81/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 5.7934 - val_loss: 2.7226\n",
            "Epoch 82/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 5.7207 - val_loss: 2.5708\n",
            "Epoch 83/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 5.6294 - val_loss: 2.3395\n",
            "Epoch 84/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 5.5540 - val_loss: 2.1906\n",
            "Epoch 85/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 5.4725 - val_loss: 1.9308\n",
            "Epoch 86/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 5.4205 - val_loss: 1.5850\n",
            "Epoch 87/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 5.3383 - val_loss: 1.5654\n",
            "Epoch 88/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 5.2596 - val_loss: 1.4533\n",
            "Epoch 89/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 5.1724 - val_loss: 1.5290\n",
            "Epoch 90/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 5.0960 - val_loss: 1.7036\n",
            "Epoch 91/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 5.0260 - val_loss: 1.8582\n",
            "Epoch 92/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 4.9482 - val_loss: 1.8311\n",
            "Epoch 93/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 4.8783 - val_loss: 1.7626\n",
            "Epoch 94/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 4.7857 - val_loss: 1.7787\n",
            "Epoch 95/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 4.7609 - val_loss: 1.8819\n",
            "Epoch 96/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 4.6532 - val_loss: 1.5570\n",
            "Epoch 97/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 4.5634 - val_loss: 1.1632\n",
            "Epoch 98/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 4.4828 - val_loss: 0.8067\n",
            "Epoch 99/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 4.4397 - val_loss: 0.4978\n",
            "Epoch 100/500\n",
            "12/12 [==============================] - 0s 583us/step - loss: 4.3977 - val_loss: 0.3734\n",
            "Epoch 101/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 4.3419 - val_loss: 0.3926\n",
            "Epoch 102/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 4.2692 - val_loss: 0.4719\n",
            "Epoch 103/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 4.1923 - val_loss: 0.5714\n",
            "Epoch 104/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 4.1059 - val_loss: 0.6098\n",
            "Epoch 105/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 4.0461 - val_loss: 0.6903\n",
            "Epoch 106/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 3.9783 - val_loss: 0.7415\n",
            "Epoch 107/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 3.9236 - val_loss: 0.7906\n",
            "Epoch 108/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 3.8536 - val_loss: 0.8003\n",
            "Epoch 109/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 3.7952 - val_loss: 0.8862\n",
            "Epoch 110/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 3.7616 - val_loss: 0.9971\n",
            "Epoch 111/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 3.6807 - val_loss: 0.8892\n",
            "Epoch 112/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 3.6021 - val_loss: 0.7813\n",
            "Epoch 113/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 3.5445 - val_loss: 0.6976\n",
            "Epoch 114/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 3.4609 - val_loss: 0.4682\n",
            "Epoch 115/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 3.3942 - val_loss: 0.2508\n",
            "Epoch 116/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 3.3298 - val_loss: 0.1270\n",
            "Epoch 117/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 3.2865 - val_loss: 0.0603\n",
            "Epoch 118/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 3.2318 - val_loss: 0.0549\n",
            "Epoch 119/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 3.1656 - val_loss: 0.0759\n",
            "Epoch 120/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 3.1007 - val_loss: 0.0956\n",
            "Epoch 121/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 3.0301 - val_loss: 0.1072\n",
            "Epoch 122/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2.9602 - val_loss: 0.1538\n",
            "Epoch 123/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2.9163 - val_loss: 0.2278\n",
            "Epoch 124/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 2.8524 - val_loss: 0.2479\n",
            "Epoch 125/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 2.7925 - val_loss: 0.2477\n",
            "Epoch 126/500\n",
            "12/12 [==============================] - 0s 580us/step - loss: 2.7508 - val_loss: 0.2237\n",
            "Epoch 127/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 2.6788 - val_loss: 0.1369\n",
            "Epoch 128/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 2.6197 - val_loss: 0.0769\n",
            "Epoch 129/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 2.5514 - val_loss: 0.0245\n",
            "Epoch 130/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2.4994 - val_loss: 0.0052\n",
            "Epoch 131/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2.4481 - val_loss: 0.0133\n",
            "Epoch 132/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2.4031 - val_loss: 0.0314\n",
            "Epoch 133/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2.3551 - val_loss: 0.0489\n",
            "Epoch 134/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 2.3089 - val_loss: 0.0541\n",
            "Epoch 135/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 2.2727 - val_loss: 0.0486\n",
            "Epoch 136/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 2.2105 - val_loss: 0.0523\n",
            "Epoch 137/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2.1742 - val_loss: 0.0528\n",
            "Epoch 138/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2.1280 - val_loss: 0.0580\n",
            "Epoch 139/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2.1005 - val_loss: 0.0545\n",
            "Epoch 140/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2.0587 - val_loss: 0.0759\n",
            "Epoch 141/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2.0075 - val_loss: 0.1078\n",
            "Epoch 142/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.9713 - val_loss: 0.1356\n",
            "Epoch 143/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.9365 - val_loss: 0.1619\n",
            "Epoch 144/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.9048 - val_loss: 0.2027\n",
            "Epoch 145/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.8656 - val_loss: 0.2605\n",
            "Epoch 146/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.8292 - val_loss: 0.3081\n",
            "Epoch 147/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.8000 - val_loss: 0.3211\n",
            "Epoch 148/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.7729 - val_loss: 0.3380\n",
            "Epoch 149/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.7334 - val_loss: 0.3901\n",
            "Epoch 150/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.7053 - val_loss: 0.4064\n",
            "Epoch 151/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.6715 - val_loss: 0.4249\n",
            "Epoch 152/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.6430 - val_loss: 0.4533\n",
            "Epoch 153/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.6168 - val_loss: 0.4768\n",
            "Epoch 154/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.5947 - val_loss: 0.4988\n",
            "Epoch 155/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.5609 - val_loss: 0.5396\n",
            "Epoch 156/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.5446 - val_loss: 0.5445\n",
            "Epoch 157/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.5143 - val_loss: 0.5889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 158/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.4950 - val_loss: 0.5438\n",
            "Epoch 159/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.4543 - val_loss: 0.5018\n",
            "Epoch 160/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.4423 - val_loss: 0.4205\n",
            "Epoch 161/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.4124 - val_loss: 0.4099\n",
            "Epoch 162/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.3933 - val_loss: 0.4007\n",
            "Epoch 163/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 1.3753 - val_loss: 0.4107\n",
            "Epoch 164/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.3570 - val_loss: 0.4656\n",
            "Epoch 165/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.3356 - val_loss: 0.6221\n",
            "Epoch 166/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.3012 - val_loss: 0.7040\n",
            "Epoch 167/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.2860 - val_loss: 0.7572\n",
            "Epoch 168/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.2700 - val_loss: 0.7770\n",
            "Epoch 169/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.2483 - val_loss: 0.7224\n",
            "Epoch 170/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.2271 - val_loss: 0.6720\n",
            "Epoch 171/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.2109 - val_loss: 0.5868\n",
            "Epoch 172/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.1928 - val_loss: 0.5636\n",
            "Epoch 173/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 1.1747 - val_loss: 0.5344\n",
            "Epoch 174/500\n",
            "12/12 [==============================] - 0s 581us/step - loss: 1.1570 - val_loss: 0.5806\n",
            "Epoch 175/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.1357 - val_loss: 0.6056\n",
            "Epoch 176/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.1248 - val_loss: 0.6161\n",
            "Epoch 177/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.1040 - val_loss: 0.6850\n",
            "Epoch 178/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 1.0862 - val_loss: 0.6740\n",
            "Epoch 179/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 1.0680 - val_loss: 0.6552\n",
            "Epoch 180/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.0511 - val_loss: 0.6143\n",
            "Epoch 181/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 1.0300 - val_loss: 0.5280\n",
            "Epoch 182/500\n",
            "12/12 [==============================] - 0s 580us/step - loss: 1.0189 - val_loss: 0.4505\n",
            "Epoch 183/500\n",
            "12/12 [==============================] - 0s 581us/step - loss: 1.0104 - val_loss: 0.3959\n",
            "Epoch 184/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.9999 - val_loss: 0.4132\n",
            "Epoch 185/500\n",
            "12/12 [==============================] - 0s 583us/step - loss: 0.9835 - val_loss: 0.5000\n",
            "Epoch 186/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.9620 - val_loss: 0.5734\n",
            "Epoch 187/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.9453 - val_loss: 0.5695\n",
            "Epoch 188/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.9262 - val_loss: 0.5627\n",
            "Epoch 189/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.9107 - val_loss: 0.5948\n",
            "Epoch 190/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.8987 - val_loss: 0.6103\n",
            "Epoch 191/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.8852 - val_loss: 0.6054\n",
            "Epoch 192/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.8668 - val_loss: 0.6525\n",
            "Epoch 193/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.8573 - val_loss: 0.6942\n",
            "Epoch 194/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.8449 - val_loss: 0.6564\n",
            "Epoch 195/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.8292 - val_loss: 0.6492\n",
            "Epoch 196/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.8157 - val_loss: 0.6478\n",
            "Epoch 197/500\n",
            "12/12 [==============================] - 0s 581us/step - loss: 0.8052 - val_loss: 0.6382\n",
            "Epoch 198/500\n",
            "12/12 [==============================] - 0s 664us/step - loss: 0.7911 - val_loss: 0.6299\n",
            "Epoch 199/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.7801 - val_loss: 0.6331\n",
            "Epoch 200/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.7680 - val_loss: 0.6241\n",
            "Epoch 201/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.7575 - val_loss: 0.6237\n",
            "Epoch 202/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.7531 - val_loss: 0.5809\n",
            "Epoch 203/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.7348 - val_loss: 0.6166\n",
            "Epoch 204/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.7252 - val_loss: 0.6391\n",
            "Epoch 205/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.7179 - val_loss: 0.6463\n",
            "Epoch 206/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.7131 - val_loss: 0.5858\n",
            "Epoch 207/500\n",
            "12/12 [==============================] - 0s 831us/step - loss: 0.6962 - val_loss: 0.6171\n",
            "Epoch 208/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.6863 - val_loss: 0.6679\n",
            "Epoch 209/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.6791 - val_loss: 0.7196\n",
            "Epoch 210/500\n",
            "12/12 [==============================] - 0s 750us/step - loss: 0.6737 - val_loss: 0.7737\n",
            "Epoch 211/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.6696 - val_loss: 0.7474\n",
            "Epoch 212/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.6598 - val_loss: 0.6864\n",
            "Epoch 213/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.6487 - val_loss: 0.5939\n",
            "Epoch 214/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.6366 - val_loss: 0.5461\n",
            "Epoch 215/500\n",
            "12/12 [==============================] - 0s 831us/step - loss: 0.6291 - val_loss: 0.4827\n",
            "Epoch 216/500\n",
            "12/12 [==============================] - 0s 749us/step - loss: 0.6187 - val_loss: 0.4269\n",
            "Epoch 217/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.6131 - val_loss: 0.3437\n",
            "Epoch 218/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.6033 - val_loss: 0.3079\n",
            "Epoch 219/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.5970 - val_loss: 0.3001\n",
            "Epoch 220/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.5901 - val_loss: 0.3116\n",
            "Epoch 221/500\n",
            "12/12 [==============================] - 0s 747us/step - loss: 0.5766 - val_loss: 0.3284\n",
            "Epoch 222/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.5692 - val_loss: 0.3789\n",
            "Epoch 223/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.5550 - val_loss: 0.4715\n",
            "Epoch 224/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.5456 - val_loss: 0.5841\n",
            "Epoch 225/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.5417 - val_loss: 0.6477\n",
            "Epoch 226/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.5373 - val_loss: 0.6583\n",
            "Epoch 227/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.5310 - val_loss: 0.6370\n",
            "Epoch 228/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.5221 - val_loss: 0.5848\n",
            "Epoch 229/500\n",
            "12/12 [==============================] - 0s 831us/step - loss: 0.5111 - val_loss: 0.4910\n",
            "Epoch 230/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.5045 - val_loss: 0.3461\n",
            "Epoch 231/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.4952 - val_loss: 0.2833\n",
            "Epoch 232/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.4908 - val_loss: 0.2796\n",
            "Epoch 233/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.4812 - val_loss: 0.3511\n",
            "Epoch 234/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.4700 - val_loss: 0.4325\n",
            "Epoch 235/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.4623 - val_loss: 0.4904\n",
            "Epoch 236/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.4574 - val_loss: 0.4830\n",
            "Epoch 237/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.4489 - val_loss: 0.4352\n",
            "Epoch 238/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.4386 - val_loss: 0.3489\n",
            "Epoch 239/500\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.4323 - val_loss: 0.2990\n",
            "Epoch 240/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.4254 - val_loss: 0.2471\n",
            "Epoch 241/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.4211 - val_loss: 0.2091\n",
            "Epoch 242/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.4151 - val_loss: 0.2253\n",
            "Epoch 243/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.4075 - val_loss: 0.3208\n",
            "Epoch 244/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.4002 - val_loss: 0.3849\n",
            "Epoch 245/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.3896 - val_loss: 0.2833\n",
            "Epoch 246/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.3802 - val_loss: 0.2078\n",
            "Epoch 247/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.3798 - val_loss: 0.1701\n",
            "Epoch 248/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.3734 - val_loss: 0.2109\n",
            "Epoch 249/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.3641 - val_loss: 0.2880\n",
            "Epoch 250/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.3581 - val_loss: 0.2975\n",
            "Epoch 251/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.3520 - val_loss: 0.1928\n",
            "Epoch 252/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.3474 - val_loss: 0.1775\n",
            "Epoch 253/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.3423 - val_loss: 0.2357\n",
            "Epoch 254/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.3304 - val_loss: 0.2141\n",
            "Epoch 255/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.3243 - val_loss: 0.2328\n",
            "Epoch 256/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.3197 - val_loss: 0.2835\n",
            "Epoch 257/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.3142 - val_loss: 0.2366\n",
            "Epoch 258/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.3049 - val_loss: 0.2063\n",
            "Epoch 259/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.3009 - val_loss: 0.1688\n",
            "Epoch 260/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.2961 - val_loss: 0.1287\n",
            "Epoch 261/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.2909 - val_loss: 0.1896\n",
            "Epoch 262/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.2807 - val_loss: 0.2512\n",
            "Epoch 263/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.2797 - val_loss: 0.2592\n",
            "Epoch 264/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.2755 - val_loss: 0.1600\n",
            "Epoch 265/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.2682 - val_loss: 0.1202\n",
            "Epoch 266/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.2616 - val_loss: 0.1461\n",
            "Epoch 267/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.2572 - val_loss: 0.1664\n",
            "Epoch 268/500\n",
            "12/12 [==============================] - 0s 500us/step - loss: 0.2537 - val_loss: 0.1154\n",
            "Epoch 269/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.2472 - val_loss: 0.0682\n",
            "Epoch 270/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.2434 - val_loss: 0.0654\n",
            "Epoch 271/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.2374 - val_loss: 0.0795\n",
            "Epoch 272/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.2338 - val_loss: 0.0917\n",
            "Epoch 273/500\n",
            "12/12 [==============================] - 0s 581us/step - loss: 0.2269 - val_loss: 0.0535\n",
            "Epoch 274/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.2240 - val_loss: 0.0845\n",
            "Epoch 275/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.2166 - val_loss: 0.1418\n",
            "Epoch 276/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.2145 - val_loss: 0.1013\n",
            "Epoch 277/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.2090 - val_loss: 0.0689\n",
            "Epoch 278/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.2040 - val_loss: 0.0382\n",
            "Epoch 279/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1983 - val_loss: 0.0260\n",
            "Epoch 280/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1955 - val_loss: 0.0330\n",
            "Epoch 281/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1908 - val_loss: 0.0547\n",
            "Epoch 282/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1859 - val_loss: 0.0172\n",
            "Epoch 283/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1844 - val_loss: 0.0220\n",
            "Epoch 284/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1821 - val_loss: 0.0468\n",
            "Epoch 285/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.1759 - val_loss: 0.0103\n",
            "Epoch 286/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1749 - val_loss: 0.0026\n",
            "Epoch 287/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.1702 - val_loss: 0.0377\n",
            "Epoch 288/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.1675 - val_loss: 0.0292\n",
            "Epoch 289/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.1609 - val_loss: 0.0057\n",
            "Epoch 290/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1578 - val_loss: 0.0082\n",
            "Epoch 291/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1527 - val_loss: 0.0249\n",
            "Epoch 292/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1499 - val_loss: 0.0094\n",
            "Epoch 293/500\n",
            "12/12 [==============================] - 0s 583us/step - loss: 0.1454 - val_loss: 5.6498e-04\n",
            "Epoch 294/500\n",
            "12/12 [==============================] - 0s 498us/step - loss: 0.1440 - val_loss: 0.0116\n",
            "Epoch 295/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.1408 - val_loss: 0.0074\n",
            "Epoch 296/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1371 - val_loss: 8.9132e-04\n",
            "Epoch 297/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1344 - val_loss: 0.0043\n",
            "Epoch 298/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1312 - val_loss: 0.0012\n",
            "Epoch 299/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.1287 - val_loss: 2.1633e-04\n",
            "Epoch 300/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1275 - val_loss: 1.3877e-04\n",
            "Epoch 301/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.1238 - val_loss: 0.0057\n",
            "Epoch 302/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1222 - val_loss: 4.2217e-04\n",
            "Epoch 303/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1197 - val_loss: 6.2915e-04\n",
            "Epoch 304/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1177 - val_loss: 4.6783e-04\n",
            "Epoch 305/500\n",
            "12/12 [==============================] - 0s 583us/step - loss: 0.1158 - val_loss: 0.0031\n",
            "Epoch 306/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1135 - val_loss: 4.4054e-04\n",
            "Epoch 307/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.1127 - val_loss: 0.0013\n",
            "Epoch 308/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1101 - val_loss: 0.0022\n",
            "Epoch 309/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1076 - val_loss: 7.4568e-04\n",
            "Epoch 310/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.1065 - val_loss: 0.0044\n",
            "Epoch 311/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.1051 - val_loss: 0.0024\n",
            "Epoch 312/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1035 - val_loss: 0.0038\n",
            "Epoch 313/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1019 - val_loss: 0.0181\n",
            "Epoch 314/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.1012 - val_loss: 7.3277e-04\n",
            "Epoch 315/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0997 - val_loss: 0.0069\n",
            "Epoch 316/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 0s 499us/step - loss: 0.0977 - val_loss: 0.0063\n",
            "Epoch 317/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.0963 - val_loss: 0.0029\n",
            "Epoch 318/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0942 - val_loss: 0.0358\n",
            "Epoch 319/500\n",
            "12/12 [==============================] - 0s 581us/step - loss: 0.0964 - val_loss: 8.5841e-04\n",
            "Epoch 320/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.0949 - val_loss: 0.0119\n",
            "Epoch 321/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0938 - val_loss: 0.0258\n",
            "Epoch 322/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0897 - val_loss: 9.1649e-04\n",
            "Epoch 323/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0885 - val_loss: 0.0199\n",
            "Epoch 324/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0871 - val_loss: 0.0112\n",
            "Epoch 325/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0849 - val_loss: 0.0043\n",
            "Epoch 326/500\n",
            "12/12 [==============================] - 0s 583us/step - loss: 0.0843 - val_loss: 0.0156\n",
            "Epoch 327/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0822 - val_loss: 0.0109\n",
            "Epoch 328/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0814 - val_loss: 0.0047\n",
            "Epoch 329/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0796 - val_loss: 0.0155\n",
            "Epoch 330/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0786 - val_loss: 0.0102\n",
            "Epoch 331/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0775 - val_loss: 0.0092\n",
            "Epoch 332/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0768 - val_loss: 0.0191\n",
            "Epoch 333/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0747 - val_loss: 0.0053\n",
            "Epoch 334/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0739 - val_loss: 0.0199\n",
            "Epoch 335/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0736 - val_loss: 0.0094\n",
            "Epoch 336/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0722 - val_loss: 0.0116\n",
            "Epoch 337/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0706 - val_loss: 0.0320\n",
            "Epoch 338/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0706 - val_loss: 0.0116\n",
            "Epoch 339/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0690 - val_loss: 0.0076\n",
            "Epoch 340/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0686 - val_loss: 0.0185\n",
            "Epoch 341/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0675 - val_loss: 0.0124\n",
            "Epoch 342/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0662 - val_loss: 0.0095\n",
            "Epoch 343/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0660 - val_loss: 0.0119\n",
            "Epoch 344/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0643 - val_loss: 0.0176\n",
            "Epoch 345/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0634 - val_loss: 0.0081\n",
            "Epoch 346/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0633 - val_loss: 0.0173\n",
            "Epoch 347/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0623 - val_loss: 0.0197\n",
            "Epoch 348/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0614 - val_loss: 0.0095\n",
            "Epoch 349/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0599 - val_loss: 0.0231\n",
            "Epoch 350/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0596 - val_loss: 0.0074\n",
            "Epoch 351/500\n",
            "12/12 [==============================] - 0s 663us/step - loss: 0.0588 - val_loss: 0.0192\n",
            "Epoch 352/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0580 - val_loss: 0.0233\n",
            "Epoch 353/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0570 - val_loss: 0.0081\n",
            "Epoch 354/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0565 - val_loss: 0.0298\n",
            "Epoch 355/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0553 - val_loss: 0.0121\n",
            "Epoch 356/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0547 - val_loss: 0.0219\n",
            "Epoch 357/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0549 - val_loss: 0.0177\n",
            "Epoch 358/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0525 - val_loss: 0.0151\n",
            "Epoch 359/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0519 - val_loss: 0.0247\n",
            "Epoch 360/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0510 - val_loss: 0.0111\n",
            "Epoch 361/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0504 - val_loss: 0.0246\n",
            "Epoch 362/500\n",
            "12/12 [==============================] - 0s 752us/step - loss: 0.0502 - val_loss: 0.0255\n",
            "Epoch 363/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0494 - val_loss: 0.0127\n",
            "Epoch 364/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0492 - val_loss: 0.0200\n",
            "Epoch 365/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0481 - val_loss: 0.0268\n",
            "Epoch 366/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0475 - val_loss: 0.0047\n",
            "Epoch 367/500\n",
            "12/12 [==============================] - 0s 747us/step - loss: 0.0477 - val_loss: 0.0177\n",
            "Epoch 368/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0465 - val_loss: 0.0152\n",
            "Epoch 369/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0461 - val_loss: 0.0074\n",
            "Epoch 370/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0460 - val_loss: 0.0188\n",
            "Epoch 371/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0453 - val_loss: 0.0079\n",
            "Epoch 372/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0441 - val_loss: 0.0314\n",
            "Epoch 373/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0451 - val_loss: 0.0087\n",
            "Epoch 374/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0440 - val_loss: 0.0084\n",
            "Epoch 375/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0430 - val_loss: 0.0272\n",
            "Epoch 376/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0427 - val_loss: 0.0072\n",
            "Epoch 377/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0416 - val_loss: 0.0179\n",
            "Epoch 378/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0411 - val_loss: 0.0108\n",
            "Epoch 379/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0402 - val_loss: 0.0079\n",
            "Epoch 380/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0397 - val_loss: 0.0171\n",
            "Epoch 381/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0393 - val_loss: 0.0134\n",
            "Epoch 382/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0390 - val_loss: 0.0077\n",
            "Epoch 383/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0382 - val_loss: 0.0263\n",
            "Epoch 384/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0388 - val_loss: 0.0039\n",
            "Epoch 385/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0390 - val_loss: 0.0139\n",
            "Epoch 386/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0399 - val_loss: 0.0234\n",
            "Epoch 387/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0378 - val_loss: 0.0026\n",
            "Epoch 388/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0375 - val_loss: 0.0124\n",
            "Epoch 389/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0358 - val_loss: 0.0013\n",
            "Epoch 390/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0367 - val_loss: 0.0160\n",
            "Epoch 391/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0361 - val_loss: 0.0172\n",
            "Epoch 392/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0346 - val_loss: 0.0025\n",
            "Epoch 393/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0360 - val_loss: 0.0078\n",
            "Epoch 394/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0347 - val_loss: 0.0190\n",
            "Epoch 395/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0338 - val_loss: 0.0015\n",
            "Epoch 396/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0344 - val_loss: 0.0087\n",
            "Epoch 397/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0344 - val_loss: 0.0156\n",
            "Epoch 398/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0328 - val_loss: 0.0012\n",
            "Epoch 399/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0328 - val_loss: 0.0052\n",
            "Epoch 400/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0321 - val_loss: 0.0136\n",
            "Epoch 401/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0319 - val_loss: 0.0021\n",
            "Epoch 402/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0315 - val_loss: 0.0035\n",
            "Epoch 403/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0308 - val_loss: 0.0062\n",
            "Epoch 404/500\n",
            "12/12 [==============================] - 0s 584us/step - loss: 0.0304 - val_loss: 0.0035\n",
            "Epoch 405/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0305 - val_loss: 0.0030\n",
            "Epoch 406/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0293 - val_loss: 0.0118\n",
            "Epoch 407/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0305 - val_loss: 0.0070\n",
            "Epoch 408/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0287 - val_loss: 6.4577e-04\n",
            "Epoch 409/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0289 - val_loss: 0.0028\n",
            "Epoch 410/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0283 - val_loss: 0.0058\n",
            "Epoch 411/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0277 - val_loss: 8.6838e-04\n",
            "Epoch 412/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0272 - val_loss: 0.0028\n",
            "Epoch 413/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0271 - val_loss: 0.0043\n",
            "Epoch 414/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0262 - val_loss: 3.8827e-04\n",
            "Epoch 415/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0262 - val_loss: 0.0061\n",
            "Epoch 416/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0262 - val_loss: 0.0024\n",
            "Epoch 417/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0255 - val_loss: 7.3436e-04\n",
            "Epoch 418/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0261 - val_loss: 0.0041\n",
            "Epoch 419/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0253 - val_loss: 0.0087\n",
            "Epoch 420/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0253 - val_loss: 6.1103e-04\n",
            "Epoch 421/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0242 - val_loss: 3.2399e-04\n",
            "Epoch 422/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0238 - val_loss: 0.0047\n",
            "Epoch 423/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0241 - val_loss: 3.0655e-04\n",
            "Epoch 424/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0243 - val_loss: 1.7274e-04\n",
            "Epoch 425/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.0229 - val_loss: 0.0053\n",
            "Epoch 426/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0236 - val_loss: 7.3015e-04\n",
            "Epoch 427/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0222 - val_loss: 2.1329e-04\n",
            "Epoch 428/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0223 - val_loss: 8.9538e-04\n",
            "Epoch 429/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0222 - val_loss: 0.0015\n",
            "Epoch 430/500\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.046 - 0s 664us/step - loss: 0.0217 - val_loss: 2.2382e-04\n",
            "Epoch 431/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0220 - val_loss: 0.0013\n",
            "Epoch 432/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0216 - val_loss: 0.0039\n",
            "Epoch 433/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0217 - val_loss: 4.1387e-04\n",
            "Epoch 434/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0218 - val_loss: 4.1408e-04\n",
            "Epoch 435/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0214 - val_loss: 0.0012\n",
            "Epoch 436/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0204 - val_loss: 8.2371e-04\n",
            "Epoch 437/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0202 - val_loss: 1.8358e-04\n",
            "Epoch 438/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0201 - val_loss: 2.6183e-04\n",
            "Epoch 439/500\n",
            "12/12 [==============================] - 0s 581us/step - loss: 0.0196 - val_loss: 9.6992e-04\n",
            "Epoch 440/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0199 - val_loss: 7.2168e-05\n",
            "Epoch 441/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0195 - val_loss: 5.0249e-04\n",
            "Epoch 442/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0190 - val_loss: 5.0797e-04\n",
            "Epoch 443/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0194 - val_loss: 7.9877e-05\n",
            "Epoch 444/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0188 - val_loss: 6.4837e-04\n",
            "Epoch 445/500\n",
            "12/12 [==============================] - 0s 746us/step - loss: 0.0188 - val_loss: 8.6560e-05\n",
            "Epoch 446/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0186 - val_loss: 7.3654e-04\n",
            "Epoch 447/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0181 - val_loss: 3.4294e-04\n",
            "Epoch 448/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0181 - val_loss: 1.5626e-04\n",
            "Epoch 449/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0173 - val_loss: 0.0017\n",
            "Epoch 450/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0185 - val_loss: 3.4585e-04\n",
            "Epoch 451/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0172 - val_loss: 6.5810e-04\n",
            "Epoch 452/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0173 - val_loss: 7.6945e-04\n",
            "Epoch 453/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0169 - val_loss: 0.0015\n",
            "Epoch 454/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0167 - val_loss: 1.5886e-05\n",
            "Epoch 455/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0161 - val_loss: 3.7390e-04\n",
            "Epoch 456/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0161 - val_loss: 6.7751e-04\n",
            "Epoch 457/500\n",
            "12/12 [==============================] - 0s 750us/step - loss: 0.0162 - val_loss: 4.0474e-06\n",
            "Epoch 458/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0156 - val_loss: 7.9085e-05\n",
            "Epoch 459/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0155 - val_loss: 4.4382e-06\n",
            "Epoch 460/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0153 - val_loss: 1.0652e-06\n",
            "Epoch 461/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0152 - val_loss: 7.2721e-04\n",
            "Epoch 462/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0148 - val_loss: 5.1569e-05\n",
            "Epoch 463/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0155 - val_loss: 4.4511e-05\n",
            "Epoch 464/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0147 - val_loss: 0.0046\n",
            "Epoch 465/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0149 - val_loss: 7.7923e-04\n",
            "Epoch 466/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0143 - val_loss: 1.5669e-05\n",
            "Epoch 467/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0148 - val_loss: 0.0024\n",
            "Epoch 468/500\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0142 - val_loss: 0.0033\n",
            "Epoch 469/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0139 - val_loss: 6.7108e-04\n",
            "Epoch 470/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0138 - val_loss: 0.0026\n",
            "Epoch 471/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0136 - val_loss: 0.0025\n",
            "Epoch 472/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0133 - val_loss: 5.6620e-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 473/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0134 - val_loss: 3.1193e-04\n",
            "Epoch 474/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0129 - val_loss: 0.0029\n",
            "Epoch 475/500\n",
            "12/12 [==============================] - 0s 746us/step - loss: 0.0129 - val_loss: 0.0018\n",
            "Epoch 476/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0129 - val_loss: 6.2249e-05\n",
            "Epoch 477/500\n",
            "12/12 [==============================] - 0s 750us/step - loss: 0.0131 - val_loss: 0.0010\n",
            "Epoch 478/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0125 - val_loss: 0.0030\n",
            "Epoch 479/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0124 - val_loss: 0.0028\n",
            "Epoch 480/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0121 - val_loss: 2.3934e-04\n",
            "Epoch 481/500\n",
            "12/12 [==============================] - 0s 664us/step - loss: 0.0123 - val_loss: 6.3279e-04\n",
            "Epoch 482/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0120 - val_loss: 0.0020\n",
            "Epoch 483/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0118 - val_loss: 0.0022\n",
            "Epoch 484/500\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.019 - 0s 582us/step - loss: 0.0117 - val_loss: 3.5348e-04\n",
            "Epoch 485/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0115 - val_loss: 0.0013\n",
            "Epoch 486/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0113 - val_loss: 0.0018\n",
            "Epoch 487/500\n",
            "12/12 [==============================] - 0s 831us/step - loss: 0.0113 - val_loss: 2.8758e-04\n",
            "Epoch 488/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0118 - val_loss: 1.0378e-04\n",
            "Epoch 489/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0109 - val_loss: 0.0037\n",
            "Epoch 490/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0116 - val_loss: 0.0022\n",
            "Epoch 491/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0107 - val_loss: 5.5178e-05\n",
            "Epoch 492/500\n",
            "12/12 [==============================] - 0s 499us/step - loss: 0.0118 - val_loss: 4.5636e-04\n",
            "Epoch 493/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0108 - val_loss: 0.0044\n",
            "Epoch 494/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0108 - val_loss: 0.0016\n",
            "Epoch 495/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0103 - val_loss: 0.0011\n",
            "Epoch 496/500\n",
            "12/12 [==============================] - 0s 582us/step - loss: 0.0103 - val_loss: 0.0024\n",
            "Epoch 497/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0102 - val_loss: 0.0024\n",
            "Epoch 498/500\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0100 - val_loss: 0.0024\n",
            "Epoch 499/500\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0101 - val_loss: 0.0019\n",
            "Epoch 500/500\n",
            "12/12 [==============================] - 0s 748us/step - loss: 0.0098 - val_loss: 0.0032\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x22c23705648>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzGX_vi0eRmq",
        "outputId": "86cf7e74-9a38-45cb-afeb-da6319b3a5d4"
      },
      "source": [
        "model.predict(np.array([[[4.],[5.],[6.]]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[15.151405]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WsaqnryeRmq"
      },
      "source": [
        "X=np.random.uniform(20,size=(1000,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1WkJ-mGeRmq",
        "outputId": "b220f6e1-9517-4658-b8c7-796072590184"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5YEeUJMeRmq"
      },
      "source": [
        "y=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9JBh8SeeRmq"
      },
      "source": [
        "for x in X:\n",
        "    y.append(x[0]+x[1]+5*x[2]+9*x[9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sjfL0yaeRmq"
      },
      "source": [
        "y=np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elAM9MsYeRmq",
        "outputId": "99374020-eb94-40b6-a1b4-040171b1bcd0"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2WJGtGieRmq",
        "outputId": "3a815e44-1c70-4369-e418-0ea83a7f8d22"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2ZvwV0LeRmq"
      },
      "source": [
        "X=X.reshape(1000,10,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7JFopZxeRmq",
        "outputId": "780811a9-4868-4179-b827-b4db062621a7"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(20, activation='relu', input_shape=(10, 1), return_sequences=True))\n",
        "model.add(LSTM(20, activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_20 (LSTM)               (None, 10, 20)            1760      \n",
            "_________________________________________________________________\n",
            "lstm_21 (LSTM)               (None, 20)                3280      \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 5,261\n",
            "Trainable params: 5,261\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWyoej_3eRmq"
      },
      "source": [
        "model.compile(optimizer='adam', loss='mse')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M_nWMZMeRmq",
        "outputId": "7539377d-a3c2-491d-dacd-98af8714e814"
      },
      "source": [
        "model.fit(X, y, epochs=500, validation_split=0.2, shuffle=True, batch_size=5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/500\n",
            "800/800 [==============================] - 1s 2ms/step - loss: 10815.1740 - val_loss: 2976.6415\n",
            "Epoch 2/500\n",
            "800/800 [==============================] - 1s 942us/step - loss: 2102.7354 - val_loss: 1175.4745\n",
            "Epoch 3/500\n",
            "800/800 [==============================] - 1s 941us/step - loss: 1258.2618 - val_loss: 1161.1582\n",
            "Epoch 4/500\n",
            "800/800 [==============================] - 1s 945us/step - loss: 1358.5360 - val_loss: 1023.6908\n",
            "Epoch 5/500\n",
            "800/800 [==============================] - 1s 933us/step - loss: 1101.9426 - val_loss: 962.2832\n",
            "Epoch 6/500\n",
            "800/800 [==============================] - 1s 945us/step - loss: 1041.4829 - val_loss: 858.5984\n",
            "Epoch 7/500\n",
            "800/800 [==============================] - 1s 946us/step - loss: 898.1065 - val_loss: 990.1506\n",
            "Epoch 8/500\n",
            "800/800 [==============================] - 1s 941us/step - loss: 789.0600 - val_loss: 746.6462\n",
            "Epoch 9/500\n",
            "800/800 [==============================] - 1s 952us/step - loss: 766.7912 - val_loss: 1150.8294\n",
            "Epoch 10/500\n",
            "800/800 [==============================] - 1s 958us/step - loss: 748.3578 - val_loss: 581.4991\n",
            "Epoch 11/500\n",
            "800/800 [==============================] - 1s 951us/step - loss: 644.6661 - val_loss: 549.8498\n",
            "Epoch 12/500\n",
            "800/800 [==============================] - 1s 945us/step - loss: 505.3544 - val_loss: 785.3663\n",
            "Epoch 13/500\n",
            "800/800 [==============================] - 1s 931us/step - loss: 505.6918 - val_loss: 523.6832\n",
            "Epoch 14/500\n",
            "800/800 [==============================] - 1s 974us/step - loss: 910.8675 - val_loss: 462.4824\n",
            "Epoch 15/500\n",
            "800/800 [==============================] - 1s 972us/step - loss: 396.0058 - val_loss: 389.3723\n",
            "Epoch 16/500\n",
            "800/800 [==============================] - 1s 946us/step - loss: 340.4382 - val_loss: 352.5215\n",
            "Epoch 17/500\n",
            "800/800 [==============================] - 1s 953us/step - loss: 283.8110 - val_loss: 388.8084\n",
            "Epoch 18/500\n",
            "800/800 [==============================] - 1s 948us/step - loss: 252.5777 - val_loss: 382.5964\n",
            "Epoch 19/500\n",
            "800/800 [==============================] - 1s 954us/step - loss: 230.8050 - val_loss: 264.7556\n",
            "Epoch 20/500\n",
            "800/800 [==============================] - 1s 952us/step - loss: 206.5638 - val_loss: 259.8304\n",
            "Epoch 21/500\n",
            "800/800 [==============================] - 1s 956us/step - loss: 171.1126 - val_loss: 173.9696\n",
            "Epoch 22/500\n",
            "800/800 [==============================] - 1s 960us/step - loss: 141.5817 - val_loss: 136.7201\n",
            "Epoch 23/500\n",
            "800/800 [==============================] - 1s 952us/step - loss: 107.8600 - val_loss: 104.3017\n",
            "Epoch 24/500\n",
            "800/800 [==============================] - 1s 955us/step - loss: 88.2370 - val_loss: 81.1204\n",
            "Epoch 25/500\n",
            "800/800 [==============================] - 1s 950us/step - loss: 73.2714 - val_loss: 61.0459\n",
            "Epoch 26/500\n",
            "800/800 [==============================] - 1s 954us/step - loss: 60.1938 - val_loss: 69.3076\n",
            "Epoch 27/500\n",
            "800/800 [==============================] - 1s 944us/step - loss: 56.5072 - val_loss: 74.9931\n",
            "Epoch 28/500\n",
            "800/800 [==============================] - 1s 949us/step - loss: 48.6408 - val_loss: 65.7354\n",
            "Epoch 29/500\n",
            "800/800 [==============================] - 1s 942us/step - loss: 37.7497 - val_loss: 35.7119\n",
            "Epoch 30/500\n",
            "800/800 [==============================] - 1s 944us/step - loss: 37.2504 - val_loss: 31.2108\n",
            "Epoch 31/500\n",
            "800/800 [==============================] - 1s 973us/step - loss: 31.5649 - val_loss: 28.6674\n",
            "Epoch 32/500\n",
            "800/800 [==============================] - 1s 951us/step - loss: 35.1880 - val_loss: 21.8162\n",
            "Epoch 33/500\n",
            "800/800 [==============================] - 1s 944us/step - loss: 22.7684 - val_loss: 27.0557\n",
            "Epoch 34/500\n",
            "800/800 [==============================] - 1s 944us/step - loss: 22.5947 - val_loss: 30.9497\n",
            "Epoch 35/500\n",
            "800/800 [==============================] - 1s 954us/step - loss: 25.0414 - val_loss: 19.7154\n",
            "Epoch 36/500\n",
            "800/800 [==============================] - 1s 965us/step - loss: 22.2013 - val_loss: 37.2163\n",
            "Epoch 37/500\n",
            "800/800 [==============================] - 1s 952us/step - loss: 18.7006 - val_loss: 20.9186\n",
            "Epoch 38/500\n",
            "800/800 [==============================] - 1s 952us/step - loss: 22.6771 - val_loss: 115.7832\n",
            "Epoch 39/500\n",
            "800/800 [==============================] - 1s 961us/step - loss: 22.8271 - val_loss: 13.2980\n",
            "Epoch 40/500\n",
            "800/800 [==============================] - 1s 950us/step - loss: 13.6511 - val_loss: 14.8445\n",
            "Epoch 41/500\n",
            "800/800 [==============================] - 1s 932us/step - loss: 15.7183 - val_loss: 15.6967\n",
            "Epoch 42/500\n",
            "800/800 [==============================] - 1s 964us/step - loss: 14.2171 - val_loss: 12.1780\n",
            "Epoch 43/500\n",
            "800/800 [==============================] - 1s 946us/step - loss: 13.2411 - val_loss: 11.0933\n",
            "Epoch 44/500\n",
            "800/800 [==============================] - 1s 968us/step - loss: 13.8636 - val_loss: 30.2483\n",
            "Epoch 45/500\n",
            "800/800 [==============================] - 1s 941us/step - loss: 20.8229 - val_loss: 25.0273\n",
            "Epoch 46/500\n",
            "800/800 [==============================] - 1s 951us/step - loss: 9.8551 - val_loss: 9.3016\n",
            "Epoch 47/500\n",
            "800/800 [==============================] - 1s 952us/step - loss: 10.6726 - val_loss: 9.6498\n",
            "Epoch 48/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 9.2794 - val_loss: 22.9535\n",
            "Epoch 49/500\n",
            "800/800 [==============================] - 1s 954us/step - loss: 17.7333 - val_loss: 8.2106\n",
            "Epoch 50/500\n",
            "800/800 [==============================] - 1s 957us/step - loss: 13.6521 - val_loss: 13.1552\n",
            "Epoch 51/500\n",
            "800/800 [==============================] - 1s 949us/step - loss: 18.2227 - val_loss: 10.2619\n",
            "Epoch 52/500\n",
            "800/800 [==============================] - 1s 970us/step - loss: 13.9345 - val_loss: 21.8639\n",
            "Epoch 53/500\n",
            "800/800 [==============================] - 1s 951us/step - loss: 12.8366 - val_loss: 12.3689\n",
            "Epoch 54/500\n",
            "800/800 [==============================] - 1s 964us/step - loss: 17.8153 - val_loss: 13.8753\n",
            "Epoch 55/500\n",
            "800/800 [==============================] - 1s 971us/step - loss: 8.2304 - val_loss: 9.4075\n",
            "Epoch 56/500\n",
            "800/800 [==============================] - 1s 953us/step - loss: 15.7222 - val_loss: 25.8035\n",
            "Epoch 57/500\n",
            "800/800 [==============================] - 1s 984us/step - loss: 8.8665 - val_loss: 12.0067\n",
            "Epoch 58/500\n",
            "800/800 [==============================] - 1s 967us/step - loss: 8.9298 - val_loss: 22.7732\n",
            "Epoch 59/500\n",
            "800/800 [==============================] - 1s 985us/step - loss: 11.6187 - val_loss: 5.9064\n",
            "Epoch 60/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 8.4585 - val_loss: 7.4890\n",
            "Epoch 61/500\n",
            "800/800 [==============================] - 1s 961us/step - loss: 8.4030 - val_loss: 11.1806\n",
            "Epoch 62/500\n",
            "800/800 [==============================] - 1s 996us/step - loss: 7.6063 - val_loss: 7.6990\n",
            "Epoch 63/500\n",
            "800/800 [==============================] - 1s 975us/step - loss: 8.8943 - val_loss: 6.2989\n",
            "Epoch 64/500\n",
            "800/800 [==============================] - 1s 990us/step - loss: 8.7563 - val_loss: 4.6444\n",
            "Epoch 65/500\n",
            "800/800 [==============================] - 1s 974us/step - loss: 8.1735 - val_loss: 14.8576\n",
            "Epoch 66/500\n",
            "800/800 [==============================] - 1s 987us/step - loss: 12.7962 - val_loss: 6.7160\n",
            "Epoch 67/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 6.3633 - val_loss: 6.3797\n",
            "Epoch 68/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 10.7313 - val_loss: 31.6255\n",
            "Epoch 69/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 7.3380 - val_loss: 4.5348\n",
            "Epoch 70/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 5.8795 - val_loss: 2.8410\n",
            "Epoch 71/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 5.6234 - val_loss: 3.9259\n",
            "Epoch 72/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.7615 - val_loss: 3.1019\n",
            "Epoch 73/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 10.5005 - val_loss: 9.7418\n",
            "Epoch 74/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 12.8579 - val_loss: 5.5617\n",
            "Epoch 75/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 5.0027 - val_loss: 4.2471\n",
            "Epoch 76/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.8685 - val_loss: 4.2676\n",
            "Epoch 77/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.4044 - val_loss: 3.9806\n",
            "Epoch 78/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "800/800 [==============================] - 1s 1ms/step - loss: 6.0239 - val_loss: 4.4130\n",
            "Epoch 79/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 6.0421 - val_loss: 4.6251\n",
            "Epoch 80/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.7090 - val_loss: 3.6584\n",
            "Epoch 81/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 8.3047 - val_loss: 13.6556\n",
            "Epoch 82/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 7.8479 - val_loss: 3.4388\n",
            "Epoch 83/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 7.0497 - val_loss: 4.4287\n",
            "Epoch 84/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.4363 - val_loss: 5.4378\n",
            "Epoch 85/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.9295 - val_loss: 2.0826\n",
            "Epoch 86/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 8.5044 - val_loss: 8.1455\n",
            "Epoch 87/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 6.3112 - val_loss: 4.8249\n",
            "Epoch 88/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.3710 - val_loss: 2.6989\n",
            "Epoch 89/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.8656 - val_loss: 7.9773\n",
            "Epoch 90/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 11.3265 - val_loss: 14.5798\n",
            "Epoch 91/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.5263 - val_loss: 5.4223\n",
            "Epoch 92/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.1827 - val_loss: 6.0284\n",
            "Epoch 93/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.3162 - val_loss: 3.9876\n",
            "Epoch 94/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.1916 - val_loss: 3.9555\n",
            "Epoch 95/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.9554 - val_loss: 7.5080\n",
            "Epoch 96/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 6.3817 - val_loss: 2.2408\n",
            "Epoch 97/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 6.3511 - val_loss: 5.2916\n",
            "Epoch 98/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 6.4642 - val_loss: 2.5878\n",
            "Epoch 99/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7240 - val_loss: 2.0566\n",
            "Epoch 100/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.0843 - val_loss: 3.1431\n",
            "Epoch 101/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.3910 - val_loss: 4.8016\n",
            "Epoch 102/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 16.3328 - val_loss: 7.5409\n",
            "Epoch 103/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.4578 - val_loss: 2.5259\n",
            "Epoch 104/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.4016 - val_loss: 1.8956\n",
            "Epoch 105/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.7459 - val_loss: 4.0874\n",
            "Epoch 106/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 5.1035 - val_loss: 2.2745\n",
            "Epoch 107/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.8803 - val_loss: 2.7109\n",
            "Epoch 108/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.3610 - val_loss: 2.8639\n",
            "Epoch 109/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.2187 - val_loss: 3.1106\n",
            "Epoch 110/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.5349 - val_loss: 4.4480\n",
            "Epoch 111/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.1318 - val_loss: 5.2329\n",
            "Epoch 112/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.3073 - val_loss: 4.7437\n",
            "Epoch 113/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 13.8905 - val_loss: 8.8197\n",
            "Epoch 114/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 14.7287 - val_loss: 4.2022\n",
            "Epoch 115/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.1131 - val_loss: 1.7933\n",
            "Epoch 116/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7272 - val_loss: 1.2196\n",
            "Epoch 117/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.2481 - val_loss: 0.9520\n",
            "Epoch 118/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5948 - val_loss: 1.4995\n",
            "Epoch 119/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.5361 - val_loss: 4.7374\n",
            "Epoch 120/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.2796 - val_loss: 1.3258\n",
            "Epoch 121/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.5316 - val_loss: 1.3416\n",
            "Epoch 122/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.0643 - val_loss: 3.4877\n",
            "Epoch 123/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.6573 - val_loss: 7.4351\n",
            "Epoch 124/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.7070 - val_loss: 7.5667\n",
            "Epoch 125/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.7847 - val_loss: 1.1384\n",
            "Epoch 126/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.9867 - val_loss: 1.0307\n",
            "Epoch 127/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7878 - val_loss: 9.2201\n",
            "Epoch 128/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.5885 - val_loss: 2.3206\n",
            "Epoch 129/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.9082 - val_loss: 7.3268\n",
            "Epoch 130/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.9801 - val_loss: 1.0844\n",
            "Epoch 131/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.3546 - val_loss: 1.4119\n",
            "Epoch 132/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.1513 - val_loss: 0.8250\n",
            "Epoch 133/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.8665 - val_loss: 2.4507\n",
            "Epoch 134/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 5.2238 - val_loss: 10.0281\n",
            "Epoch 135/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 7.9066 - val_loss: 7.8654\n",
            "Epoch 136/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.8818 - val_loss: 1.5467\n",
            "Epoch 137/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.1552 - val_loss: 6.4991\n",
            "Epoch 138/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.3857 - val_loss: 1.1929\n",
            "Epoch 139/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.6802 - val_loss: 5.8067\n",
            "Epoch 140/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.7378 - val_loss: 2.6537\n",
            "Epoch 141/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.9198 - val_loss: 12.5743\n",
            "Epoch 142/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.2719 - val_loss: 0.8286\n",
            "Epoch 143/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.0782 - val_loss: 12.3526\n",
            "Epoch 144/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 8.7763 - val_loss: 0.9789\n",
            "Epoch 145/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5160 - val_loss: 1.3187\n",
            "Epoch 146/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.1831 - val_loss: 2.0270\n",
            "Epoch 147/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.2228 - val_loss: 2.0893\n",
            "Epoch 148/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7858 - val_loss: 1.3424\n",
            "Epoch 149/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.8810 - val_loss: 0.8134\n",
            "Epoch 150/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.0942 - val_loss: 1.5108\n",
            "Epoch 151/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.3256 - val_loss: 38.8638\n",
            "Epoch 152/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 14.0233 - val_loss: 9.7146\n",
            "Epoch 153/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 8.0101 - val_loss: 7.4785\n",
            "Epoch 154/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7387 - val_loss: 1.6532\n",
            "Epoch 155/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7350 - val_loss: 1.2511\n",
            "Epoch 156/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.4072 - val_loss: 1.6006\n",
            "Epoch 157/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8275 - val_loss: 0.9497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 158/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.6849 - val_loss: 1.4039\n",
            "Epoch 159/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.1045 - val_loss: 0.8218\n",
            "Epoch 160/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9475 - val_loss: 3.9438\n",
            "Epoch 161/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9056 - val_loss: 0.5392\n",
            "Epoch 162/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.8351 - val_loss: 9.0363\n",
            "Epoch 163/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7469 - val_loss: 1.3612\n",
            "Epoch 164/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.8076 - val_loss: 0.9393\n",
            "Epoch 165/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8163 - val_loss: 1.2464\n",
            "Epoch 166/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.0389 - val_loss: 4.1131\n",
            "Epoch 167/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.9129 - val_loss: 1.3967\n",
            "Epoch 168/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.3524 - val_loss: 3.0111\n",
            "Epoch 169/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.2796 - val_loss: 4.7731\n",
            "Epoch 170/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.9547 - val_loss: 0.7695\n",
            "Epoch 171/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0899 - val_loss: 0.9041\n",
            "Epoch 172/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7995 - val_loss: 1.1336\n",
            "Epoch 173/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0813 - val_loss: 0.9005\n",
            "Epoch 174/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.7282 - val_loss: 9.8522\n",
            "Epoch 175/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.6450 - val_loss: 1.5247\n",
            "Epoch 176/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.1655 - val_loss: 1.0167\n",
            "Epoch 177/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.2571 - val_loss: 2.6519\n",
            "Epoch 178/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.6231 - val_loss: 2.3570\n",
            "Epoch 179/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0012 - val_loss: 2.9544\n",
            "Epoch 180/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 7.3886 - val_loss: 3.9615\n",
            "Epoch 181/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.0026 - val_loss: 4.2177\n",
            "Epoch 182/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 5.6354 - val_loss: 7.6020\n",
            "Epoch 183/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7274 - val_loss: 4.1736\n",
            "Epoch 184/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 6.4732 - val_loss: 13.1604\n",
            "Epoch 185/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.4970 - val_loss: 2.8252\n",
            "Epoch 186/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0971 - val_loss: 3.3772\n",
            "Epoch 187/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.1721 - val_loss: 2.3139\n",
            "Epoch 188/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.1447 - val_loss: 1.2414\n",
            "Epoch 189/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 8.7339 - val_loss: 6.6919\n",
            "Epoch 190/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.9660 - val_loss: 1.1521\n",
            "Epoch 191/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5326 - val_loss: 1.4363\n",
            "Epoch 192/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5124 - val_loss: 0.9064\n",
            "Epoch 193/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.4098 - val_loss: 2.0341\n",
            "Epoch 194/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.8797 - val_loss: 5.9947\n",
            "Epoch 195/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 7.7178 - val_loss: 1.9777\n",
            "Epoch 196/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5304 - val_loss: 1.5234\n",
            "Epoch 197/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0382 - val_loss: 0.5071\n",
            "Epoch 198/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5771 - val_loss: 0.5465\n",
            "Epoch 199/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6088 - val_loss: 1.4710\n",
            "Epoch 200/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.1054 - val_loss: 4.6130\n",
            "Epoch 201/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.0479 - val_loss: 1.2627\n",
            "Epoch 202/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.6768 - val_loss: 1.4279\n",
            "Epoch 203/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8965 - val_loss: 1.0313\n",
            "Epoch 204/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8701 - val_loss: 2.1368\n",
            "Epoch 205/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.2961 - val_loss: 6.7152\n",
            "Epoch 206/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 7.8014 - val_loss: 1.0680\n",
            "Epoch 207/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0631 - val_loss: 1.1415\n",
            "Epoch 208/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8347 - val_loss: 0.9332\n",
            "Epoch 209/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0685 - val_loss: 1.6716\n",
            "Epoch 210/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0249 - val_loss: 2.1070\n",
            "Epoch 211/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.4485 - val_loss: 1.7468\n",
            "Epoch 212/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.2127 - val_loss: 2.9080\n",
            "Epoch 213/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.2005 - val_loss: 1.8779\n",
            "Epoch 214/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7522 - val_loss: 2.6625\n",
            "Epoch 215/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7969 - val_loss: 2.0508\n",
            "Epoch 216/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.3213 - val_loss: 1.4031\n",
            "Epoch 217/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.1119 - val_loss: 1.9818\n",
            "Epoch 218/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0652 - val_loss: 2.3435\n",
            "Epoch 219/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 12.4344 - val_loss: 47.1731\n",
            "Epoch 220/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 6.6588 - val_loss: 1.1016\n",
            "Epoch 221/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8153 - val_loss: 0.6264\n",
            "Epoch 222/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6888 - val_loss: 1.4227\n",
            "Epoch 223/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7338 - val_loss: 1.1179\n",
            "Epoch 224/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9021 - val_loss: 2.3226\n",
            "Epoch 225/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4736 - val_loss: 0.4829\n",
            "Epoch 226/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3641 - val_loss: 0.5833\n",
            "Epoch 227/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8406 - val_loss: 0.7623\n",
            "Epoch 228/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6225 - val_loss: 0.4395\n",
            "Epoch 229/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.4522 - val_loss: 11.2826\n",
            "Epoch 230/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.2833 - val_loss: 1.7102\n",
            "Epoch 231/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.2859 - val_loss: 0.8376\n",
            "Epoch 232/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0373 - val_loss: 1.7700\n",
            "Epoch 233/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6472 - val_loss: 0.6427\n",
            "Epoch 234/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8868 - val_loss: 1.9732\n",
            "Epoch 235/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.2966 - val_loss: 0.6907\n",
            "Epoch 236/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9315 - val_loss: 2.6990\n",
            "Epoch 237/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5420 - val_loss: 14.3224\n",
            "Epoch 238/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.7465 - val_loss: 7.8774\n",
            "Epoch 239/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.1321 - val_loss: 0.7004\n",
            "Epoch 240/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.4042 - val_loss: 1.1435\n",
            "Epoch 241/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7300 - val_loss: 0.4388\n",
            "Epoch 242/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8294 - val_loss: 1.7395\n",
            "Epoch 243/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.2690 - val_loss: 0.3101\n",
            "Epoch 244/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7351 - val_loss: 0.8963\n",
            "Epoch 245/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.1943 - val_loss: 0.8597\n",
            "Epoch 246/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8333 - val_loss: 0.6032\n",
            "Epoch 247/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 5.1078 - val_loss: 30.2738\n",
            "Epoch 248/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 6.1361 - val_loss: 0.9927\n",
            "Epoch 249/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6860 - val_loss: 0.4656\n",
            "Epoch 250/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.4125 - val_loss: 0.5919\n",
            "Epoch 251/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6785 - val_loss: 1.1801\n",
            "Epoch 252/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9275 - val_loss: 1.4696\n",
            "Epoch 253/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7108 - val_loss: 0.9903\n",
            "Epoch 254/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.6139 - val_loss: 2.1961\n",
            "Epoch 255/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6552 - val_loss: 1.8813\n",
            "Epoch 256/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5289 - val_loss: 1.0444\n",
            "Epoch 257/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7148 - val_loss: 2.7240\n",
            "Epoch 258/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.8641 - val_loss: 2.0416\n",
            "Epoch 259/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 5.2275 - val_loss: 2.6047\n",
            "Epoch 260/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.3049 - val_loss: 0.5217\n",
            "Epoch 261/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0096 - val_loss: 0.6228\n",
            "Epoch 262/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4161 - val_loss: 0.6391\n",
            "Epoch 263/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.0403 - val_loss: 5.6585\n",
            "Epoch 264/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.0771 - val_loss: 0.5313\n",
            "Epoch 265/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7617 - val_loss: 0.4284\n",
            "Epoch 266/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8818 - val_loss: 0.8060\n",
            "Epoch 267/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.6799 - val_loss: 0.9547\n",
            "Epoch 268/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 6.7323 - val_loss: 3.8879\n",
            "Epoch 269/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.6303 - val_loss: 0.8125\n",
            "Epoch 270/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4497 - val_loss: 2.1418\n",
            "Epoch 271/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4563 - val_loss: 0.4339\n",
            "Epoch 272/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6464 - val_loss: 2.1524\n",
            "Epoch 273/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9501 - val_loss: 0.6269\n",
            "Epoch 274/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.2524 - val_loss: 0.4358\n",
            "Epoch 275/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.5256 - val_loss: 3.7343\n",
            "Epoch 276/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.4275 - val_loss: 0.8737\n",
            "Epoch 277/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6266 - val_loss: 1.4080\n",
            "Epoch 278/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.1448 - val_loss: 0.6440\n",
            "Epoch 279/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7872 - val_loss: 0.5852\n",
            "Epoch 280/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7967 - val_loss: 3.1414\n",
            "Epoch 281/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.2127 - val_loss: 2.6653\n",
            "Epoch 282/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.5874 - val_loss: 0.7267\n",
            "Epoch 283/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.3039 - val_loss: 2.7329\n",
            "Epoch 284/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7786 - val_loss: 0.3576\n",
            "Epoch 285/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.6773 - val_loss: 3.8208\n",
            "Epoch 286/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0411 - val_loss: 0.6844\n",
            "Epoch 287/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.1411 - val_loss: 1.0381\n",
            "Epoch 288/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.2193 - val_loss: 0.6120\n",
            "Epoch 289/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6064 - val_loss: 0.3514\n",
            "Epoch 290/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.6798 - val_loss: 1.5592\n",
            "Epoch 291/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9601 - val_loss: 0.9798\n",
            "Epoch 292/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5481 - val_loss: 0.6138\n",
            "Epoch 293/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.1050 - val_loss: 1.5721\n",
            "Epoch 294/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8556 - val_loss: 0.3722\n",
            "Epoch 295/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5025 - val_loss: 1.6990\n",
            "Epoch 296/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0460 - val_loss: 0.5264\n",
            "Epoch 297/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4544 - val_loss: 0.7446\n",
            "Epoch 298/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5609 - val_loss: 1.2200\n",
            "Epoch 299/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 19.7880 - val_loss: 1.2606\n",
            "Epoch 300/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6965 - val_loss: 0.6452\n",
            "Epoch 301/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4215 - val_loss: 0.4447\n",
            "Epoch 302/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3999 - val_loss: 0.7330\n",
            "Epoch 303/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3034 - val_loss: 0.4271\n",
            "Epoch 304/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5955 - val_loss: 0.3904\n",
            "Epoch 305/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3878 - val_loss: 0.2602\n",
            "Epoch 306/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2497 - val_loss: 0.2494\n",
            "Epoch 307/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4342 - val_loss: 0.9014\n",
            "Epoch 308/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4632 - val_loss: 0.2596\n",
            "Epoch 309/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5050 - val_loss: 0.3146\n",
            "Epoch 310/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4365 - val_loss: 0.5492\n",
            "Epoch 311/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5183 - val_loss: 0.7169\n",
            "Epoch 312/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6054 - val_loss: 1.0226\n",
            "Epoch 313/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.4365 - val_loss: 2.4479\n",
            "Epoch 314/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.9009 - val_loss: 0.4534\n",
            "Epoch 315/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5653 - val_loss: 1.0585\n",
            "Epoch 316/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7459 - val_loss: 0.4569\n",
            "Epoch 317/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.6134 - val_loss: 0.5566\n",
            "Epoch 318/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6864 - val_loss: 1.3837\n",
            "Epoch 319/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5286 - val_loss: 0.5983\n",
            "Epoch 320/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0397 - val_loss: 1.0191\n",
            "Epoch 321/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.4648 - val_loss: 7.7756\n",
            "Epoch 322/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.6709 - val_loss: 0.5757\n",
            "Epoch 323/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.7581 - val_loss: 1.7740\n",
            "Epoch 324/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7954 - val_loss: 1.6186\n",
            "Epoch 325/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7867 - val_loss: 0.5167\n",
            "Epoch 326/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.1924 - val_loss: 0.8346\n",
            "Epoch 327/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6166 - val_loss: 0.5160\n",
            "Epoch 328/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6734 - val_loss: 0.4961\n",
            "Epoch 329/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5356 - val_loss: 0.3758\n",
            "Epoch 330/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4960 - val_loss: 0.1999\n",
            "Epoch 331/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0315 - val_loss: 1.7282\n",
            "Epoch 332/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5063 - val_loss: 0.5025\n",
            "Epoch 333/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5883 - val_loss: 0.2473\n",
            "Epoch 334/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5108 - val_loss: 0.3760\n",
            "Epoch 335/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 6.3837 - val_loss: 4.5643\n",
            "Epoch 336/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.8391 - val_loss: 0.6263\n",
            "Epoch 337/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5785 - val_loss: 0.3012\n",
            "Epoch 338/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2710 - val_loss: 0.4082\n",
            "Epoch 339/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4605 - val_loss: 0.5798\n",
            "Epoch 340/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6475 - val_loss: 0.3750\n",
            "Epoch 341/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5196 - val_loss: 1.1985\n",
            "Epoch 342/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6041 - val_loss: 0.2470\n",
            "Epoch 343/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4265 - val_loss: 0.8139\n",
            "Epoch 344/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5231 - val_loss: 1.2132\n",
            "Epoch 345/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.1410 - val_loss: 1.4553\n",
            "Epoch 346/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7262 - val_loss: 1.1312\n",
            "Epoch 347/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5152 - val_loss: 6.3420\n",
            "Epoch 348/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.5993 - val_loss: 0.7131\n",
            "Epoch 349/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5372 - val_loss: 0.3090\n",
            "Epoch 350/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5860 - val_loss: 0.2367\n",
            "Epoch 351/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4754 - val_loss: 0.2854\n",
            "Epoch 352/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4364 - val_loss: 0.5990\n",
            "Epoch 353/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4567 - val_loss: 1.6119\n",
            "Epoch 354/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.9798 - val_loss: 13.0850\n",
            "Epoch 355/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7082 - val_loss: 0.2824\n",
            "Epoch 356/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6147 - val_loss: 0.7714\n",
            "Epoch 357/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3390 - val_loss: 0.2696\n",
            "Epoch 358/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0572 - val_loss: 1.2639\n",
            "Epoch 359/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4910 - val_loss: 0.3731\n",
            "Epoch 360/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.0872 - val_loss: 8.1146\n",
            "Epoch 361/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.7025 - val_loss: 0.2044\n",
            "Epoch 362/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3902 - val_loss: 0.5577\n",
            "Epoch 363/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4606 - val_loss: 0.6244\n",
            "Epoch 364/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2945 - val_loss: 0.1643\n",
            "Epoch 365/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.1045 - val_loss: 0.8847\n",
            "Epoch 366/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5308 - val_loss: 0.3595\n",
            "Epoch 367/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.2033 - val_loss: 1.6046\n",
            "Epoch 368/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.2506 - val_loss: 5.1058\n",
            "Epoch 369/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.4187 - val_loss: 7.4090\n",
            "Epoch 370/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.8873 - val_loss: 0.5983\n",
            "Epoch 371/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3904 - val_loss: 0.3709\n",
            "Epoch 372/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2502 - val_loss: 0.1823\n",
            "Epoch 373/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6728 - val_loss: 0.7995\n",
            "Epoch 374/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.2257 - val_loss: 0.5357\n",
            "Epoch 375/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2824 - val_loss: 0.2403\n",
            "Epoch 376/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2691 - val_loss: 0.2165\n",
            "Epoch 377/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3240 - val_loss: 1.6653\n",
            "Epoch 378/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.5960 - val_loss: 0.7983\n",
            "Epoch 379/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9537 - val_loss: 1.1152\n",
            "Epoch 380/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9417 - val_loss: 0.4060\n",
            "Epoch 381/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4636 - val_loss: 0.2731\n",
            "Epoch 382/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5007 - val_loss: 0.6908\n",
            "Epoch 383/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.0585 - val_loss: 7.7113\n",
            "Epoch 384/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5056 - val_loss: 0.6757\n",
            "Epoch 385/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7239 - val_loss: 0.8513\n",
            "Epoch 386/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4011 - val_loss: 0.6807\n",
            "Epoch 387/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.0850 - val_loss: 2.4810\n",
            "Epoch 388/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.6960 - val_loss: 0.2788\n",
            "Epoch 389/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2540 - val_loss: 0.1470\n",
            "Epoch 390/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2202 - val_loss: 0.2524\n",
            "Epoch 391/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4078 - val_loss: 0.3700\n",
            "Epoch 392/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6461 - val_loss: 0.1889\n",
            "Epoch 393/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3003 - val_loss: 1.0932\n",
            "Epoch 394/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9937 - val_loss: 0.7446\n",
            "Epoch 395/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8236 - val_loss: 0.3850\n",
            "Epoch 396/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8969 - val_loss: 0.7825\n",
            "Epoch 397/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.7965 - val_loss: 13.3233\n",
            "Epoch 398/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.4085 - val_loss: 0.3387\n",
            "Epoch 399/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3554 - val_loss: 0.2706\n",
            "Epoch 400/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3628 - val_loss: 0.6761\n",
            "Epoch 401/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2773 - val_loss: 0.1655\n",
            "Epoch 402/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6738 - val_loss: 2.8483\n",
            "Epoch 403/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5908 - val_loss: 0.3851\n",
            "Epoch 404/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4923 - val_loss: 0.8234\n",
            "Epoch 405/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4894 - val_loss: 1.0828\n",
            "Epoch 406/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8336 - val_loss: 0.7789\n",
            "Epoch 407/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5816 - val_loss: 0.3862\n",
            "Epoch 408/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2343 - val_loss: 0.1710\n",
            "Epoch 409/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0745 - val_loss: 1.8035\n",
            "Epoch 410/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9634 - val_loss: 0.5071\n",
            "Epoch 411/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9257 - val_loss: 1.4981\n",
            "Epoch 412/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.7602 - val_loss: 2.2129\n",
            "Epoch 413/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8879 - val_loss: 0.2848\n",
            "Epoch 414/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7143 - val_loss: 0.4577\n",
            "Epoch 415/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4051 - val_loss: 0.1828\n",
            "Epoch 416/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6046 - val_loss: 0.2928\n",
            "Epoch 417/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4501 - val_loss: 0.2405\n",
            "Epoch 418/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.3431 - val_loss: 1.3051\n",
            "Epoch 419/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.7476 - val_loss: 2.2737\n",
            "Epoch 420/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6512 - val_loss: 0.1891\n",
            "Epoch 421/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3557 - val_loss: 0.4222\n",
            "Epoch 422/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3496 - val_loss: 0.3355\n",
            "Epoch 423/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.4234 - val_loss: 5.0108\n",
            "Epoch 424/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.4315 - val_loss: 0.3289\n",
            "Epoch 425/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3020 - val_loss: 0.2851\n",
            "Epoch 426/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.1555 - val_loss: 0.1797\n",
            "Epoch 427/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2017 - val_loss: 0.1975\n",
            "Epoch 428/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8160 - val_loss: 0.2319\n",
            "Epoch 429/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9145 - val_loss: 2.1899\n",
            "Epoch 430/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6070 - val_loss: 0.7721\n",
            "Epoch 431/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4153 - val_loss: 0.2011\n",
            "Epoch 432/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4211 - val_loss: 0.5095\n",
            "Epoch 433/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.5460 - val_loss: 4.7772\n",
            "Epoch 434/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.8057 - val_loss: 0.3240\n",
            "Epoch 435/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4160 - val_loss: 0.4659\n",
            "Epoch 436/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6509 - val_loss: 2.0817\n",
            "Epoch 437/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6501 - val_loss: 0.2465\n",
            "Epoch 438/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3166 - val_loss: 0.6148\n",
            "Epoch 439/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2547 - val_loss: 0.6904\n",
            "Epoch 440/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6230 - val_loss: 0.3722\n",
            "Epoch 441/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0268 - val_loss: 0.9392\n",
            "Epoch 442/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8504 - val_loss: 0.7097\n",
            "Epoch 443/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.6442 - val_loss: 0.1831\n",
            "Epoch 444/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5791 - val_loss: 1.2258\n",
            "Epoch 445/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2285 - val_loss: 0.2420\n",
            "Epoch 446/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6006 - val_loss: 0.7724\n",
            "Epoch 447/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8200 - val_loss: 0.5307\n",
            "Epoch 448/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.3880 - val_loss: 0.3523\n",
            "Epoch 449/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4615 - val_loss: 0.2756\n",
            "Epoch 450/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.3311 - val_loss: 6.1844\n",
            "Epoch 451/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.8759 - val_loss: 4.3707\n",
            "Epoch 452/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0618 - val_loss: 0.5271\n",
            "Epoch 453/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5717 - val_loss: 0.9798\n",
            "Epoch 454/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2452 - val_loss: 0.3218\n",
            "Epoch 455/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4066 - val_loss: 1.0180\n",
            "Epoch 456/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5498 - val_loss: 1.1813\n",
            "Epoch 457/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6689 - val_loss: 0.8323\n",
            "Epoch 458/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8593 - val_loss: 1.4318\n",
            "Epoch 459/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.3433 - val_loss: 1.2870\n",
            "Epoch 460/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 3.6708 - val_loss: 0.2580\n",
            "Epoch 461/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4452 - val_loss: 0.9066\n",
            "Epoch 462/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5922 - val_loss: 0.2501\n",
            "Epoch 463/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2311 - val_loss: 0.1716\n",
            "Epoch 464/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2745 - val_loss: 0.2896\n",
            "Epoch 465/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9848 - val_loss: 0.9083\n",
            "Epoch 466/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.1192 - val_loss: 0.9513\n",
            "Epoch 467/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8931 - val_loss: 0.7452\n",
            "Epoch 468/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5124 - val_loss: 0.1115\n",
            "Epoch 469/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8826 - val_loss: 0.7718\n",
            "Epoch 470/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.9469 - val_loss: 1.1631\n",
            "Epoch 471/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.0741 - val_loss: 0.2940\n",
            "Epoch 472/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3286 - val_loss: 0.1653\n",
            "Epoch 473/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.1853 - val_loss: 0.3972\n",
            "Epoch 474/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6958 - val_loss: 0.8957\n",
            "Epoch 475/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7837 - val_loss: 0.3788\n",
            "Epoch 476/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3726 - val_loss: 0.1613\n",
            "Epoch 477/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 5.7300 - val_loss: 8.9538\n",
            "Epoch 478/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 4.1177 - val_loss: 0.6416\n",
            "Epoch 479/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.9832 - val_loss: 0.5914\n",
            "Epoch 480/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5196 - val_loss: 0.8666\n",
            "Epoch 481/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6631 - val_loss: 1.3690\n",
            "Epoch 482/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3901 - val_loss: 0.2155\n",
            "Epoch 483/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5248 - val_loss: 0.2534\n",
            "Epoch 484/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3829 - val_loss: 0.5501\n",
            "Epoch 485/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.2416 - val_loss: 0.1982\n",
            "Epoch 486/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.1601 - val_loss: 0.7043\n",
            "Epoch 487/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5896 - val_loss: 0.6259\n",
            "Epoch 488/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4029 - val_loss: 0.1427\n",
            "Epoch 489/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7127 - val_loss: 0.4883\n",
            "Epoch 490/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.6618 - val_loss: 0.4806\n",
            "Epoch 491/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.8330 - val_loss: 0.9345\n",
            "Epoch 492/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.7819 - val_loss: 0.2905\n",
            "Epoch 493/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.3593 - val_loss: 0.9571\n",
            "Epoch 494/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.5342 - val_loss: 0.1860\n",
            "Epoch 495/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4823 - val_loss: 0.2359\n",
            "Epoch 496/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4065 - val_loss: 1.2554\n",
            "Epoch 497/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 2.6750 - val_loss: 1.1678\n",
            "Epoch 498/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 1.3813 - val_loss: 0.2393\n",
            "Epoch 499/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.1825 - val_loss: 0.2314\n",
            "Epoch 500/500\n",
            "800/800 [==============================] - 1s 1ms/step - loss: 0.4713 - val_loss: 2.5549\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x22c25f67808>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbWWm3lkeRmr"
      },
      "source": [
        "### Many to One with Multiple features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ACjbKAKeRmr",
        "outputId": "339b52b2-e426-4ffd-b2eb-d1cbe49ce2f3"
      },
      "source": [
        "X1 = np.array([x+3 for x in range(0, 135, 3)])\n",
        "print(X1)\n",
        "\n",
        "X2 = np.array([x+5 for x in range(0, 225, 5)])\n",
        "print(X2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  3   6   9  12  15  18  21  24  27  30  33  36  39  42  45  48  51  54\n",
            "  57  60  63  66  69  72  75  78  81  84  87  90  93  96  99 102 105 108\n",
            " 111 114 117 120 123 126 129 132 135]\n",
            "[  5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90\n",
            "  95 100 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175 180\n",
            " 185 190 195 200 205 210 215 220 225]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC1dnqQneRmr",
        "outputId": "1152da25-7bc6-4aff-dd45-dd0a8f77ad26"
      },
      "source": [
        "X1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou54WnwjeRmr"
      },
      "source": [
        "X1= X1.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd8yVdnKeRmr"
      },
      "source": [
        "X2=X2.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSn1aEuEeRmr"
      },
      "source": [
        "X= np.concatenate((X1,X2), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR7Ll6WFeRmr",
        "outputId": "39793d82-13f4-4bc7-b493-12822f72a266"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RjQfkrqeRmr",
        "outputId": "58427b49-c73d-494a-d7b2-5c355207898a"
      },
      "source": [
        "X = array(X).reshape(15, 3, 2)\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[  3   5]\n",
            "  [  6  10]\n",
            "  [  9  15]]\n",
            "\n",
            " [[ 12  20]\n",
            "  [ 15  25]\n",
            "  [ 18  30]]\n",
            "\n",
            " [[ 21  35]\n",
            "  [ 24  40]\n",
            "  [ 27  45]]\n",
            "\n",
            " [[ 30  50]\n",
            "  [ 33  55]\n",
            "  [ 36  60]]\n",
            "\n",
            " [[ 39  65]\n",
            "  [ 42  70]\n",
            "  [ 45  75]]\n",
            "\n",
            " [[ 48  80]\n",
            "  [ 51  85]\n",
            "  [ 54  90]]\n",
            "\n",
            " [[ 57  95]\n",
            "  [ 60 100]\n",
            "  [ 63 105]]\n",
            "\n",
            " [[ 66 110]\n",
            "  [ 69 115]\n",
            "  [ 72 120]]\n",
            "\n",
            " [[ 75 125]\n",
            "  [ 78 130]\n",
            "  [ 81 135]]\n",
            "\n",
            " [[ 84 140]\n",
            "  [ 87 145]\n",
            "  [ 90 150]]\n",
            "\n",
            " [[ 93 155]\n",
            "  [ 96 160]\n",
            "  [ 99 165]]\n",
            "\n",
            " [[102 170]\n",
            "  [105 175]\n",
            "  [108 180]]\n",
            "\n",
            " [[111 185]\n",
            "  [114 190]\n",
            "  [117 195]]\n",
            "\n",
            " [[120 200]\n",
            "  [123 205]\n",
            "  [126 210]]\n",
            "\n",
            " [[129 215]\n",
            "  [132 220]\n",
            "  [135 225]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QgyzCd0eRmr"
      },
      "source": [
        "y=list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5olC0oizeRmr",
        "outputId": "ca88fc9d-249f-4239-ea97-249d30753303"
      },
      "source": [
        "for x in X:\n",
        "    print(x)\n",
        "    y.append( x[2,0]*3 + x[2,1]*2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3  5]\n",
            " [ 6 10]\n",
            " [ 9 15]]\n",
            "[[12 20]\n",
            " [15 25]\n",
            " [18 30]]\n",
            "[[21 35]\n",
            " [24 40]\n",
            " [27 45]]\n",
            "[[30 50]\n",
            " [33 55]\n",
            " [36 60]]\n",
            "[[39 65]\n",
            " [42 70]\n",
            " [45 75]]\n",
            "[[48 80]\n",
            " [51 85]\n",
            " [54 90]]\n",
            "[[ 57  95]\n",
            " [ 60 100]\n",
            " [ 63 105]]\n",
            "[[ 66 110]\n",
            " [ 69 115]\n",
            " [ 72 120]]\n",
            "[[ 75 125]\n",
            " [ 78 130]\n",
            " [ 81 135]]\n",
            "[[ 84 140]\n",
            " [ 87 145]\n",
            " [ 90 150]]\n",
            "[[ 93 155]\n",
            " [ 96 160]\n",
            " [ 99 165]]\n",
            "[[102 170]\n",
            " [105 175]\n",
            " [108 180]]\n",
            "[[111 185]\n",
            " [114 190]\n",
            " [117 195]]\n",
            "[[120 200]\n",
            " [123 205]\n",
            " [126 210]]\n",
            "[[129 215]\n",
            " [132 220]\n",
            " [135 225]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSI26IiseRmr"
      },
      "source": [
        "y=np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtuwLMNKeRmr",
        "outputId": "d53d026b-79c6-493c-9e90-b4f592664909"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(3, 2)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "history = model.fit(X, y, epochs=1000, validation_split=0.2, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12 samples, validate on 3 samples\n",
            "Epoch 1/1000\n",
            "12/12 [==============================] - 1s 64ms/step - loss: 166755.3594 - val_loss: 604987.8125\n",
            "Epoch 2/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 165835.2344 - val_loss: 600003.3750\n",
            "Epoch 3/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 164425.8281 - val_loss: 594288.4375\n",
            "Epoch 4/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 162914.7188 - val_loss: 587300.2500\n",
            "Epoch 5/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 161101.6406 - val_loss: 578545.1875\n",
            "Epoch 6/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 158943.2656 - val_loss: 567153.1875\n",
            "Epoch 7/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 156391.2031 - val_loss: 553553.1875\n",
            "Epoch 8/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 153467.0938 - val_loss: 539737.3125\n",
            "Epoch 9/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 150403.6719 - val_loss: 527800.6875\n",
            "Epoch 10/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 147448.8906 - val_loss: 518124.7500\n",
            "Epoch 11/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 144739.0469 - val_loss: 510084.0938\n",
            "Epoch 12/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 142237.1875 - val_loss: 502054.2500\n",
            "Epoch 13/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 139726.8594 - val_loss: 492830.8438\n",
            "Epoch 14/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 137096.8438 - val_loss: 484752.9062\n",
            "Epoch 15/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 134552.0312 - val_loss: 477125.9062\n",
            "Epoch 16/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 132133.5312 - val_loss: 468754.0312\n",
            "Epoch 17/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 129704.2891 - val_loss: 459468.0000\n",
            "Epoch 18/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 127181.0078 - val_loss: 449932.9062\n",
            "Epoch 19/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 124540.7266 - val_loss: 440466.8438\n",
            "Epoch 20/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 121754.0859 - val_loss: 430176.0000\n",
            "Epoch 21/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 118689.9453 - val_loss: 417002.0938\n",
            "Epoch 22/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 115089.7266 - val_loss: 399144.3438\n",
            "Epoch 23/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 110891.2109 - val_loss: 381827.5938\n",
            "Epoch 24/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 106684.9375 - val_loss: 368865.6562\n",
            "Epoch 25/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 103084.8984 - val_loss: 357409.8438\n",
            "Epoch 26/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 99840.0391 - val_loss: 345703.4688\n",
            "Epoch 27/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 96510.5000 - val_loss: 333503.3750\n",
            "Epoch 28/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 92853.3672 - val_loss: 318693.0000\n",
            "Epoch 29/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 88500.8672 - val_loss: 297606.8438\n",
            "Epoch 30/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 82955.3125 - val_loss: 270977.0000\n",
            "Epoch 31/1000\n",
            "12/12 [==============================] - 0s 806us/step - loss: 76295.4141 - val_loss: 240697.4219\n",
            "Epoch 32/1000\n",
            "12/12 [==============================] - 0s 795us/step - loss: 69153.6562 - val_loss: 212487.1875\n",
            "Epoch 33/1000\n",
            "12/12 [==============================] - 0s 705us/step - loss: 62285.4961 - val_loss: 190757.4531\n",
            "Epoch 34/1000\n",
            "12/12 [==============================] - 0s 704us/step - loss: 56051.1133 - val_loss: 172915.6562\n",
            "Epoch 35/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 50539.3281 - val_loss: 157985.0625\n",
            "Epoch 36/1000\n",
            "12/12 [==============================] - 0s 458us/step - loss: 45801.5938 - val_loss: 144832.3125\n",
            "Epoch 37/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 41731.4805 - val_loss: 131655.5781\n",
            "Epoch 38/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 37915.1523 - val_loss: 118011.6484\n",
            "Epoch 39/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 34144.0586 - val_loss: 103326.4609\n",
            "Epoch 40/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 30232.3438 - val_loss: 84506.8438\n",
            "Epoch 41/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 25755.0254 - val_loss: 60020.8750\n",
            "Epoch 42/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 20303.8086 - val_loss: 47461.2539\n",
            "Epoch 43/1000\n",
            "12/12 [==============================] - 0s 945us/step - loss: 15975.2607 - val_loss: 37327.1992\n",
            "Epoch 44/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 12588.5234 - val_loss: 27907.7734\n",
            "Epoch 45/1000\n",
            "12/12 [==============================] - 0s 943us/step - loss: 9535.1182 - val_loss: 18626.8730\n",
            "Epoch 46/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6700.4595 - val_loss: 9573.6533\n",
            "Epoch 47/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 4147.3164 - val_loss: 2739.2312\n",
            "Epoch 48/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2182.8091 - val_loss: 104.8481\n",
            "Epoch 49/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 1029.6704 - val_loss: 441.3801\n",
            "Epoch 50/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 601.6095 - val_loss: 1972.1395\n",
            "Epoch 51/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 613.0427 - val_loss: 4092.9714\n",
            "Epoch 52/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 877.0496 - val_loss: 6484.7798\n",
            "Epoch 53/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 1275.6083 - val_loss: 8861.8818\n",
            "Epoch 54/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1719.5161 - val_loss: 10983.2568\n",
            "Epoch 55/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2138.7727 - val_loss: 12638.9014\n",
            "Epoch 56/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2475.2822 - val_loss: 13548.4414\n",
            "Epoch 57/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2676.0615 - val_loss: 13625.2549\n",
            "Epoch 58/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 2723.0632 - val_loss: 13446.9111\n",
            "Epoch 59/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2679.5969 - val_loss: 12983.5342\n",
            "Epoch 60/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2577.9739 - val_loss: 12144.6650\n",
            "Epoch 61/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2408.7019 - val_loss: 10987.3584\n",
            "Epoch 62/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 2180.0603 - val_loss: 9598.3477\n",
            "Epoch 63/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1909.0267 - val_loss: 8097.7461\n",
            "Epoch 64/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 1616.4323 - val_loss: 6608.5820\n",
            "Epoch 65/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 1323.9869 - val_loss: 5228.0825\n",
            "Epoch 66/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1054.3785 - val_loss: 3982.6836\n",
            "Epoch 67/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 817.7878 - val_loss: 2910.7766\n",
            "Epoch 68/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 623.8810 - val_loss: 2025.3842\n",
            "Epoch 69/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 475.1487 - val_loss: 1325.9943\n",
            "Epoch 70/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 370.0019 - val_loss: 804.5933\n",
            "Epoch 71/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 304.1449 - val_loss: 438.9421\n",
            "Epoch 72/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 269.8804 - val_loss: 216.9423\n",
            "Epoch 73/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 259.0855 - val_loss: 80.0564\n",
            "Epoch 74/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 262.4590 - val_loss: 32.5399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 75/1000\n",
            "12/12 [==============================] - 0s 845us/step - loss: 268.2394 - val_loss: 30.7069\n",
            "Epoch 76/1000\n",
            "12/12 [==============================] - 0s 893us/step - loss: 268.2782 - val_loss: 46.6804\n",
            "Epoch 77/1000\n",
            "12/12 [==============================] - 0s 904us/step - loss: 263.4707 - val_loss: 105.6499\n",
            "Epoch 78/1000\n",
            "12/12 [==============================] - 0s 810us/step - loss: 258.1294 - val_loss: 140.9591\n",
            "Epoch 79/1000\n",
            "12/12 [==============================] - 0s 840us/step - loss: 252.4054 - val_loss: 187.4275\n",
            "Epoch 80/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 246.7559 - val_loss: 141.2772\n",
            "Epoch 81/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 234.4062 - val_loss: 78.7322\n",
            "Epoch 82/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 219.0118 - val_loss: 42.4857\n",
            "Epoch 83/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 204.0028 - val_loss: 34.4081\n",
            "Epoch 84/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 189.7702 - val_loss: 58.5850\n",
            "Epoch 85/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 179.4162 - val_loss: 89.5159\n",
            "Epoch 86/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 172.2245 - val_loss: 82.3363\n",
            "Epoch 87/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 162.4446 - val_loss: 47.6954\n",
            "Epoch 88/1000\n",
            "12/12 [==============================] - 0s 357us/step - loss: 150.6233 - val_loss: 15.2146\n",
            "Epoch 89/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 140.4443 - val_loss: 1.6390\n",
            "Epoch 90/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 133.0960 - val_loss: 0.0838\n",
            "Epoch 91/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 128.1287 - val_loss: 2.1458\n",
            "Epoch 92/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 124.3006 - val_loss: 9.1702\n",
            "Epoch 93/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 120.0322 - val_loss: 18.2553\n",
            "Epoch 94/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 116.7431 - val_loss: 20.3941\n",
            "Epoch 95/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 113.1820 - val_loss: 13.7074\n",
            "Epoch 96/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 109.9409 - val_loss: 0.8148\n",
            "Epoch 97/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 106.5494 - val_loss: 20.9317\n",
            "Epoch 98/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 103.2859 - val_loss: 12.2038\n",
            "Epoch 99/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 99.5813 - val_loss: 0.1948\n",
            "Epoch 100/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 95.4590 - val_loss: 0.0835\n",
            "Epoch 101/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 93.1938 - val_loss: 0.1614\n",
            "Epoch 102/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 89.5660 - val_loss: 1.3171\n",
            "Epoch 103/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 86.6282 - val_loss: 2.3528\n",
            "Epoch 104/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 84.1873 - val_loss: 2.0572\n",
            "Epoch 105/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 81.2922 - val_loss: 1.0933\n",
            "Epoch 106/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 78.7063 - val_loss: 0.5038\n",
            "Epoch 107/1000\n",
            "12/12 [==============================] - 0s 669us/step - loss: 76.6337 - val_loss: 0.3706\n",
            "Epoch 108/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 74.0603 - val_loss: 0.3546\n",
            "Epoch 109/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 71.5535 - val_loss: 0.2953\n",
            "Epoch 110/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 69.4447 - val_loss: 0.4922\n",
            "Epoch 111/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 67.3279 - val_loss: 1.7232\n",
            "Epoch 112/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 65.2720 - val_loss: 4.2221\n",
            "Epoch 113/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 63.5543 - val_loss: 6.9874\n",
            "Epoch 114/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 61.9844 - val_loss: 8.6794\n",
            "Epoch 115/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 60.3336 - val_loss: 8.9685\n",
            "Epoch 116/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 58.7320 - val_loss: 8.6223\n",
            "Epoch 117/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 57.3726 - val_loss: 9.2908\n",
            "Epoch 118/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 56.0828 - val_loss: 12.1337\n",
            "Epoch 119/1000\n",
            "12/12 [==============================] - 0s 966us/step - loss: 54.7401 - val_loss: 16.1979\n",
            "Epoch 120/1000\n",
            "12/12 [==============================] - 0s 870us/step - loss: 53.5544 - val_loss: 19.4881\n",
            "Epoch 121/1000\n",
            "12/12 [==============================] - 0s 709us/step - loss: 52.5161 - val_loss: 20.8089\n",
            "Epoch 122/1000\n",
            "12/12 [==============================] - 0s 791us/step - loss: 51.4484 - val_loss: 20.3081\n",
            "Epoch 123/1000\n",
            "12/12 [==============================] - 0s 838us/step - loss: 50.3852 - val_loss: 19.3875\n",
            "Epoch 124/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 49.4486 - val_loss: 20.0891\n",
            "Epoch 125/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 48.5662 - val_loss: 23.0859\n",
            "Epoch 126/1000\n",
            "12/12 [==============================] - 0s 894us/step - loss: 47.6598 - val_loss: 26.9228\n",
            "Epoch 127/1000\n",
            "12/12 [==============================] - 0s 684us/step - loss: 46.8119 - val_loss: 29.8667\n",
            "Epoch 128/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 46.0379 - val_loss: 31.0883\n",
            "Epoch 129/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 45.2713 - val_loss: 30.5549\n",
            "Epoch 130/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 44.4975 - val_loss: 28.8322\n",
            "Epoch 131/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 43.7520 - val_loss: 27.0343\n",
            "Epoch 132/1000\n",
            "12/12 [==============================] - 0s 840us/step - loss: 43.0517 - val_loss: 26.3843\n",
            "Epoch 133/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 42.3529 - val_loss: 27.1314\n",
            "Epoch 134/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 41.6470 - val_loss: 28.2359\n",
            "Epoch 135/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 40.9730 - val_loss: 28.3332\n",
            "Epoch 136/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 40.3218 - val_loss: 26.6242\n",
            "Epoch 137/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 39.6617 - val_loss: 23.3557\n",
            "Epoch 138/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 39.0156 - val_loss: 20.2568\n",
            "Epoch 139/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 38.3996 - val_loss: 19.3425\n",
            "Epoch 140/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 37.7744 - val_loss: 20.0990\n",
            "Epoch 141/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 37.1568 - val_loss: 19.7727\n",
            "Epoch 142/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 36.5719 - val_loss: 16.5931\n",
            "Epoch 143/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 35.9802 - val_loss: 12.3981\n",
            "Epoch 144/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 35.4226 - val_loss: 10.7198\n",
            "Epoch 145/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 34.8779 - val_loss: 11.2564\n",
            "Epoch 146/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 34.3456 - val_loss: 10.6269\n",
            "Epoch 147/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 33.7951 - val_loss: 7.7151\n",
            "Epoch 148/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 33.2367 - val_loss: 5.5464\n",
            "Epoch 149/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 32.7649 - val_loss: 11.2162\n",
            "Epoch 150/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 32.3523 - val_loss: 6.5525\n",
            "Epoch 151/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 31.7613 - val_loss: 2.4755\n",
            "Epoch 152/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 31.3928 - val_loss: 5.8610\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 153/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 30.8071 - val_loss: 8.3089\n",
            "Epoch 154/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 30.4070 - val_loss: 4.0172\n",
            "Epoch 155/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 29.8626 - val_loss: 3.1125\n",
            "Epoch 156/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 29.4405 - val_loss: 7.2458\n",
            "Epoch 157/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 28.9276 - val_loss: 9.1217\n",
            "Epoch 158/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 28.5045 - val_loss: 5.3261\n",
            "Epoch 159/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 28.0785 - val_loss: 5.6489\n",
            "Epoch 160/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 27.6689 - val_loss: 9.7181\n",
            "Epoch 161/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 27.2608 - val_loss: 8.9320\n",
            "Epoch 162/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 26.8343 - val_loss: 5.2979\n",
            "Epoch 163/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 26.4302 - val_loss: 5.6665\n",
            "Epoch 164/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 25.9982 - val_loss: 8.0473\n",
            "Epoch 165/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 25.5984 - val_loss: 6.2895\n",
            "Epoch 166/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 25.1836 - val_loss: 4.0896\n",
            "Epoch 167/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 24.8112 - val_loss: 4.7459\n",
            "Epoch 168/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 24.4050 - val_loss: 6.5620\n",
            "Epoch 169/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 24.0310 - val_loss: 5.7425\n",
            "Epoch 170/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 23.6361 - val_loss: 4.0891\n",
            "Epoch 171/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 23.2654 - val_loss: 4.3337\n",
            "Epoch 172/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 22.8822 - val_loss: 5.6893\n",
            "Epoch 173/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 22.5164 - val_loss: 5.6863\n",
            "Epoch 174/1000\n",
            "12/12 [==============================] - 0s 884us/step - loss: 22.1569 - val_loss: 4.1908\n",
            "Epoch 175/1000\n",
            "12/12 [==============================] - 0s 786us/step - loss: 21.7876 - val_loss: 3.7136\n",
            "Epoch 176/1000\n",
            "12/12 [==============================] - 0s 710us/step - loss: 21.4281 - val_loss: 4.4217\n",
            "Epoch 177/1000\n",
            "12/12 [==============================] - 0s 265us/step - loss: 21.0629 - val_loss: 4.4069\n",
            "Epoch 178/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 20.7127 - val_loss: 3.6367\n",
            "Epoch 179/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 20.3590 - val_loss: 3.4258\n",
            "Epoch 180/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 20.0131 - val_loss: 4.4059\n",
            "Epoch 181/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 19.6658 - val_loss: 4.6976\n",
            "Epoch 182/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 19.3241 - val_loss: 4.0041\n",
            "Epoch 183/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 18.9863 - val_loss: 4.1959\n",
            "Epoch 184/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 18.6516 - val_loss: 5.1641\n",
            "Epoch 185/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 18.3164 - val_loss: 5.3264\n",
            "Epoch 186/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 17.9910 - val_loss: 4.2661\n",
            "Epoch 187/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 17.6634 - val_loss: 4.1042\n",
            "Epoch 188/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 17.3433 - val_loss: 5.0528\n",
            "Epoch 189/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 17.0168 - val_loss: 5.3903\n",
            "Epoch 190/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 16.7005 - val_loss: 4.8282\n",
            "Epoch 191/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 16.3841 - val_loss: 4.4363\n",
            "Epoch 192/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 16.0720 - val_loss: 4.7182\n",
            "Epoch 193/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 15.7651 - val_loss: 4.5726\n",
            "Epoch 194/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 15.4606 - val_loss: 4.1709\n",
            "Epoch 195/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 15.1580 - val_loss: 4.1384\n",
            "Epoch 196/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 14.8556 - val_loss: 4.4350\n",
            "Epoch 197/1000\n",
            "12/12 [==============================] - 0s 935us/step - loss: 14.5572 - val_loss: 4.8344\n",
            "Epoch 198/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 14.2658 - val_loss: 4.5496\n",
            "Epoch 199/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 13.9713 - val_loss: 4.1780\n",
            "Epoch 200/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 13.6816 - val_loss: 4.0422\n",
            "Epoch 201/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 13.3968 - val_loss: 4.3696\n",
            "Epoch 202/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 13.1113 - val_loss: 4.5489\n",
            "Epoch 203/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 12.8329 - val_loss: 4.6181\n",
            "Epoch 204/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 12.5556 - val_loss: 4.4640\n",
            "Epoch 205/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 12.2781 - val_loss: 4.3992\n",
            "Epoch 206/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 12.0068 - val_loss: 4.1749\n",
            "Epoch 207/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 11.7387 - val_loss: 4.2149\n",
            "Epoch 208/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 11.4713 - val_loss: 4.4082\n",
            "Epoch 209/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 11.2053 - val_loss: 4.3536\n",
            "Epoch 210/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 10.9459 - val_loss: 4.5362\n",
            "Epoch 211/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 10.6889 - val_loss: 4.5297\n",
            "Epoch 212/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 10.4334 - val_loss: 4.3116\n",
            "Epoch 213/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 10.1788 - val_loss: 4.2812\n",
            "Epoch 214/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.9312 - val_loss: 4.0596\n",
            "Epoch 215/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.6862 - val_loss: 4.0698\n",
            "Epoch 216/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.4424 - val_loss: 4.2502\n",
            "Epoch 217/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 9.2002 - val_loss: 4.2109\n",
            "Epoch 218/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.9605 - val_loss: 4.3993\n",
            "Epoch 219/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.7271 - val_loss: 4.3794\n",
            "Epoch 220/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.4953 - val_loss: 3.8520\n",
            "Epoch 221/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.2706 - val_loss: 4.3436\n",
            "Epoch 222/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.0423 - val_loss: 4.2818\n",
            "Epoch 223/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.8207 - val_loss: 4.1208\n",
            "Epoch 224/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.6025 - val_loss: 3.8315\n",
            "Epoch 225/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.3894 - val_loss: 4.3420\n",
            "Epoch 226/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.1786 - val_loss: 4.0217\n",
            "Epoch 227/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.9681 - val_loss: 3.6531\n",
            "Epoch 228/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 6.7682 - val_loss: 4.4016\n",
            "Epoch 229/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.5652 - val_loss: 4.3888\n",
            "Epoch 230/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 6.3668 - val_loss: 3.4063\n",
            "Epoch 231/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.1795 - val_loss: 4.1761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 232/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 5.9807 - val_loss: 4.4374\n",
            "Epoch 233/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 5.7960 - val_loss: 3.6088\n",
            "Epoch 234/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 5.6153 - val_loss: 4.1369\n",
            "Epoch 235/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 5.4308 - val_loss: 4.6580\n",
            "Epoch 236/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 5.2598 - val_loss: 3.5754\n",
            "Epoch 237/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 5.0865 - val_loss: 3.9858\n",
            "Epoch 238/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 4.9138 - val_loss: 4.4416\n",
            "Epoch 239/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 4.7517 - val_loss: 3.7951\n",
            "Epoch 240/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 4.5882 - val_loss: 4.0686\n",
            "Epoch 241/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 4.4299 - val_loss: 4.2214\n",
            "Epoch 242/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 4.2763 - val_loss: 4.1435\n",
            "Epoch 243/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 4.1262 - val_loss: 3.7988\n",
            "Epoch 244/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 3.9822 - val_loss: 4.3826\n",
            "Epoch 245/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 3.8391 - val_loss: 4.0105\n",
            "Epoch 246/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 3.6997 - val_loss: 4.0182\n",
            "Epoch 247/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 3.5643 - val_loss: 4.3529\n",
            "Epoch 248/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 3.4348 - val_loss: 3.8547\n",
            "Epoch 249/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 3.3068 - val_loss: 3.9681\n",
            "Epoch 250/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 3.1821 - val_loss: 4.4076\n",
            "Epoch 251/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 3.0619 - val_loss: 4.0214\n",
            "Epoch 252/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2.9466 - val_loss: 4.2904\n",
            "Epoch 253/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 2.8335 - val_loss: 4.4504\n",
            "Epoch 254/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2.7243 - val_loss: 4.0437\n",
            "Epoch 255/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 2.6176 - val_loss: 4.2205\n",
            "Epoch 256/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2.5133 - val_loss: 4.3405\n",
            "Epoch 257/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2.4165 - val_loss: 3.7982\n",
            "Epoch 258/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2.3223 - val_loss: 4.1881\n",
            "Epoch 259/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2.2288 - val_loss: 4.4082\n",
            "Epoch 260/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 2.1402 - val_loss: 3.9781\n",
            "Epoch 261/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2.0542 - val_loss: 4.3848\n",
            "Epoch 262/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.9697 - val_loss: 4.5842\n",
            "Epoch 263/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.8905 - val_loss: 4.3678\n",
            "Epoch 264/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.8146 - val_loss: 4.5071\n",
            "Epoch 265/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 1.7408 - val_loss: 4.5283\n",
            "Epoch 266/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.6697 - val_loss: 4.2787\n",
            "Epoch 267/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 1.6012 - val_loss: 4.3809\n",
            "Epoch 268/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.5366 - val_loss: 4.1766\n",
            "Epoch 269/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.4746 - val_loss: 4.2647\n",
            "Epoch 270/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 1.4149 - val_loss: 4.3766\n",
            "Epoch 271/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 1.3575 - val_loss: 4.2869\n",
            "Epoch 272/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.3023 - val_loss: 4.4733\n",
            "Epoch 273/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.2493 - val_loss: 4.5590\n",
            "Epoch 274/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.1998 - val_loss: 4.6827\n",
            "Epoch 275/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.1525 - val_loss: 4.6028\n",
            "Epoch 276/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.1069 - val_loss: 4.6429\n",
            "Epoch 277/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 1.0631 - val_loss: 4.6349\n",
            "Epoch 278/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.0210 - val_loss: 4.5206\n",
            "Epoch 279/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.9812 - val_loss: 4.4344\n",
            "Epoch 280/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.9438 - val_loss: 4.5724\n",
            "Epoch 281/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.9081 - val_loss: 4.4502\n",
            "Epoch 282/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.8738 - val_loss: 4.5435\n",
            "Epoch 283/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.8411 - val_loss: 4.6566\n",
            "Epoch 284/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.8099 - val_loss: 4.5947\n",
            "Epoch 285/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.7810 - val_loss: 4.8614\n",
            "Epoch 286/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.7533 - val_loss: 4.6785\n",
            "Epoch 287/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.7267 - val_loss: 4.7329\n",
            "Epoch 288/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.7012 - val_loss: 4.8415\n",
            "Epoch 289/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.6771 - val_loss: 4.6410\n",
            "Epoch 290/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.6543 - val_loss: 4.6861\n",
            "Epoch 291/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.6328 - val_loss: 4.7851\n",
            "Epoch 292/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.6124 - val_loss: 4.6209\n",
            "Epoch 293/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.5929 - val_loss: 4.7946\n",
            "Epoch 294/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.5742 - val_loss: 4.7689\n",
            "Epoch 295/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.5566 - val_loss: 4.7276\n",
            "Epoch 296/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.5399 - val_loss: 5.0062\n",
            "Epoch 297/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.5242 - val_loss: 4.6682\n",
            "Epoch 298/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.5097 - val_loss: 5.0865\n",
            "Epoch 299/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.4953 - val_loss: 4.8301\n",
            "Epoch 300/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.4813 - val_loss: 4.8152\n",
            "Epoch 301/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.4683 - val_loss: 5.0268\n",
            "Epoch 302/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.4565 - val_loss: 4.6215\n",
            "Epoch 303/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.4450 - val_loss: 4.9793\n",
            "Epoch 304/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.4336 - val_loss: 4.8329\n",
            "Epoch 305/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.4227 - val_loss: 4.7499\n",
            "Epoch 306/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.4129 - val_loss: 5.0485\n",
            "Epoch 307/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.4035 - val_loss: 4.7918\n",
            "Epoch 308/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.3943 - val_loss: 5.0669\n",
            "Epoch 309/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.3854 - val_loss: 5.0118\n",
            "Epoch 310/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.3771 - val_loss: 4.9039\n",
            "Epoch 311/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.3693 - val_loss: 5.1298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 312/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.3619 - val_loss: 4.8198\n",
            "Epoch 313/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.3547 - val_loss: 5.0370\n",
            "Epoch 314/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 4.9507\n",
            "Epoch 315/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.3408 - val_loss: 4.8472\n",
            "Epoch 316/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.3346 - val_loss: 5.1008\n",
            "Epoch 317/1000\n",
            "12/12 [==============================] - 0s 982us/step - loss: 0.3289 - val_loss: 4.7539\n",
            "Epoch 318/1000\n",
            "12/12 [==============================] - 0s 716us/step - loss: 0.3232 - val_loss: 5.1120\n",
            "Epoch 319/1000\n",
            "12/12 [==============================] - 0s 795us/step - loss: 0.3174 - val_loss: 4.9534\n",
            "Epoch 320/1000\n",
            "12/12 [==============================] - 0s 796us/step - loss: 0.3118 - val_loss: 4.9161\n",
            "Epoch 321/1000\n",
            "12/12 [==============================] - 0s 347us/step - loss: 0.3068 - val_loss: 5.1546\n",
            "Epoch 322/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.3022 - val_loss: 4.8156\n",
            "Epoch 323/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2976 - val_loss: 5.1308\n",
            "Epoch 324/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.2928 - val_loss: 4.9615\n",
            "Epoch 325/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2882 - val_loss: 4.9309\n",
            "Epoch 326/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.2840 - val_loss: 5.1206\n",
            "Epoch 327/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2801 - val_loss: 4.8433\n",
            "Epoch 328/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2764 - val_loss: 5.1180\n",
            "Epoch 329/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2727 - val_loss: 4.9112\n",
            "Epoch 330/1000\n",
            "12/12 [==============================] - 0s 345us/step - loss: 0.2688 - val_loss: 4.9806\n",
            "Epoch 331/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2652 - val_loss: 5.0868\n",
            "Epoch 332/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2620 - val_loss: 4.8659\n",
            "Epoch 333/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.2588 - val_loss: 5.1170\n",
            "Epoch 334/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2556 - val_loss: 4.9064\n",
            "Epoch 335/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2524 - val_loss: 4.9965\n",
            "Epoch 336/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.2494 - val_loss: 5.0214\n",
            "Epoch 337/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.2466 - val_loss: 4.8554\n",
            "Epoch 338/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2439 - val_loss: 5.0877\n",
            "Epoch 339/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2412 - val_loss: 4.8559\n",
            "Epoch 340/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2385 - val_loss: 5.0197\n",
            "Epoch 341/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2358 - val_loss: 4.9604\n",
            "Epoch 342/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2332 - val_loss: 4.9308\n",
            "Epoch 343/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2308 - val_loss: 5.0247\n",
            "Epoch 344/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.2285 - val_loss: 4.8750\n",
            "Epoch 345/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2261 - val_loss: 5.0155\n",
            "Epoch 346/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2238 - val_loss: 4.8729\n",
            "Epoch 347/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2216 - val_loss: 4.9477\n",
            "Epoch 348/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2194 - val_loss: 4.9028\n",
            "Epoch 349/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.2172 - val_loss: 4.8708\n",
            "Epoch 350/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2151 - val_loss: 4.9091\n",
            "Epoch 351/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.2130 - val_loss: 4.8676\n",
            "Epoch 352/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2110 - val_loss: 4.8842\n",
            "Epoch 353/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2090 - val_loss: 4.8590\n",
            "Epoch 354/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2070 - val_loss: 4.8471\n",
            "Epoch 355/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2050 - val_loss: 4.8201\n",
            "Epoch 356/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.2031 - val_loss: 4.8198\n",
            "Epoch 357/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2012 - val_loss: 4.7771\n",
            "Epoch 358/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1994 - val_loss: 4.7854\n",
            "Epoch 359/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1975 - val_loss: 4.7320\n",
            "Epoch 360/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1957 - val_loss: 4.7663\n",
            "Epoch 361/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1939 - val_loss: 4.6670\n",
            "Epoch 362/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.1921 - val_loss: 4.7481\n",
            "Epoch 363/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1904 - val_loss: 4.5967\n",
            "Epoch 364/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1886 - val_loss: 4.7428\n",
            "Epoch 365/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1869 - val_loss: 4.5090\n",
            "Epoch 366/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1853 - val_loss: 4.7733\n",
            "Epoch 367/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1837 - val_loss: 4.3747\n",
            "Epoch 368/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1824 - val_loss: 4.8821\n",
            "Epoch 369/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1813 - val_loss: 4.1176\n",
            "Epoch 370/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1813 - val_loss: 5.2063\n",
            "Epoch 371/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1829 - val_loss: 3.5965\n",
            "Epoch 372/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1890 - val_loss: 5.9828\n",
            "Epoch 373/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2020 - val_loss: 2.7161\n",
            "Epoch 374/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2310 - val_loss: 7.0653\n",
            "Epoch 375/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2540 - val_loss: 2.2301\n",
            "Epoch 376/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2688 - val_loss: 5.8931\n",
            "Epoch 377/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2073 - val_loss: 4.0667\n",
            "Epoch 378/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1674 - val_loss: 3.1095\n",
            "Epoch 379/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1897 - val_loss: 5.9419\n",
            "Epoch 380/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.2063 - val_loss: 3.3212\n",
            "Epoch 381/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1811 - val_loss: 4.2023\n",
            "Epoch 382/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1609 - val_loss: 5.7099\n",
            "Epoch 383/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1820 - val_loss: 3.2821\n",
            "Epoch 384/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1880 - val_loss: 5.0531\n",
            "Epoch 385/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1602 - val_loss: 5.2324\n",
            "Epoch 386/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1618 - val_loss: 3.4497\n",
            "Epoch 387/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1775 - val_loss: 5.2918\n",
            "Epoch 388/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1615 - val_loss: 4.6457\n",
            "Epoch 389/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1512 - val_loss: 3.5850\n",
            "Epoch 390/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1622 - val_loss: 5.0985\n",
            "Epoch 391/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1575 - val_loss: 4.2084\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 392/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1464 - val_loss: 3.6722\n",
            "Epoch 393/1000\n",
            "12/12 [==============================] - 0s 669us/step - loss: 0.1511 - val_loss: 4.8995\n",
            "Epoch 394/1000\n",
            "12/12 [==============================] - 0s 663us/step - loss: 0.1518 - val_loss: 3.9369\n",
            "Epoch 395/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1428 - val_loss: 3.8332\n",
            "Epoch 396/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.1422 - val_loss: 4.7881\n",
            "Epoch 397/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1453 - val_loss: 3.7266\n",
            "Epoch 398/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1402 - val_loss: 4.0894\n",
            "Epoch 399/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 4.5574\n",
            "Epoch 400/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.1373 - val_loss: 3.5412\n",
            "Epoch 401/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1375 - val_loss: 4.2944\n",
            "Epoch 402/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1320 - val_loss: 4.0529\n",
            "Epoch 403/1000\n",
            "12/12 [==============================] - 0s 668us/step - loss: 0.1290 - val_loss: 3.4903\n",
            "Epoch 404/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1303 - val_loss: 4.2511\n",
            "Epoch 405/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1292 - val_loss: 3.4999\n",
            "Epoch 406/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.1252 - val_loss: 3.6330\n",
            "Epoch 407/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1224 - val_loss: 3.9236\n",
            "Epoch 408/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1224 - val_loss: 3.2251\n",
            "Epoch 409/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1220 - val_loss: 3.8444\n",
            "Epoch 410/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.1191 - val_loss: 3.4321\n",
            "Epoch 411/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1160 - val_loss: 3.3796\n",
            "Epoch 412/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1144 - val_loss: 3.7385\n",
            "Epoch 413/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1140 - val_loss: 3.1166\n",
            "Epoch 414/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1131 - val_loss: 3.6810\n",
            "Epoch 415/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1109 - val_loss: 3.1696\n",
            "Epoch 416/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1082 - val_loss: 3.3377\n",
            "Epoch 417/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1059 - val_loss: 3.3257\n",
            "Epoch 418/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1043 - val_loss: 3.0048\n",
            "Epoch 419/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1032 - val_loss: 3.3795\n",
            "Epoch 420/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1021 - val_loss: 2.8307\n",
            "Epoch 421/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1006 - val_loss: 3.2828\n",
            "Epoch 422/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0987 - val_loss: 2.8005\n",
            "Epoch 423/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0966 - val_loss: 3.1013\n",
            "Epoch 424/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0944 - val_loss: 2.8363\n",
            "Epoch 425/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0923 - val_loss: 2.9091\n",
            "Epoch 426/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0904 - val_loss: 2.8704\n",
            "Epoch 427/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0887 - val_loss: 2.7330\n",
            "Epoch 428/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0871 - val_loss: 2.8757\n",
            "Epoch 429/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0856 - val_loss: 2.5598\n",
            "Epoch 430/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0842 - val_loss: 2.8677\n",
            "Epoch 431/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0829 - val_loss: 2.3657\n",
            "Epoch 432/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0818 - val_loss: 2.8862\n",
            "Epoch 433/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0812 - val_loss: 2.1269\n",
            "Epoch 434/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0813 - val_loss: 2.9875\n",
            "Epoch 435/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0824 - val_loss: 1.8141\n",
            "Epoch 436/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0861 - val_loss: 3.2393\n",
            "Epoch 437/1000\n",
            "12/12 [==============================] - 0s 668us/step - loss: 0.0918 - val_loss: 1.4140\n",
            "Epoch 438/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.1047 - val_loss: 3.6361\n",
            "Epoch 439/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1164 - val_loss: 1.0524\n",
            "Epoch 440/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1370 - val_loss: 3.6736\n",
            "Epoch 441/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1290 - val_loss: 1.1523\n",
            "Epoch 442/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1116 - val_loss: 2.6052\n",
            "Epoch 443/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0749 - val_loss: 2.1402\n",
            "Epoch 444/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0623 - val_loss: 1.4659\n",
            "Epoch 445/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0771 - val_loss: 2.9877\n",
            "Epoch 446/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0870 - val_loss: 1.4620\n",
            "Epoch 447/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0777 - val_loss: 2.3629\n",
            "Epoch 448/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0583 - val_loss: 2.3882\n",
            "Epoch 449/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0576 - val_loss: 1.5512\n",
            "Epoch 450/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0692 - val_loss: 2.7037\n",
            "Epoch 451/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0673 - val_loss: 1.7434\n",
            "Epoch 452/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0551 - val_loss: 1.8775\n",
            "Epoch 453/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0502 - val_loss: 2.3411\n",
            "Epoch 454/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0564 - val_loss: 1.4657\n",
            "Epoch 455/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0575 - val_loss: 2.0389\n",
            "Epoch 456/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0488 - val_loss: 1.8903\n",
            "Epoch 457/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0459 - val_loss: 1.4476\n",
            "Epoch 458/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0501 - val_loss: 2.0516\n",
            "Epoch 459/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0488 - val_loss: 1.5719\n",
            "Epoch 460/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0429 - val_loss: 1.5768\n",
            "Epoch 461/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0415 - val_loss: 1.9281\n",
            "Epoch 462/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0439 - val_loss: 1.3858\n",
            "Epoch 463/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0430 - val_loss: 1.7245\n",
            "Epoch 464/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0387 - val_loss: 1.6293\n",
            "Epoch 465/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0371 - val_loss: 1.3378\n",
            "Epoch 466/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0384 - val_loss: 1.7021\n",
            "Epoch 467/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0381 - val_loss: 1.3039\n",
            "Epoch 468/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0353 - val_loss: 1.3939\n",
            "Epoch 469/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0331 - val_loss: 1.4658\n",
            "Epoch 470/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0332 - val_loss: 1.1525\n",
            "Epoch 471/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0336 - val_loss: 1.4326\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 472/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0323 - val_loss: 1.1877\n",
            "Epoch 473/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0302 - val_loss: 1.2089\n",
            "Epoch 474/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0290 - val_loss: 1.3135\n",
            "Epoch 475/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0290 - val_loss: 1.0619\n",
            "Epoch 476/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0290 - val_loss: 1.2986\n",
            "Epoch 477/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0281 - val_loss: 1.0698\n",
            "Epoch 478/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0266 - val_loss: 1.1443\n",
            "Epoch 479/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0254 - val_loss: 1.1357\n",
            "Epoch 480/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0248 - val_loss: 0.9925\n",
            "Epoch 481/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0247 - val_loss: 1.1483\n",
            "Epoch 482/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0244 - val_loss: 0.9192\n",
            "Epoch 483/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0239 - val_loss: 1.0800\n",
            "Epoch 484/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0230 - val_loss: 0.9161\n",
            "Epoch 485/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0220 - val_loss: 0.9768\n",
            "Epoch 486/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0212 - val_loss: 0.9423\n",
            "Epoch 487/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0206 - val_loss: 0.8853\n",
            "Epoch 488/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0202 - val_loss: 0.9623\n",
            "Epoch 489/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0200 - val_loss: 0.8177\n",
            "Epoch 490/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0198 - val_loss: 0.9635\n",
            "Epoch 491/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0195 - val_loss: 0.7624\n",
            "Epoch 492/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0194 - val_loss: 0.9536\n",
            "Epoch 493/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0192 - val_loss: 0.7044\n",
            "Epoch 494/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0192 - val_loss: 0.9501\n",
            "Epoch 495/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0194 - val_loss: 0.6324\n",
            "Epoch 496/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0200 - val_loss: 0.9788\n",
            "Epoch 497/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0211 - val_loss: 0.5358\n",
            "Epoch 498/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0236 - val_loss: 1.0756\n",
            "Epoch 499/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0276 - val_loss: 0.4012\n",
            "Epoch 500/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0360 - val_loss: 1.2952\n",
            "Epoch 501/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0476 - val_loss: 0.2406\n",
            "Epoch 502/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0713 - val_loss: 1.6281\n",
            "Epoch 503/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0914 - val_loss: 0.1456\n",
            "Epoch 504/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1209 - val_loss: 1.5861\n",
            "Epoch 505/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0984 - val_loss: 0.2552\n",
            "Epoch 506/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0589 - val_loss: 0.7766\n",
            "Epoch 507/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0181 - val_loss: 0.9399\n",
            "Epoch 508/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0254 - val_loss: 0.3109\n",
            "Epoch 509/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0551 - val_loss: 1.2833\n",
            "Epoch 510/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0469 - val_loss: 0.5873\n",
            "Epoch 511/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.6737\n",
            "Epoch 512/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 0.0162 - val_loss: 1.2664\n",
            "Epoch 513/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0345 - val_loss: 0.5067\n",
            "Epoch 514/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0350 - val_loss: 0.9852\n",
            "Epoch 515/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0159 - val_loss: 0.9970\n",
            "Epoch 516/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0167 - val_loss: 0.5269\n",
            "Epoch 517/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0286 - val_loss: 1.0332\n",
            "Epoch 518/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0207 - val_loss: 0.7831\n",
            "Epoch 519/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0122 - val_loss: 0.5686\n",
            "Epoch 520/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0197 - val_loss: 0.9920\n",
            "Epoch 521/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0207 - val_loss: 0.6820\n",
            "Epoch 522/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0127 - val_loss: 0.6364\n",
            "Epoch 523/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0142 - val_loss: 0.9688\n",
            "Epoch 524/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0184 - val_loss: 0.6473\n",
            "Epoch 525/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0137 - val_loss: 0.7202\n",
            "Epoch 526/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0114 - val_loss: 0.9348\n",
            "Epoch 527/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.6255\n",
            "Epoch 528/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0145 - val_loss: 0.7755\n",
            "Epoch 529/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0108 - val_loss: 0.8521\n",
            "Epoch 530/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0125 - val_loss: 0.5981\n",
            "Epoch 531/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0140 - val_loss: 0.7818\n",
            "Epoch 532/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0111 - val_loss: 0.7488\n",
            "Epoch 533/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0106 - val_loss: 0.5812\n",
            "Epoch 534/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0125 - val_loss: 0.7686\n",
            "Epoch 535/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0114 - val_loss: 0.6691\n",
            "Epoch 536/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0098 - val_loss: 0.5951\n",
            "Epoch 537/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0108 - val_loss: 0.7615\n",
            "Epoch 538/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0112 - val_loss: 0.6248\n",
            "Epoch 539/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0097 - val_loss: 0.6365\n",
            "Epoch 540/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0094 - val_loss: 0.7468\n",
            "Epoch 541/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0103 - val_loss: 0.6011\n",
            "Epoch 542/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0099 - val_loss: 0.6777\n",
            "Epoch 543/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0089 - val_loss: 0.7026\n",
            "Epoch 544/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0092 - val_loss: 0.5877\n",
            "Epoch 545/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0096 - val_loss: 0.6925\n",
            "Epoch 546/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0090 - val_loss: 0.6380\n",
            "Epoch 547/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0085 - val_loss: 0.5894\n",
            "Epoch 548/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0087 - val_loss: 0.6787\n",
            "Epoch 549/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0089 - val_loss: 0.5863\n",
            "Epoch 550/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0084 - val_loss: 0.6097\n",
            "Epoch 551/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 0s 666us/step - loss: 0.0081 - val_loss: 0.6487\n",
            "Epoch 552/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0083 - val_loss: 0.5668\n",
            "Epoch 553/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0083 - val_loss: 0.6364\n",
            "Epoch 554/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0080 - val_loss: 0.6099\n",
            "Epoch 555/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0077 - val_loss: 0.5771\n",
            "Epoch 556/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0078 - val_loss: 0.6403\n",
            "Epoch 557/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0079 - val_loss: 0.5720\n",
            "Epoch 558/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0077 - val_loss: 0.5984\n",
            "Epoch 559/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0074 - val_loss: 0.6070\n",
            "Epoch 560/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0074 - val_loss: 0.5543\n",
            "Epoch 561/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0075 - val_loss: 0.6057\n",
            "Epoch 562/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0074 - val_loss: 0.5612\n",
            "Epoch 563/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0072 - val_loss: 0.5640\n",
            "Epoch 564/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0071 - val_loss: 0.5863\n",
            "Epoch 565/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0071 - val_loss: 0.5396\n",
            "Epoch 566/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0071 - val_loss: 0.5808\n",
            "Epoch 567/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0070 - val_loss: 0.5531\n",
            "Epoch 568/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0068 - val_loss: 0.5515\n",
            "Epoch 569/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0068 - val_loss: 0.5728\n",
            "Epoch 570/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0068 - val_loss: 0.5348\n",
            "Epoch 571/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0068 - val_loss: 0.5676\n",
            "Epoch 572/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0067 - val_loss: 0.5414\n",
            "Epoch 573/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0066 - val_loss: 0.5436\n",
            "Epoch 574/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0065 - val_loss: 0.5534\n",
            "Epoch 575/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0065 - val_loss: 0.5259\n",
            "Epoch 576/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0065 - val_loss: 0.5528\n",
            "Epoch 577/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0064 - val_loss: 0.5260\n",
            "Epoch 578/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0063 - val_loss: 0.5394\n",
            "Epoch 579/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0063 - val_loss: 0.5364\n",
            "Epoch 580/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0062 - val_loss: 0.5259\n",
            "Epoch 581/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0062 - val_loss: 0.5439\n",
            "Epoch 582/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0062 - val_loss: 0.5209\n",
            "Epoch 583/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0061 - val_loss: 0.5416\n",
            "Epoch 584/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0061 - val_loss: 0.5242\n",
            "Epoch 585/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0060 - val_loss: 0.5322\n",
            "Epoch 586/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0060 - val_loss: 0.5302\n",
            "Epoch 587/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0059 - val_loss: 0.5221\n",
            "Epoch 588/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0059 - val_loss: 0.5339\n",
            "Epoch 589/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0059 - val_loss: 0.5164\n",
            "Epoch 590/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0059 - val_loss: 0.5330\n",
            "Epoch 591/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0058 - val_loss: 0.5163\n",
            "Epoch 592/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0058 - val_loss: 0.5289\n",
            "Epoch 593/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0057 - val_loss: 0.5198\n",
            "Epoch 594/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0057 - val_loss: 0.5239\n",
            "Epoch 595/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0057 - val_loss: 0.5238\n",
            "Epoch 596/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0056 - val_loss: 0.5194\n",
            "Epoch 597/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0056 - val_loss: 0.5264\n",
            "Epoch 598/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0056 - val_loss: 0.5153\n",
            "Epoch 599/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0056 - val_loss: 0.5273\n",
            "Epoch 600/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0055 - val_loss: 0.5124\n",
            "Epoch 601/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0055 - val_loss: 0.5270\n",
            "Epoch 602/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0055 - val_loss: 0.5101\n",
            "Epoch 603/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0055 - val_loss: 0.5266\n",
            "Epoch 604/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0054 - val_loss: 0.5082\n",
            "Epoch 605/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0054 - val_loss: 0.5274\n",
            "Epoch 606/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0054 - val_loss: 0.5059\n",
            "Epoch 607/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0054 - val_loss: 0.5298\n",
            "Epoch 608/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0053 - val_loss: 0.5020\n",
            "Epoch 609/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0053 - val_loss: 0.5346\n",
            "Epoch 610/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0053 - val_loss: 0.4952\n",
            "Epoch 611/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0053 - val_loss: 0.5432\n",
            "Epoch 612/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0054 - val_loss: 0.4827\n",
            "Epoch 613/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0054 - val_loss: 0.5599\n",
            "Epoch 614/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0056 - val_loss: 0.4600\n",
            "Epoch 615/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0058 - val_loss: 0.5939\n",
            "Epoch 616/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0063 - val_loss: 0.4182\n",
            "Epoch 617/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0073 - val_loss: 0.6656\n",
            "Epoch 618/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0092 - val_loss: 0.3424\n",
            "Epoch 619/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0130 - val_loss: 0.8215\n",
            "Epoch 620/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0199 - val_loss: 0.2219\n",
            "Epoch 621/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0349 - val_loss: 1.1562\n",
            "Epoch 622/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0575 - val_loss: 0.1063\n",
            "Epoch 623/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1048 - val_loss: 1.6247\n",
            "Epoch 624/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1367 - val_loss: 0.0935\n",
            "Epoch 625/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1721 - val_loss: 1.2904\n",
            "Epoch 626/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0945 - val_loss: 0.3294\n",
            "Epoch 627/1000\n",
            "12/12 [==============================] - 0s 668us/step - loss: 0.0179 - val_loss: 0.3400\n",
            "Epoch 628/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 0.0211 - val_loss: 1.2574\n",
            "Epoch 629/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0655 - val_loss: 0.3029\n",
            "Epoch 630/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0512 - val_loss: 0.8198\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 631/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0066 - val_loss: 1.2544\n",
            "Epoch 632/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0306 - val_loss: 0.4478\n",
            "Epoch 633/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0491 - val_loss: 1.1203\n",
            "Epoch 634/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0125 - val_loss: 1.2280\n",
            "Epoch 635/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0179 - val_loss: 0.5531\n",
            "Epoch 636/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0370 - val_loss: 1.1137\n",
            "Epoch 637/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0114 - val_loss: 1.1391\n",
            "Epoch 638/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0140 - val_loss: 0.5696\n",
            "Epoch 639/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0272 - val_loss: 0.9774\n",
            "Epoch 640/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0085 - val_loss: 1.0702\n",
            "Epoch 641/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0143 - val_loss: 0.5845\n",
            "Epoch 642/1000\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.8835\n",
            "Epoch 643/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0067 - val_loss: 1.0729\n",
            "Epoch 644/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0144 - val_loss: 0.6456\n",
            "Epoch 645/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0150 - val_loss: 0.8567\n",
            "Epoch 646/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0059 - val_loss: 1.1050\n",
            "Epoch 647/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0137 - val_loss: 0.7201\n",
            "Epoch 648/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0111 - val_loss: 0.8375\n",
            "Epoch 649/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0063 - val_loss: 1.0934\n",
            "Epoch 650/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.7556\n",
            "Epoch 651/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0085 - val_loss: 0.7879\n",
            "Epoch 652/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0068 - val_loss: 1.0239\n",
            "Epoch 653/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0110 - val_loss: 0.7529\n",
            "Epoch 654/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0068 - val_loss: 0.7304\n",
            "Epoch 655/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0071 - val_loss: 0.9479\n",
            "Epoch 656/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0094 - val_loss: 0.7545\n",
            "Epoch 657/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0060 - val_loss: 0.7050\n",
            "Epoch 658/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0073 - val_loss: 0.9095\n",
            "Epoch 659/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0081 - val_loss: 0.7831\n",
            "Epoch 660/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0056 - val_loss: 0.7178\n",
            "Epoch 661/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0071 - val_loss: 0.9034\n",
            "Epoch 662/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0070 - val_loss: 0.8216\n",
            "Epoch 663/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0055 - val_loss: 0.7406\n",
            "Epoch 664/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0068 - val_loss: 0.8956\n",
            "Epoch 665/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0063 - val_loss: 0.8390\n",
            "Epoch 666/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0054 - val_loss: 0.7458\n",
            "Epoch 667/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0064 - val_loss: 0.8671\n",
            "Epoch 668/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0058 - val_loss: 0.8273\n",
            "Epoch 669/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0054 - val_loss: 0.7335\n",
            "Epoch 670/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0061 - val_loss: 0.8294\n",
            "Epoch 671/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0055 - val_loss: 0.8068\n",
            "Epoch 672/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0053 - val_loss: 0.7223\n",
            "Epoch 673/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0058 - val_loss: 0.8033\n",
            "Epoch 674/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0053 - val_loss: 0.7944\n",
            "Epoch 675/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0052 - val_loss: 0.7218\n",
            "Epoch 676/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0056 - val_loss: 0.7918\n",
            "Epoch 677/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0052 - val_loss: 0.7883\n",
            "Epoch 678/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0051 - val_loss: 0.7240\n",
            "Epoch 679/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0054 - val_loss: 0.7821\n",
            "Epoch 680/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0051 - val_loss: 0.7765\n",
            "Epoch 681/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0050 - val_loss: 0.7178\n",
            "Epoch 682/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0052 - val_loss: 0.7648\n",
            "Epoch 683/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0050 - val_loss: 0.7545\n",
            "Epoch 684/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0049 - val_loss: 0.7039\n",
            "Epoch 685/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0051 - val_loss: 0.7440\n",
            "Epoch 686/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0049 - val_loss: 0.7312\n",
            "Epoch 687/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0049 - val_loss: 0.6915\n",
            "Epoch 688/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0050 - val_loss: 0.7290\n",
            "Epoch 689/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.7159\n",
            "Epoch 690/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0048 - val_loss: 0.6872\n",
            "Epoch 691/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0049 - val_loss: 0.7226\n",
            "Epoch 692/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0048 - val_loss: 0.7079\n",
            "Epoch 693/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0047 - val_loss: 0.6881\n",
            "Epoch 694/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0048 - val_loss: 0.7191\n",
            "Epoch 695/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0047 - val_loss: 0.7013\n",
            "Epoch 696/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.6881\n",
            "Epoch 697/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0047 - val_loss: 0.7135\n",
            "Epoch 698/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.6928\n",
            "Epoch 699/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0046 - val_loss: 0.6862\n",
            "Epoch 700/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0046 - val_loss: 0.7057\n",
            "Epoch 701/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.6844\n",
            "Epoch 702/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0046 - val_loss: 0.6847\n",
            "Epoch 703/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0046 - val_loss: 0.6990\n",
            "Epoch 704/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0046 - val_loss: 0.6796\n",
            "Epoch 705/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0046 - val_loss: 0.6858\n",
            "Epoch 706/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0045 - val_loss: 0.6946\n",
            "Epoch 707/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0045 - val_loss: 0.6778\n",
            "Epoch 708/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 0.6879\n",
            "Epoch 709/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 0.6903\n",
            "Epoch 710/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 0s 716us/step - loss: 0.0045 - val_loss: 0.6765\n",
            "Epoch 711/1000\n",
            "12/12 [==============================] - 0s 805us/step - loss: 0.0045 - val_loss: 0.6873\n",
            "Epoch 712/1000\n",
            "12/12 [==============================] - 0s 708us/step - loss: 0.0045 - val_loss: 0.6839\n",
            "Epoch 713/1000\n",
            "12/12 [==============================] - 0s 173us/step - loss: 0.0044 - val_loss: 0.6740\n",
            "Epoch 714/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0044 - val_loss: 0.6841\n",
            "Epoch 715/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0044 - val_loss: 0.6767\n",
            "Epoch 716/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0044 - val_loss: 0.6718\n",
            "Epoch 717/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0044 - val_loss: 0.6802\n",
            "Epoch 718/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0044 - val_loss: 0.6710\n",
            "Epoch 719/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0044 - val_loss: 0.6712\n",
            "Epoch 720/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0044 - val_loss: 0.6767\n",
            "Epoch 721/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0044 - val_loss: 0.6678\n",
            "Epoch 722/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 0.6719\n",
            "Epoch 723/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0043 - val_loss: 0.6731\n",
            "Epoch 724/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0043 - val_loss: 0.6663\n",
            "Epoch 725/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0043 - val_loss: 0.6718\n",
            "Epoch 726/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0043 - val_loss: 0.6693\n",
            "Epoch 727/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0043 - val_loss: 0.6651\n",
            "Epoch 728/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0043 - val_loss: 0.6703\n",
            "Epoch 729/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0043 - val_loss: 0.6651\n",
            "Epoch 730/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0043 - val_loss: 0.6646\n",
            "Epoch 731/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0042 - val_loss: 0.6676\n",
            "Epoch 732/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0042 - val_loss: 0.6622\n",
            "Epoch 733/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.6647\n",
            "Epoch 734/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0042 - val_loss: 0.6648\n",
            "Epoch 735/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.6611\n",
            "Epoch 736/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0042 - val_loss: 0.6647\n",
            "Epoch 737/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.6623\n",
            "Epoch 738/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0042 - val_loss: 0.6613\n",
            "Epoch 739/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.6635\n",
            "Epoch 740/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0042 - val_loss: 0.6604\n",
            "Epoch 741/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.6610\n",
            "Epoch 742/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0041 - val_loss: 0.6615\n",
            "Epoch 743/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0041 - val_loss: 0.6585\n",
            "Epoch 744/1000\n",
            "12/12 [==============================] - 0s 867us/step - loss: 0.0041 - val_loss: 0.6604\n",
            "Epoch 745/1000\n",
            "12/12 [==============================] - 0s 875us/step - loss: 0.0041 - val_loss: 0.6591\n",
            "Epoch 746/1000\n",
            "12/12 [==============================] - 0s 698us/step - loss: 0.0041 - val_loss: 0.6575\n",
            "Epoch 747/1000\n",
            "12/12 [==============================] - 0s 803us/step - loss: 0.0041 - val_loss: 0.6595\n",
            "Epoch 748/1000\n",
            "12/12 [==============================] - 0s 345us/step - loss: 0.0041 - val_loss: 0.6567\n",
            "Epoch 749/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.6576\n",
            "Epoch 750/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0041 - val_loss: 0.6576\n",
            "Epoch 751/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0041 - val_loss: 0.6556\n",
            "Epoch 752/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0040 - val_loss: 0.6573\n",
            "Epoch 753/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.6553\n",
            "Epoch 754/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0040 - val_loss: 0.6553\n",
            "Epoch 755/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.6557\n",
            "Epoch 756/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0040 - val_loss: 0.6537\n",
            "Epoch 757/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.6545\n",
            "Epoch 758/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0040 - val_loss: 0.6533\n",
            "Epoch 759/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0040 - val_loss: 0.6526\n",
            "Epoch 760/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.6532\n",
            "Epoch 761/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0040 - val_loss: 0.6515\n",
            "Epoch 762/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0040 - val_loss: 0.6518\n",
            "Epoch 763/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.6514\n",
            "Epoch 764/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.6506\n",
            "Epoch 765/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.6510\n",
            "Epoch 766/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.6496\n",
            "Epoch 767/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.6499\n",
            "Epoch 768/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0039 - val_loss: 0.6493\n",
            "Epoch 769/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.6485\n",
            "Epoch 770/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.6485\n",
            "Epoch 771/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.6476\n",
            "Epoch 772/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.6473\n",
            "Epoch 773/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.6470\n",
            "Epoch 774/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.6461\n",
            "Epoch 775/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.6464\n",
            "Epoch 776/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.6452\n",
            "Epoch 777/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.6453\n",
            "Epoch 778/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.6446\n",
            "Epoch 779/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.6441\n",
            "Epoch 780/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 0.0038 - val_loss: 0.6438\n",
            "Epoch 781/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.6432\n",
            "Epoch 782/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0038 - val_loss: 0.6428\n",
            "Epoch 783/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.6423\n",
            "Epoch 784/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.6418\n",
            "Epoch 785/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.6414\n",
            "Epoch 786/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.6407\n",
            "Epoch 787/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0038 - val_loss: 0.6406\n",
            "Epoch 788/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.6395\n",
            "Epoch 789/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0038 - val_loss: 0.6397\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 790/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.6385\n",
            "Epoch 791/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.6388\n",
            "Epoch 792/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.6375\n",
            "Epoch 793/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0037 - val_loss: 0.6378\n",
            "Epoch 794/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.6369\n",
            "Epoch 795/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.6363\n",
            "Epoch 796/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.6365\n",
            "Epoch 797/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0037 - val_loss: 0.6346\n",
            "Epoch 798/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.6358\n",
            "Epoch 799/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.6334\n",
            "Epoch 800/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0037 - val_loss: 0.6346\n",
            "Epoch 801/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.6330\n",
            "Epoch 802/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.6330\n",
            "Epoch 803/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0037 - val_loss: 0.6325\n",
            "Epoch 804/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.6316\n",
            "Epoch 805/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0036 - val_loss: 0.6318\n",
            "Epoch 806/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.6305\n",
            "Epoch 807/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 0.6307\n",
            "Epoch 808/1000\n",
            "12/12 [==============================] - 0s 805us/step - loss: 0.0036 - val_loss: 0.6297\n",
            "Epoch 809/1000\n",
            "12/12 [==============================] - 0s 794us/step - loss: 0.0036 - val_loss: 0.6294\n",
            "Epoch 810/1000\n",
            "12/12 [==============================] - 0s 695us/step - loss: 0.0036 - val_loss: 0.6288\n",
            "Epoch 811/1000\n",
            "12/12 [==============================] - 0s 173us/step - loss: 0.0036 - val_loss: 0.6282\n",
            "Epoch 812/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.6278\n",
            "Epoch 813/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.6272\n",
            "Epoch 814/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.6268\n",
            "Epoch 815/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 0.6260\n",
            "Epoch 816/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.6258\n",
            "Epoch 817/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 0.6251\n",
            "Epoch 818/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.6246\n",
            "Epoch 819/1000\n",
            "12/12 [==============================] - 0s 944us/step - loss: 0.0036 - val_loss: 0.6242\n",
            "Epoch 820/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0036 - val_loss: 0.6235\n",
            "Epoch 821/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6233\n",
            "Epoch 822/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6223\n",
            "Epoch 823/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6221\n",
            "Epoch 824/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6213\n",
            "Epoch 825/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6209\n",
            "Epoch 826/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6206\n",
            "Epoch 827/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6193\n",
            "Epoch 828/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6201\n",
            "Epoch 829/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6177\n",
            "Epoch 830/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.6193\n",
            "Epoch 831/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0035 - val_loss: 0.6161\n",
            "Epoch 832/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.6190\n",
            "Epoch 833/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6143\n",
            "Epoch 834/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6185\n",
            "Epoch 835/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6124\n",
            "Epoch 836/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.6186\n",
            "Epoch 837/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6098\n",
            "Epoch 838/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6196\n",
            "Epoch 839/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.6060\n",
            "Epoch 840/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6225\n",
            "Epoch 841/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0035 - val_loss: 0.5993\n",
            "Epoch 842/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6295\n",
            "Epoch 843/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.5872\n",
            "Epoch 844/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.6444\n",
            "Epoch 845/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.5650\n",
            "Epoch 846/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0038 - val_loss: 0.6761\n",
            "Epoch 847/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.5217\n",
            "Epoch 848/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0051 - val_loss: 0.7466\n",
            "Epoch 849/1000\n",
            "12/12 [==============================] - 0s 668us/step - loss: 0.0068 - val_loss: 0.4380\n",
            "Epoch 850/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0105 - val_loss: 0.9114\n",
            "Epoch 851/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0178 - val_loss: 0.2929\n",
            "Epoch 852/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0345 - val_loss: 1.2981\n",
            "Epoch 853/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0628 - val_loss: 0.1355\n",
            "Epoch 854/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.1259 - val_loss: 1.9505\n",
            "Epoch 855/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1806 - val_loss: 0.1162\n",
            "Epoch 856/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2491 - val_loss: 1.6846\n",
            "Epoch 857/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1424 - val_loss: 0.4323\n",
            "Epoch 858/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0220 - val_loss: 0.4480\n",
            "Epoch 859/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0301 - val_loss: 1.7226\n",
            "Epoch 860/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0950 - val_loss: 0.4993\n",
            "Epoch 861/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0546 - val_loss: 1.0110\n",
            "Epoch 862/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0059 - val_loss: 1.9172\n",
            "Epoch 863/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0580 - val_loss: 0.7413\n",
            "Epoch 864/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0522 - val_loss: 1.3131\n",
            "Epoch 865/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0073 - val_loss: 1.9500\n",
            "Epoch 866/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0440 - val_loss: 0.8410\n",
            "Epoch 867/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0389 - val_loss: 1.2149\n",
            "Epoch 868/1000\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 1.7223\n",
            "Epoch 869/1000\n",
            "12/12 [==============================] - 0s 871us/step - loss: 0.0356 - val_loss: 0.7969\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 870/1000\n",
            "12/12 [==============================] - 0s 786us/step - loss: 0.0251 - val_loss: 0.9850\n",
            "Epoch 871/1000\n",
            "12/12 [==============================] - 0s 792us/step - loss: 0.0069 - val_loss: 1.4844\n",
            "Epoch 872/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0307 - val_loss: 0.7860\n",
            "Epoch 873/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0156 - val_loss: 0.8644\n",
            "Epoch 874/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0091 - val_loss: 1.3894\n",
            "Epoch 875/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0247 - val_loss: 0.8740\n",
            "Epoch 876/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0088 - val_loss: 0.8608\n",
            "Epoch 877/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0105 - val_loss: 1.3751\n",
            "Epoch 878/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0186 - val_loss: 0.9926\n",
            "Epoch 879/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.8763\n",
            "Epoch 880/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 1.3171\n",
            "Epoch 881/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0137 - val_loss: 1.0439\n",
            "Epoch 882/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0051 - val_loss: 0.8458\n",
            "Epoch 883/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0114 - val_loss: 1.1892\n",
            "Epoch 884/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0096 - val_loss: 1.0257\n",
            "Epoch 885/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0050 - val_loss: 0.7992\n",
            "Epoch 886/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0103 - val_loss: 1.0687\n",
            "Epoch 887/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0070 - val_loss: 1.0101\n",
            "Epoch 888/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0055 - val_loss: 0.7934\n",
            "Epoch 889/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0090 - val_loss: 1.0152\n",
            "Epoch 890/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0055 - val_loss: 1.0371\n",
            "Epoch 891/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0057 - val_loss: 0.8361\n",
            "Epoch 892/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0075 - val_loss: 1.0123\n",
            "Epoch 893/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 1.0769\n",
            "Epoch 894/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0057 - val_loss: 0.8832\n",
            "Epoch 895/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0064 - val_loss: 1.0036\n",
            "Epoch 896/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0044 - val_loss: 1.0775\n",
            "Epoch 897/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0056 - val_loss: 0.8919\n",
            "Epoch 898/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0056 - val_loss: 0.9602\n",
            "Epoch 899/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 1.0318\n",
            "Epoch 900/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0054 - val_loss: 0.8673\n",
            "Epoch 901/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0050 - val_loss: 0.9045\n",
            "Epoch 902/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0042 - val_loss: 0.9771\n",
            "Epoch 903/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0051 - val_loss: 0.8430\n",
            "Epoch 904/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0046 - val_loss: 0.8676\n",
            "Epoch 905/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0041 - val_loss: 0.9424\n",
            "Epoch 906/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0048 - val_loss: 0.8354\n",
            "Epoch 907/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0043 - val_loss: 0.8529\n",
            "Epoch 908/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.9226\n",
            "Epoch 909/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0045 - val_loss: 0.8322\n",
            "Epoch 910/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.8409\n",
            "Epoch 911/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0040 - val_loss: 0.8984\n",
            "Epoch 912/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 0.8175\n",
            "Epoch 913/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0041 - val_loss: 0.8195\n",
            "Epoch 914/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.8648\n",
            "Epoch 915/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0042 - val_loss: 0.7944\n",
            "Epoch 916/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0040 - val_loss: 0.7961\n",
            "Epoch 917/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.8347\n",
            "Epoch 918/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0041 - val_loss: 0.7772\n",
            "Epoch 919/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.7838\n",
            "Epoch 920/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.8192\n",
            "Epoch 921/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0040 - val_loss: 0.7732\n",
            "Epoch 922/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.7844\n",
            "Epoch 923/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0038 - val_loss: 0.8151\n",
            "Epoch 924/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 0.7758\n",
            "Epoch 925/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.7882\n",
            "Epoch 926/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.8112\n",
            "Epoch 927/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0038 - val_loss: 0.7749\n",
            "Epoch 928/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.7861\n",
            "Epoch 929/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0037 - val_loss: 0.8008\n",
            "Epoch 930/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 0.7676\n",
            "Epoch 931/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0037 - val_loss: 0.7788\n",
            "Epoch 932/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.7880\n",
            "Epoch 933/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0037 - val_loss: 0.7598\n",
            "Epoch 934/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0037 - val_loss: 0.7726\n",
            "Epoch 935/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.7781\n",
            "Epoch 936/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0037 - val_loss: 0.7554\n",
            "Epoch 937/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.7697\n",
            "Epoch 938/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0037 - val_loss: 0.7718\n",
            "Epoch 939/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 0.7533\n",
            "Epoch 940/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0037 - val_loss: 0.7674\n",
            "Epoch 941/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.7652\n",
            "Epoch 942/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.7500\n",
            "Epoch 943/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.7627\n",
            "Epoch 944/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.7568\n",
            "Epoch 945/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0036 - val_loss: 0.7454\n",
            "Epoch 946/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.7565\n",
            "Epoch 947/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 0.7486\n",
            "Epoch 948/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0036 - val_loss: 0.7415\n",
            "Epoch 949/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.7514\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 950/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.7435\n",
            "Epoch 951/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.7404\n",
            "Epoch 952/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.7489\n",
            "Epoch 953/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 0.7410\n",
            "Epoch 954/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.7409\n",
            "Epoch 955/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 0.7471\n",
            "Epoch 956/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 0.7394\n",
            "Epoch 957/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.7408\n",
            "Epoch 958/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.7445\n",
            "Epoch 959/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.7371\n",
            "Epoch 960/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.7395\n",
            "Epoch 961/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.7409\n",
            "Epoch 962/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.7348\n",
            "Epoch 963/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.7381\n",
            "Epoch 964/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0035 - val_loss: 0.7379\n",
            "Epoch 965/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.7334\n",
            "Epoch 966/1000\n",
            "12/12 [==============================] - 0s 668us/step - loss: 0.0035 - val_loss: 0.7371\n",
            "Epoch 967/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 0.0035 - val_loss: 0.7356\n",
            "Epoch 968/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.7327\n",
            "Epoch 969/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.7365\n",
            "Epoch 970/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.7336\n",
            "Epoch 971/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.7325\n",
            "Epoch 972/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.7352\n",
            "Epoch 973/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.7313\n",
            "Epoch 974/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.7316\n",
            "Epoch 975/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.7327\n",
            "Epoch 976/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 0.7294\n",
            "Epoch 977/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7301\n",
            "Epoch 978/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7304\n",
            "Epoch 979/1000\n",
            "12/12 [==============================] - 0s 668us/step - loss: 0.0034 - val_loss: 0.7277\n",
            "Epoch 980/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7291\n",
            "Epoch 981/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7289\n",
            "Epoch 982/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7268\n",
            "Epoch 983/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7287\n",
            "Epoch 984/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0034 - val_loss: 0.7277\n",
            "Epoch 985/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7262\n",
            "Epoch 986/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7279\n",
            "Epoch 987/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7261\n",
            "Epoch 988/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7259\n",
            "Epoch 989/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7268\n",
            "Epoch 990/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7249\n",
            "Epoch 991/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0034 - val_loss: 0.7254\n",
            "Epoch 992/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7255\n",
            "Epoch 993/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0034 - val_loss: 0.7239\n",
            "Epoch 994/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7247\n",
            "Epoch 995/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7244\n",
            "Epoch 996/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 0.7232\n",
            "Epoch 997/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0033 - val_loss: 0.7243\n",
            "Epoch 998/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0033 - val_loss: 0.7232\n",
            "Epoch 999/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0033 - val_loss: 0.7228\n",
            "Epoch 1000/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0033 - val_loss: 0.7236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TzikSpReRmr",
        "outputId": "904a7c56-946f-46f8-bb0f-1c347c6850d2"
      },
      "source": [
        "model.predict(np.array( [[[120, 200],\n",
        "  [123, 205],\n",
        "  [100, 210]]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[788.6328]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCtVrHjFeRmr"
      },
      "source": [
        "## Bidirectional LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zXctRMWeRmr",
        "outputId": "80d0f25c-ee74-4e1f-a130-21cab36c3372"
      },
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(3, 2)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "history = model.fit(X, Y, epochs=1000, validation_split=0.2, verbose=1)\n",
        "test_output = model.predict(test_input, verbose=0)\n",
        "print(test_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12 samples, validate on 3 samples\n",
            "Epoch 1/1000\n",
            "12/12 [==============================] - 2s 144ms/step - loss: 3505.6660 - val_loss: 12908.0225\n",
            "Epoch 2/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 3405.7168 - val_loss: 12481.5732\n",
            "Epoch 3/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 3291.6238 - val_loss: 12029.4141\n",
            "Epoch 4/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 3171.7791 - val_loss: 11534.6875\n",
            "Epoch 5/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 3040.9434 - val_loss: 11066.3662\n",
            "Epoch 6/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2916.4358 - val_loss: 10595.4883\n",
            "Epoch 7/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 2790.6465 - val_loss: 10103.8291\n",
            "Epoch 8/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 2660.8083 - val_loss: 9601.9902\n",
            "Epoch 9/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2533.3135 - val_loss: 9078.0391\n",
            "Epoch 10/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2406.4055 - val_loss: 8552.8945\n",
            "Epoch 11/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 2281.8782 - val_loss: 8067.8672\n",
            "Epoch 12/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2162.4617 - val_loss: 7638.3071\n",
            "Epoch 13/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 2048.8647 - val_loss: 7246.4858\n",
            "Epoch 14/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1939.6934 - val_loss: 6862.3125\n",
            "Epoch 15/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1832.3436 - val_loss: 6456.6719\n",
            "Epoch 16/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 1724.3154 - val_loss: 6014.4009\n",
            "Epoch 17/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1614.6353 - val_loss: 5549.5640\n",
            "Epoch 18/1000\n",
            "12/12 [==============================] - 0s 772us/step - loss: 1505.3793 - val_loss: 5108.4141\n",
            "Epoch 19/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1398.3578 - val_loss: 4743.3311\n",
            "Epoch 20/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1300.8020 - val_loss: 4440.6567\n",
            "Epoch 21/1000\n",
            "12/12 [==============================] - 0s 695us/step - loss: 1210.6219 - val_loss: 4171.5640\n",
            "Epoch 22/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1124.6908 - val_loss: 3907.6536\n",
            "Epoch 23/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 1038.3185 - val_loss: 3631.9746\n",
            "Epoch 24/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 948.3351 - val_loss: 3322.3879\n",
            "Epoch 25/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 849.3179 - val_loss: 2963.3711\n",
            "Epoch 26/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 739.2532 - val_loss: 2531.3955\n",
            "Epoch 27/1000\n",
            "12/12 [==============================] - 0s 905us/step - loss: 615.5710 - val_loss: 2009.3706\n",
            "Epoch 28/1000\n",
            "12/12 [==============================] - 0s 706us/step - loss: 477.5194 - val_loss: 1414.7993\n",
            "Epoch 29/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 329.0664 - val_loss: 816.5962\n",
            "Epoch 30/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 182.6320 - val_loss: 313.7761\n",
            "Epoch 31/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 62.6439 - val_loss: 24.7347\n",
            "Epoch 32/1000\n",
            "12/12 [==============================] - 0s 801us/step - loss: 5.6337 - val_loss: 54.0741\n",
            "Epoch 33/1000\n",
            "12/12 [==============================] - 0s 747us/step - loss: 32.7894 - val_loss: 218.9489\n",
            "Epoch 34/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 93.5581 - val_loss: 326.0163\n",
            "Epoch 35/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 132.3447 - val_loss: 344.9662\n",
            "Epoch 36/1000\n",
            "12/12 [==============================] - 0s 386us/step - loss: 138.6304 - val_loss: 295.8178\n",
            "Epoch 37/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 120.6320 - val_loss: 211.5798\n",
            "Epoch 38/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 90.7789 - val_loss: 124.6250\n",
            "Epoch 39/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 59.7962 - val_loss: 57.9367\n",
            "Epoch 40/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 34.2943 - val_loss: 18.7208\n",
            "Epoch 41/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 16.6155 - val_loss: 1.9564\n",
            "Epoch 42/1000\n",
            "12/12 [==============================] - 0s 624us/step - loss: 6.2708 - val_loss: 1.9635\n",
            "Epoch 43/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.8862 - val_loss: 16.9725\n",
            "Epoch 44/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 2.0121 - val_loss: 41.5486\n",
            "Epoch 45/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 5.0628 - val_loss: 64.4350\n",
            "Epoch 46/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 8.9154 - val_loss: 78.2949\n",
            "Epoch 47/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 11.8819 - val_loss: 81.1894\n",
            "Epoch 48/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 13.1899 - val_loss: 73.4452\n",
            "Epoch 49/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 12.7498 - val_loss: 57.8106\n",
            "Epoch 50/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 10.9705 - val_loss: 40.5127\n",
            "Epoch 51/1000\n",
            "12/12 [==============================] - 0s 744us/step - loss: 8.5713 - val_loss: 28.3054\n",
            "Epoch 52/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.3234 - val_loss: 21.7100\n",
            "Epoch 53/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 4.5628 - val_loss: 17.9864\n",
            "Epoch 54/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 3.2540 - val_loss: 15.1648\n",
            "Epoch 55/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 2.2669 - val_loss: 12.4774\n",
            "Epoch 56/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.5110 - val_loss: 9.8235\n",
            "Epoch 57/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.9477 - val_loss: 7.2794\n",
            "Epoch 58/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.5497 - val_loss: 5.0493\n",
            "Epoch 59/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.3114 - val_loss: 3.2299\n",
            "Epoch 60/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.2127 - val_loss: 1.8614\n",
            "Epoch 61/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2303 - val_loss: 0.9431\n",
            "Epoch 62/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.3350 - val_loss: 0.4072\n",
            "Epoch 63/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.4928 - val_loss: 0.1583\n",
            "Epoch 64/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.6682 - val_loss: 0.0892\n",
            "Epoch 65/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.8342 - val_loss: 0.1095\n",
            "Epoch 66/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.9661 - val_loss: 0.1494\n",
            "Epoch 67/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.0487 - val_loss: 0.1681\n",
            "Epoch 68/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.0754 - val_loss: 0.1540\n",
            "Epoch 69/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 1.0477 - val_loss: 0.1201\n",
            "Epoch 70/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.9736 - val_loss: 0.0956\n",
            "Epoch 71/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.8666 - val_loss: 0.1166\n",
            "Epoch 72/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.7405 - val_loss: 0.2187\n",
            "Epoch 73/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.6091 - val_loss: 0.4265\n",
            "Epoch 74/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.4866 - val_loss: 0.7558\n",
            "Epoch 75/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.3812 - val_loss: 1.2028\n",
            "Epoch 76/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.2997 - val_loss: 1.7497\n",
            "Epoch 77/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2443 - val_loss: 2.3662\n",
            "Epoch 78/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2138 - val_loss: 3.0096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 79/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2048 - val_loss: 3.6454\n",
            "Epoch 80/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.2117 - val_loss: 4.2288\n",
            "Epoch 81/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2287 - val_loss: 4.7207\n",
            "Epoch 82/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2497 - val_loss: 5.0990\n",
            "Epoch 83/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2699 - val_loss: 5.3429\n",
            "Epoch 84/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.2850 - val_loss: 5.4451\n",
            "Epoch 85/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2926 - val_loss: 5.4086\n",
            "Epoch 86/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2916 - val_loss: 5.2454\n",
            "Epoch 87/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2824 - val_loss: 4.9747\n",
            "Epoch 88/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2661 - val_loss: 4.6202\n",
            "Epoch 89/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2449 - val_loss: 4.2079\n",
            "Epoch 90/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.2209 - val_loss: 3.7639\n",
            "Epoch 91/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1966 - val_loss: 3.3122\n",
            "Epoch 92/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.1738 - val_loss: 2.8731\n",
            "Epoch 93/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1542 - val_loss: 2.4626\n",
            "Epoch 94/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1386 - val_loss: 2.0919\n",
            "Epoch 95/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1274 - val_loss: 1.7673\n",
            "Epoch 96/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1202 - val_loss: 1.4914\n",
            "Epoch 97/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1165 - val_loss: 1.2643\n",
            "Epoch 98/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.1154 - val_loss: 1.0808\n",
            "Epoch 99/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1157 - val_loss: 0.9376\n",
            "Epoch 100/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1165 - val_loss: 0.8298\n",
            "Epoch 101/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1170 - val_loss: 0.7523\n",
            "Epoch 102/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.1166 - val_loss: 0.7010\n",
            "Epoch 103/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.1148 - val_loss: 0.6714\n",
            "Epoch 104/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1118 - val_loss: 0.6607\n",
            "Epoch 105/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.1074 - val_loss: 0.6659\n",
            "Epoch 106/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.1021 - val_loss: 0.6848\n",
            "Epoch 107/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0963 - val_loss: 0.7151\n",
            "Epoch 108/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0902 - val_loss: 0.7544\n",
            "Epoch 109/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0844 - val_loss: 0.8002\n",
            "Epoch 110/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0792 - val_loss: 0.8500\n",
            "Epoch 111/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0746 - val_loss: 0.9010\n",
            "Epoch 112/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0709 - val_loss: 0.9501\n",
            "Epoch 113/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0679 - val_loss: 0.9952\n",
            "Epoch 114/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0657 - val_loss: 1.0334\n",
            "Epoch 115/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0640 - val_loss: 1.0629\n",
            "Epoch 116/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0626 - val_loss: 1.0821\n",
            "Epoch 117/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0614 - val_loss: 1.0902\n",
            "Epoch 118/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0602 - val_loss: 1.0869\n",
            "Epoch 119/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0590 - val_loss: 1.0725\n",
            "Epoch 120/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0575 - val_loss: 1.0480\n",
            "Epoch 121/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0558 - val_loss: 1.0145\n",
            "Epoch 122/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0540 - val_loss: 0.9738\n",
            "Epoch 123/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0520 - val_loss: 0.9276\n",
            "Epoch 124/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0499 - val_loss: 0.8779\n",
            "Epoch 125/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0478 - val_loss: 0.8263\n",
            "Epoch 126/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0458 - val_loss: 0.7746\n",
            "Epoch 127/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0440 - val_loss: 0.7242\n",
            "Epoch 128/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0423 - val_loss: 0.6761\n",
            "Epoch 129/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0408 - val_loss: 0.6312\n",
            "Epoch 130/1000\n",
            "12/12 [==============================] - 0s 936us/step - loss: 0.0394 - val_loss: 0.5902\n",
            "Epoch 131/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0382 - val_loss: 0.5533\n",
            "Epoch 132/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0372 - val_loss: 0.5208\n",
            "Epoch 133/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0362 - val_loss: 0.4926\n",
            "Epoch 134/1000\n",
            "12/12 [==============================] - 0s 801us/step - loss: 0.0353 - val_loss: 0.4684\n",
            "Epoch 135/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0344 - val_loss: 0.4482\n",
            "Epoch 136/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0334 - val_loss: 0.4314\n",
            "Epoch 137/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0325 - val_loss: 0.4177\n",
            "Epoch 138/1000\n",
            "12/12 [==============================] - 0s 706us/step - loss: 0.0316 - val_loss: 0.4067\n",
            "Epoch 139/1000\n",
            "12/12 [==============================] - 0s 793us/step - loss: 0.0307 - val_loss: 0.3978\n",
            "Epoch 140/1000\n",
            "12/12 [==============================] - 0s 793us/step - loss: 0.0297 - val_loss: 0.3906\n",
            "Epoch 141/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0288 - val_loss: 0.3847\n",
            "Epoch 142/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0279 - val_loss: 0.3796\n",
            "Epoch 143/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0271 - val_loss: 0.3749\n",
            "Epoch 144/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0263 - val_loss: 0.3702\n",
            "Epoch 145/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0256 - val_loss: 0.3652\n",
            "Epoch 146/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0249 - val_loss: 0.3596\n",
            "Epoch 147/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0242 - val_loss: 0.3533\n",
            "Epoch 148/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0236 - val_loss: 0.3460\n",
            "Epoch 149/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0231 - val_loss: 0.3378\n",
            "Epoch 150/1000\n",
            "12/12 [==============================] - 0s 942us/step - loss: 0.0226 - val_loss: 0.3285\n",
            "Epoch 151/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0221 - val_loss: 0.3183\n",
            "Epoch 152/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0216 - val_loss: 0.3072\n",
            "Epoch 153/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0211 - val_loss: 0.2954\n",
            "Epoch 154/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0206 - val_loss: 0.2831\n",
            "Epoch 155/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0201 - val_loss: 0.2704\n",
            "Epoch 156/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.2575\n",
            "Epoch 157/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0192 - val_loss: 0.2447\n",
            "Epoch 158/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0188 - val_loss: 0.2320\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 159/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0184 - val_loss: 0.2197\n",
            "Epoch 160/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0180 - val_loss: 0.2078\n",
            "Epoch 161/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0176 - val_loss: 0.1965\n",
            "Epoch 162/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0172 - val_loss: 0.1858\n",
            "Epoch 163/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0169 - val_loss: 0.1758\n",
            "Epoch 164/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0166 - val_loss: 0.1664\n",
            "Epoch 165/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0163 - val_loss: 0.1578\n",
            "Epoch 166/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.1498\n",
            "Epoch 167/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0157 - val_loss: 0.1425\n",
            "Epoch 168/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0154 - val_loss: 0.1357\n",
            "Epoch 169/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0152 - val_loss: 0.1295\n",
            "Epoch 170/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0149 - val_loss: 0.1238\n",
            "Epoch 171/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0146 - val_loss: 0.1186\n",
            "Epoch 172/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0144 - val_loss: 0.1137\n",
            "Epoch 173/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0142 - val_loss: 0.1091\n",
            "Epoch 174/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0140 - val_loss: 0.1049\n",
            "Epoch 175/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0137 - val_loss: 0.1008\n",
            "Epoch 176/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0135 - val_loss: 0.0969\n",
            "Epoch 177/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0133 - val_loss: 0.0932\n",
            "Epoch 178/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0131 - val_loss: 0.0896\n",
            "Epoch 179/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0860\n",
            "Epoch 180/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0128 - val_loss: 0.0826\n",
            "Epoch 181/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0126 - val_loss: 0.0793\n",
            "Epoch 182/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0124 - val_loss: 0.0760\n",
            "Epoch 183/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0123 - val_loss: 0.0728\n",
            "Epoch 184/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0121 - val_loss: 0.0698\n",
            "Epoch 185/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0119 - val_loss: 0.0668\n",
            "Epoch 186/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0118 - val_loss: 0.0639\n",
            "Epoch 187/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0116 - val_loss: 0.0611\n",
            "Epoch 188/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0115 - val_loss: 0.0584\n",
            "Epoch 189/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0113 - val_loss: 0.0559\n",
            "Epoch 190/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0112 - val_loss: 0.0535\n",
            "Epoch 191/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0111 - val_loss: 0.0513\n",
            "Epoch 192/1000\n",
            "12/12 [==============================] - 0s 862us/step - loss: 0.0109 - val_loss: 0.0491\n",
            "Epoch 193/1000\n",
            "12/12 [==============================] - 0s 711us/step - loss: 0.0108 - val_loss: 0.0472\n",
            "Epoch 194/1000\n",
            "12/12 [==============================] - 0s 715us/step - loss: 0.0107 - val_loss: 0.0453\n",
            "Epoch 195/1000\n",
            "12/12 [==============================] - 0s 800us/step - loss: 0.0106 - val_loss: 0.0436\n",
            "Epoch 196/1000\n",
            "12/12 [==============================] - 0s 525us/step - loss: 0.0104 - val_loss: 0.0420\n",
            "Epoch 197/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0103 - val_loss: 0.0405\n",
            "Epoch 198/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0102 - val_loss: 0.0391\n",
            "Epoch 199/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0101 - val_loss: 0.0378\n",
            "Epoch 200/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0100 - val_loss: 0.0366\n",
            "Epoch 201/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0099 - val_loss: 0.0354\n",
            "Epoch 202/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0098 - val_loss: 0.0343\n",
            "Epoch 203/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0097 - val_loss: 0.0333\n",
            "Epoch 204/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0096 - val_loss: 0.0323\n",
            "Epoch 205/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0095 - val_loss: 0.0313\n",
            "Epoch 206/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0094 - val_loss: 0.0303\n",
            "Epoch 207/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0093 - val_loss: 0.0294\n",
            "Epoch 208/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0092 - val_loss: 0.0285\n",
            "Epoch 209/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0091 - val_loss: 0.0276\n",
            "Epoch 210/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0090 - val_loss: 0.0267\n",
            "Epoch 211/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0089 - val_loss: 0.0258\n",
            "Epoch 212/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0089 - val_loss: 0.0250\n",
            "Epoch 213/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0088 - val_loss: 0.0242\n",
            "Epoch 214/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0087 - val_loss: 0.0234\n",
            "Epoch 215/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0086 - val_loss: 0.0226\n",
            "Epoch 216/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0085 - val_loss: 0.0219\n",
            "Epoch 217/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0085 - val_loss: 0.0212\n",
            "Epoch 218/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0084 - val_loss: 0.0206\n",
            "Epoch 219/1000\n",
            "12/12 [==============================] - 0s 668us/step - loss: 0.0083 - val_loss: 0.0200\n",
            "Epoch 220/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 0.0082 - val_loss: 0.0194\n",
            "Epoch 221/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0082 - val_loss: 0.0188\n",
            "Epoch 222/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0081 - val_loss: 0.0183\n",
            "Epoch 223/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0080 - val_loss: 0.0178\n",
            "Epoch 224/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0080 - val_loss: 0.0173\n",
            "Epoch 225/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0079 - val_loss: 0.0168\n",
            "Epoch 226/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0078 - val_loss: 0.0164\n",
            "Epoch 227/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0078 - val_loss: 0.0159\n",
            "Epoch 228/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0077 - val_loss: 0.0155\n",
            "Epoch 229/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0077 - val_loss: 0.0151\n",
            "Epoch 230/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0076 - val_loss: 0.0147\n",
            "Epoch 231/1000\n",
            "12/12 [==============================] - 0s 663us/step - loss: 0.0076 - val_loss: 0.0143\n",
            "Epoch 232/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0075 - val_loss: 0.0140\n",
            "Epoch 233/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0074 - val_loss: 0.0136\n",
            "Epoch 234/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0074 - val_loss: 0.0132\n",
            "Epoch 235/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0073 - val_loss: 0.0129\n",
            "Epoch 236/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0073 - val_loss: 0.0125\n",
            "Epoch 237/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0072 - val_loss: 0.0122\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 238/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0072 - val_loss: 0.0119\n",
            "Epoch 239/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0071 - val_loss: 0.0115\n",
            "Epoch 240/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0071 - val_loss: 0.0112\n",
            "Epoch 241/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0070 - val_loss: 0.0109\n",
            "Epoch 242/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0070 - val_loss: 0.0107\n",
            "Epoch 243/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0070 - val_loss: 0.0104\n",
            "Epoch 244/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0069 - val_loss: 0.0101\n",
            "Epoch 245/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0069 - val_loss: 0.0099\n",
            "Epoch 246/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0068 - val_loss: 0.0096\n",
            "Epoch 247/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0068 - val_loss: 0.0094\n",
            "Epoch 248/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0067 - val_loss: 0.0092\n",
            "Epoch 249/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0067 - val_loss: 0.0090\n",
            "Epoch 250/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0067 - val_loss: 0.0087\n",
            "Epoch 251/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0066 - val_loss: 0.0085\n",
            "Epoch 252/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0066 - val_loss: 0.0083\n",
            "Epoch 253/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0065 - val_loss: 0.0081\n",
            "Epoch 254/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0065 - val_loss: 0.0079\n",
            "Epoch 255/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0065 - val_loss: 0.0077\n",
            "Epoch 256/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0064 - val_loss: 0.0076\n",
            "Epoch 257/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0064 - val_loss: 0.0074\n",
            "Epoch 258/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0064 - val_loss: 0.0072\n",
            "Epoch 259/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0063 - val_loss: 0.0070\n",
            "Epoch 260/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0063 - val_loss: 0.0069\n",
            "Epoch 261/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0062 - val_loss: 0.0067\n",
            "Epoch 262/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0062 - val_loss: 0.0065\n",
            "Epoch 263/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0062 - val_loss: 0.0064\n",
            "Epoch 264/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0061 - val_loss: 0.0062\n",
            "Epoch 265/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0061 - val_loss: 0.0061\n",
            "Epoch 266/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0061 - val_loss: 0.0059\n",
            "Epoch 267/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0061 - val_loss: 0.0058\n",
            "Epoch 268/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0060 - val_loss: 0.0057\n",
            "Epoch 269/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0060 - val_loss: 0.0055\n",
            "Epoch 270/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0060 - val_loss: 0.0054\n",
            "Epoch 271/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0059 - val_loss: 0.0053\n",
            "Epoch 272/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0059 - val_loss: 0.0051\n",
            "Epoch 273/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0059 - val_loss: 0.0050\n",
            "Epoch 274/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0058 - val_loss: 0.0049\n",
            "Epoch 275/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0058 - val_loss: 0.0048\n",
            "Epoch 276/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0058 - val_loss: 0.0047\n",
            "Epoch 277/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0058 - val_loss: 0.0046\n",
            "Epoch 278/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0057 - val_loss: 0.0045\n",
            "Epoch 279/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0057 - val_loss: 0.0044\n",
            "Epoch 280/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0057 - val_loss: 0.0042\n",
            "Epoch 281/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0056 - val_loss: 0.0041\n",
            "Epoch 282/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0056 - val_loss: 0.0040\n",
            "Epoch 283/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0056 - val_loss: 0.0039\n",
            "Epoch 284/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0056 - val_loss: 0.0039\n",
            "Epoch 285/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0055 - val_loss: 0.0038\n",
            "Epoch 286/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0055 - val_loss: 0.0037\n",
            "Epoch 287/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0055 - val_loss: 0.0036\n",
            "Epoch 288/1000\n",
            "12/12 [==============================] - 0s 712us/step - loss: 0.0055 - val_loss: 0.0035\n",
            "Epoch 289/1000\n",
            "12/12 [==============================] - 0s 792us/step - loss: 0.0054 - val_loss: 0.0034\n",
            "Epoch 290/1000\n",
            "12/12 [==============================] - 0s 618us/step - loss: 0.0054 - val_loss: 0.0033\n",
            "Epoch 291/1000\n",
            "12/12 [==============================] - 0s 725us/step - loss: 0.0054 - val_loss: 0.0032\n",
            "Epoch 292/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0054 - val_loss: 0.0032\n",
            "Epoch 293/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0053 - val_loss: 0.0031\n",
            "Epoch 294/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0053 - val_loss: 0.0030\n",
            "Epoch 295/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0053 - val_loss: 0.0029\n",
            "Epoch 296/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0053 - val_loss: 0.0029\n",
            "Epoch 297/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0052 - val_loss: 0.0028\n",
            "Epoch 298/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0052 - val_loss: 0.0027\n",
            "Epoch 299/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0052 - val_loss: 0.0026\n",
            "Epoch 300/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0052 - val_loss: 0.0026\n",
            "Epoch 301/1000\n",
            "12/12 [==============================] - 0s 436us/step - loss: 0.0051 - val_loss: 0.0025\n",
            "Epoch 302/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0051 - val_loss: 0.0024\n",
            "Epoch 303/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0051 - val_loss: 0.0024\n",
            "Epoch 304/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0051 - val_loss: 0.0023\n",
            "Epoch 305/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0051 - val_loss: 0.0023\n",
            "Epoch 306/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0050 - val_loss: 0.0022\n",
            "Epoch 307/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0050 - val_loss: 0.0021\n",
            "Epoch 308/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0050 - val_loss: 0.0021\n",
            "Epoch 309/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0050 - val_loss: 0.0020\n",
            "Epoch 310/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0049 - val_loss: 0.0020\n",
            "Epoch 311/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0049 - val_loss: 0.0019\n",
            "Epoch 312/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0049 - val_loss: 0.0019\n",
            "Epoch 313/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0049 - val_loss: 0.0018\n",
            "Epoch 314/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0049 - val_loss: 0.0018\n",
            "Epoch 315/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0048 - val_loss: 0.0017\n",
            "Epoch 316/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0048 - val_loss: 0.0017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 317/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0048 - val_loss: 0.0016\n",
            "Epoch 318/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0048 - val_loss: 0.0016\n",
            "Epoch 319/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0048 - val_loss: 0.0015\n",
            "Epoch 320/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0047 - val_loss: 0.0015\n",
            "Epoch 321/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0047 - val_loss: 0.0014\n",
            "Epoch 322/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0047 - val_loss: 0.0014\n",
            "Epoch 323/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0047 - val_loss: 0.0013\n",
            "Epoch 324/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0047 - val_loss: 0.0013\n",
            "Epoch 325/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0046 - val_loss: 0.0013\n",
            "Epoch 326/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0046 - val_loss: 0.0012\n",
            "Epoch 327/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0046 - val_loss: 0.0012\n",
            "Epoch 328/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0046 - val_loss: 0.0011\n",
            "Epoch 329/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0046 - val_loss: 0.0011\n",
            "Epoch 330/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0045 - val_loss: 0.0011\n",
            "Epoch 331/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0045 - val_loss: 0.0010\n",
            "Epoch 332/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 9.9812e-04\n",
            "Epoch 333/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0045 - val_loss: 9.6419e-04\n",
            "Epoch 334/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0045 - val_loss: 9.3142e-04\n",
            "Epoch 335/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 8.9896e-04\n",
            "Epoch 336/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0044 - val_loss: 8.6714e-04\n",
            "Epoch 337/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0044 - val_loss: 8.3669e-04\n",
            "Epoch 338/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0044 - val_loss: 8.0670e-04\n",
            "Epoch 339/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0044 - val_loss: 7.7748e-04\n",
            "Epoch 340/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0044 - val_loss: 7.4845e-04\n",
            "Epoch 341/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 7.2064e-04\n",
            "Epoch 342/1000\n",
            "12/12 [==============================] - 0s 663us/step - loss: 0.0043 - val_loss: 6.9362e-04\n",
            "Epoch 343/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0043 - val_loss: 6.6751e-04\n",
            "Epoch 344/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0043 - val_loss: 6.4161e-04\n",
            "Epoch 345/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0043 - val_loss: 6.1682e-04\n",
            "Epoch 346/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0043 - val_loss: 5.9278e-04\n",
            "Epoch 347/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 0.0042 - val_loss: 5.6961e-04\n",
            "Epoch 348/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 5.4659e-04\n",
            "Epoch 349/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0042 - val_loss: 5.2482e-04\n",
            "Epoch 350/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0042 - val_loss: 5.0294e-04\n",
            "Epoch 351/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0042 - val_loss: 4.8186e-04\n",
            "Epoch 352/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0041 - val_loss: 4.6153e-04\n",
            "Epoch 353/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0041 - val_loss: 4.4184e-04\n",
            "Epoch 354/1000\n",
            "12/12 [==============================] - 0s 668us/step - loss: 0.0041 - val_loss: 4.2257e-04\n",
            "Epoch 355/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 4.0422e-04\n",
            "Epoch 356/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0041 - val_loss: 3.8614e-04\n",
            "Epoch 357/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0041 - val_loss: 3.6887e-04\n",
            "Epoch 358/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0040 - val_loss: 3.5210e-04\n",
            "Epoch 359/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0040 - val_loss: 3.3599e-04\n",
            "Epoch 360/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 3.2061e-04\n",
            "Epoch 361/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0040 - val_loss: 3.0571e-04\n",
            "Epoch 362/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 2.9149e-04\n",
            "Epoch 363/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0040 - val_loss: 2.7745e-04\n",
            "Epoch 364/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 2.6403e-04\n",
            "Epoch 365/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 2.5139e-04\n",
            "Epoch 366/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 2.3919e-04\n",
            "Epoch 367/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 2.2774e-04\n",
            "Epoch 368/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0039 - val_loss: 2.1662e-04\n",
            "Epoch 369/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 2.0604e-04\n",
            "Epoch 370/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0039 - val_loss: 1.9605e-04\n",
            "Epoch 371/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0038 - val_loss: 1.8641e-04\n",
            "Epoch 372/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0038 - val_loss: 1.7738e-04\n",
            "Epoch 373/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 1.6875e-04\n",
            "Epoch 374/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 1.6052e-04\n",
            "Epoch 375/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0038 - val_loss: 1.5310e-04\n",
            "Epoch 376/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0038 - val_loss: 1.4604e-04\n",
            "Epoch 377/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 1.3931e-04\n",
            "Epoch 378/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 1.3326e-04\n",
            "Epoch 379/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 1.2768e-04\n",
            "Epoch 380/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 1.2245e-04\n",
            "Epoch 381/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 1.1781e-04\n",
            "Epoch 382/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 1.1344e-04\n",
            "Epoch 383/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0037 - val_loss: 1.0975e-04\n",
            "Epoch 384/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 1.0641e-04\n",
            "Epoch 385/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 1.0357e-04\n",
            "Epoch 386/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 1.0102e-04\n",
            "Epoch 387/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0036 - val_loss: 9.9062e-05\n",
            "Epoch 388/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 9.7538e-05\n",
            "Epoch 389/1000\n",
            "12/12 [==============================] - 0s 675us/step - loss: 0.0036 - val_loss: 9.6351e-05\n",
            "Epoch 390/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0036 - val_loss: 9.5589e-05\n",
            "Epoch 391/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0035 - val_loss: 9.5263e-05\n",
            "Epoch 392/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 9.5475e-05\n",
            "Epoch 393/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 9.5857e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 394/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0035 - val_loss: 9.6816e-05\n",
            "Epoch 395/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 9.8164e-05\n",
            "Epoch 396/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0035 - val_loss: 9.9832e-05\n",
            "Epoch 397/1000\n",
            "12/12 [==============================] - 0s 697us/step - loss: 0.0035 - val_loss: 1.0216e-04\n",
            "Epoch 398/1000\n",
            "12/12 [==============================] - 0s 811us/step - loss: 0.0034 - val_loss: 1.0483e-04\n",
            "Epoch 399/1000\n",
            "12/12 [==============================] - 0s 819us/step - loss: 0.0034 - val_loss: 1.0751e-04\n",
            "Epoch 400/1000\n",
            "12/12 [==============================] - 0s 712us/step - loss: 0.0034 - val_loss: 1.1089e-04\n",
            "Epoch 401/1000\n",
            "12/12 [==============================] - 0s 787us/step - loss: 0.0034 - val_loss: 1.1472e-04\n",
            "Epoch 402/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 1.1885e-04\n",
            "Epoch 403/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0034 - val_loss: 1.2336e-04\n",
            "Epoch 404/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0034 - val_loss: 1.2828e-04\n",
            "Epoch 405/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0033 - val_loss: 1.3356e-04\n",
            "Epoch 406/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0033 - val_loss: 1.3926e-04\n",
            "Epoch 407/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0033 - val_loss: 1.4538e-04\n",
            "Epoch 408/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0033 - val_loss: 1.5189e-04\n",
            "Epoch 409/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0033 - val_loss: 1.5854e-04\n",
            "Epoch 410/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0033 - val_loss: 1.6587e-04\n",
            "Epoch 411/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0033 - val_loss: 1.7305e-04\n",
            "Epoch 412/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0032 - val_loss: 1.8108e-04\n",
            "Epoch 413/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0032 - val_loss: 1.8926e-04\n",
            "Epoch 414/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0032 - val_loss: 1.9780e-04\n",
            "Epoch 415/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0032 - val_loss: 2.0678e-04\n",
            "Epoch 416/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0032 - val_loss: 2.1605e-04\n",
            "Epoch 417/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0032 - val_loss: 2.2567e-04\n",
            "Epoch 418/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0032 - val_loss: 2.3545e-04\n",
            "Epoch 419/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0032 - val_loss: 2.4582e-04\n",
            "Epoch 420/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0031 - val_loss: 2.5629e-04\n",
            "Epoch 421/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0031 - val_loss: 2.6708e-04\n",
            "Epoch 422/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0031 - val_loss: 2.7823e-04\n",
            "Epoch 423/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0031 - val_loss: 2.9007e-04\n",
            "Epoch 424/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0031 - val_loss: 3.0162e-04\n",
            "Epoch 425/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0031 - val_loss: 3.1387e-04\n",
            "Epoch 426/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0031 - val_loss: 3.2647e-04\n",
            "Epoch 427/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0031 - val_loss: 3.3966e-04\n",
            "Epoch 428/1000\n",
            "12/12 [==============================] - 0s 662us/step - loss: 0.0030 - val_loss: 3.5291e-04\n",
            "Epoch 429/1000\n",
            "12/12 [==============================] - 0s 620us/step - loss: 0.0030 - val_loss: 3.6626e-04\n",
            "Epoch 430/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0030 - val_loss: 3.8050e-04\n",
            "Epoch 431/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0030 - val_loss: 3.9455e-04\n",
            "Epoch 432/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0030 - val_loss: 4.0904e-04\n",
            "Epoch 433/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0030 - val_loss: 4.2354e-04\n",
            "Epoch 434/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0030 - val_loss: 4.3851e-04\n",
            "Epoch 435/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0030 - val_loss: 4.5382e-04\n",
            "Epoch 436/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0029 - val_loss: 4.6923e-04\n",
            "Epoch 437/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0029 - val_loss: 4.8512e-04\n",
            "Epoch 438/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0029 - val_loss: 5.0168e-04\n",
            "Epoch 439/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0029 - val_loss: 5.1787e-04\n",
            "Epoch 440/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0029 - val_loss: 5.3466e-04\n",
            "Epoch 441/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0029 - val_loss: 5.5177e-04\n",
            "Epoch 442/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0029 - val_loss: 5.6930e-04\n",
            "Epoch 443/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 0.0029 - val_loss: 5.8716e-04\n",
            "Epoch 444/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0028 - val_loss: 6.0473e-04\n",
            "Epoch 445/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0028 - val_loss: 6.2304e-04\n",
            "Epoch 446/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0028 - val_loss: 6.4124e-04\n",
            "Epoch 447/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0028 - val_loss: 6.6052e-04\n",
            "Epoch 448/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0028 - val_loss: 6.7894e-04\n",
            "Epoch 449/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0028 - val_loss: 6.9848e-04\n",
            "Epoch 450/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0028 - val_loss: 7.1792e-04\n",
            "Epoch 451/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0028 - val_loss: 7.3769e-04\n",
            "Epoch 452/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0028 - val_loss: 7.5772e-04\n",
            "Epoch 453/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0027 - val_loss: 7.7742e-04\n",
            "Epoch 454/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0027 - val_loss: 7.9797e-04\n",
            "Epoch 455/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0027 - val_loss: 8.1916e-04\n",
            "Epoch 456/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0027 - val_loss: 8.4019e-04\n",
            "Epoch 457/1000\n",
            "12/12 [==============================] - 0s 668us/step - loss: 0.0027 - val_loss: 8.6177e-04\n",
            "Epoch 458/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 0.0027 - val_loss: 8.8307e-04\n",
            "Epoch 459/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0027 - val_loss: 9.0547e-04\n",
            "Epoch 460/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0027 - val_loss: 9.2761e-04\n",
            "Epoch 461/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0027 - val_loss: 9.4946e-04\n",
            "Epoch 462/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0026 - val_loss: 9.7246e-04\n",
            "Epoch 463/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0026 - val_loss: 9.9480e-04\n",
            "Epoch 464/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0026 - val_loss: 0.0010\n",
            "Epoch 465/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0026 - val_loss: 0.0010\n",
            "Epoch 466/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0026 - val_loss: 0.0011\n",
            "Epoch 467/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0026 - val_loss: 0.0011\n",
            "Epoch 468/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0026 - val_loss: 0.0011\n",
            "Epoch 469/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0026 - val_loss: 0.0011\n",
            "Epoch 470/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0026 - val_loss: 0.0012\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 471/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0025 - val_loss: 0.0012\n",
            "Epoch 472/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0025 - val_loss: 0.0012\n",
            "Epoch 473/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0025 - val_loss: 0.0012\n",
            "Epoch 474/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0025 - val_loss: 0.0013\n",
            "Epoch 475/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0025 - val_loss: 0.0013\n",
            "Epoch 476/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0025 - val_loss: 0.0013\n",
            "Epoch 477/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0025 - val_loss: 0.0013\n",
            "Epoch 478/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0025 - val_loss: 0.0014\n",
            "Epoch 479/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0025 - val_loss: 0.0014\n",
            "Epoch 480/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0025 - val_loss: 0.0014\n",
            "Epoch 481/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0024 - val_loss: 0.0014\n",
            "Epoch 482/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0024 - val_loss: 0.0015\n",
            "Epoch 483/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0024 - val_loss: 0.0015\n",
            "Epoch 484/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0024 - val_loss: 0.0015\n",
            "Epoch 485/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0024 - val_loss: 0.0015\n",
            "Epoch 486/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 487/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 488/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0024 - val_loss: 0.0016\n",
            "Epoch 489/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 490/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 491/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 492/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0023 - val_loss: 0.0017\n",
            "Epoch 493/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0023 - val_loss: 0.0018\n",
            "Epoch 494/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0023 - val_loss: 0.0018\n",
            "Epoch 495/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0023 - val_loss: 0.0018\n",
            "Epoch 496/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0023 - val_loss: 0.0019\n",
            "Epoch 497/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0023 - val_loss: 0.0019\n",
            "Epoch 498/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0023 - val_loss: 0.0019\n",
            "Epoch 499/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0023 - val_loss: 0.0019\n",
            "Epoch 500/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0023 - val_loss: 0.0020\n",
            "Epoch 501/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0023 - val_loss: 0.0020\n",
            "Epoch 502/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 503/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0022 - val_loss: 0.0021\n",
            "Epoch 504/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0022 - val_loss: 0.0021\n",
            "Epoch 505/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0022 - val_loss: 0.0021\n",
            "Epoch 506/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 507/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 508/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 509/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 510/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 511/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 512/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 513/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0022 - val_loss: 0.0024\n",
            "Epoch 514/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0021 - val_loss: 0.0024\n",
            "Epoch 515/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0021 - val_loss: 0.0024\n",
            "Epoch 516/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0021 - val_loss: 0.0025\n",
            "Epoch 517/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0021 - val_loss: 0.0025\n",
            "Epoch 518/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0021 - val_loss: 0.0025\n",
            "Epoch 519/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 520/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 521/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 522/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 523/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0021 - val_loss: 0.0027\n",
            "Epoch 524/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0021 - val_loss: 0.0027\n",
            "Epoch 525/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0021 - val_loss: 0.0027\n",
            "Epoch 526/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0020 - val_loss: 0.0028\n",
            "Epoch 527/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0020 - val_loss: 0.0028\n",
            "Epoch 528/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0020 - val_loss: 0.0028\n",
            "Epoch 529/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0020 - val_loss: 0.0029\n",
            "Epoch 530/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0020 - val_loss: 0.0029\n",
            "Epoch 531/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0020 - val_loss: 0.0029\n",
            "Epoch 532/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0020 - val_loss: 0.0030\n",
            "Epoch 533/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0020 - val_loss: 0.0030\n",
            "Epoch 534/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0020 - val_loss: 0.0030\n",
            "Epoch 535/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 536/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 537/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0020 - val_loss: 0.0031\n",
            "Epoch 538/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0020 - val_loss: 0.0032\n",
            "Epoch 539/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 540/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0019 - val_loss: 0.0032\n",
            "Epoch 541/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 542/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 543/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 544/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0019 - val_loss: 0.0034\n",
            "Epoch 545/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0019 - val_loss: 0.0034\n",
            "Epoch 546/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0019 - val_loss: 0.0034\n",
            "Epoch 547/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0019 - val_loss: 0.0034\n",
            "Epoch 548/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 549/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0019 - val_loss: 0.0035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 550/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 551/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 552/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 553/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0018 - val_loss: 0.0036\n",
            "Epoch 554/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 555/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 556/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0018 - val_loss: 0.0037\n",
            "Epoch 557/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0018 - val_loss: 0.0038\n",
            "Epoch 558/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0018 - val_loss: 0.0038\n",
            "Epoch 559/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0018 - val_loss: 0.0038\n",
            "Epoch 560/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0018 - val_loss: 0.0039\n",
            "Epoch 561/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0018 - val_loss: 0.0039\n",
            "Epoch 562/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0018 - val_loss: 0.0039\n",
            "Epoch 563/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0018 - val_loss: 0.0040\n",
            "Epoch 564/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0018 - val_loss: 0.0040\n",
            "Epoch 565/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0018 - val_loss: 0.0040\n",
            "Epoch 566/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0018 - val_loss: 0.0041\n",
            "Epoch 567/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0018 - val_loss: 0.0041\n",
            "Epoch 568/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0017 - val_loss: 0.0041\n",
            "Epoch 569/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0017 - val_loss: 0.0042\n",
            "Epoch 570/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0017 - val_loss: 0.0042\n",
            "Epoch 571/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0017 - val_loss: 0.0042\n",
            "Epoch 572/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0017 - val_loss: 0.0043\n",
            "Epoch 573/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0017 - val_loss: 0.0043\n",
            "Epoch 574/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0017 - val_loss: 0.0043\n",
            "Epoch 575/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0017 - val_loss: 0.0043\n",
            "Epoch 576/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0017 - val_loss: 0.0044\n",
            "Epoch 577/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0017 - val_loss: 0.0044\n",
            "Epoch 578/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0017 - val_loss: 0.0044\n",
            "Epoch 579/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0017 - val_loss: 0.0045\n",
            "Epoch 580/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0017 - val_loss: 0.0045\n",
            "Epoch 581/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0017 - val_loss: 0.0045\n",
            "Epoch 582/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0017 - val_loss: 0.0046\n",
            "Epoch 583/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0017 - val_loss: 0.0046\n",
            "Epoch 584/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0017 - val_loss: 0.0046\n",
            "Epoch 585/1000\n",
            "12/12 [==============================] - 0s 705us/step - loss: 0.0016 - val_loss: 0.0047\n",
            "Epoch 586/1000\n",
            "12/12 [==============================] - 0s 700us/step - loss: 0.0016 - val_loss: 0.0047\n",
            "Epoch 587/1000\n",
            "12/12 [==============================] - 0s 703us/step - loss: 0.0016 - val_loss: 0.0047\n",
            "Epoch 588/1000\n",
            "12/12 [==============================] - 0s 596us/step - loss: 0.0016 - val_loss: 0.0047\n",
            "Epoch 589/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0016 - val_loss: 0.0048\n",
            "Epoch 590/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0016 - val_loss: 0.0048\n",
            "Epoch 591/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0016 - val_loss: 0.0048\n",
            "Epoch 592/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0016 - val_loss: 0.0049\n",
            "Epoch 593/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0016 - val_loss: 0.0049\n",
            "Epoch 594/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0016 - val_loss: 0.0049\n",
            "Epoch 595/1000\n",
            "12/12 [==============================] - 0s 973us/step - loss: 0.0016 - val_loss: 0.0049\n",
            "Epoch 596/1000\n",
            "12/12 [==============================] - 0s 636us/step - loss: 0.0016 - val_loss: 0.0050\n",
            "Epoch 597/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0016 - val_loss: 0.0050\n",
            "Epoch 598/1000\n",
            "12/12 [==============================] - 0s 847us/step - loss: 0.0016 - val_loss: 0.0050\n",
            "Epoch 599/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0016 - val_loss: 0.0051\n",
            "Epoch 600/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0016 - val_loss: 0.0051\n",
            "Epoch 601/1000\n",
            "12/12 [==============================] - 0s 689us/step - loss: 0.0016 - val_loss: 0.0051\n",
            "Epoch 602/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0016 - val_loss: 0.0051\n",
            "Epoch 603/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0052\n",
            "Epoch 604/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0052\n",
            "Epoch 605/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0015 - val_loss: 0.0052\n",
            "Epoch 606/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0053\n",
            "Epoch 607/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0053\n",
            "Epoch 608/1000\n",
            "12/12 [==============================] - 0s 735us/step - loss: 0.0015 - val_loss: 0.0053\n",
            "Epoch 609/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0053\n",
            "Epoch 610/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0054\n",
            "Epoch 611/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0015 - val_loss: 0.0054\n",
            "Epoch 612/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0054\n",
            "Epoch 613/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0015 - val_loss: 0.0055\n",
            "Epoch 614/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0055\n",
            "Epoch 615/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0055\n",
            "Epoch 616/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0055\n",
            "Epoch 617/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0056\n",
            "Epoch 618/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0056\n",
            "Epoch 619/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0056\n",
            "Epoch 620/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0056\n",
            "Epoch 621/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0015 - val_loss: 0.0057\n",
            "Epoch 622/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0015 - val_loss: 0.0057\n",
            "Epoch 623/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 0.0015 - val_loss: 0.0057\n",
            "Epoch 624/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0057\n",
            "Epoch 625/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0014 - val_loss: 0.0058\n",
            "Epoch 626/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0058\n",
            "Epoch 627/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0058\n",
            "Epoch 628/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 629/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0059\n",
            "Epoch 630/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0059\n",
            "Epoch 631/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0059\n",
            "Epoch 632/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0059\n",
            "Epoch 633/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0059\n",
            "Epoch 634/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0014 - val_loss: 0.0060\n",
            "Epoch 635/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0060\n",
            "Epoch 636/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0060\n",
            "Epoch 637/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0014 - val_loss: 0.0060\n",
            "Epoch 638/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 639/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 640/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 641/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 642/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 643/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0062\n",
            "Epoch 644/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0062\n",
            "Epoch 645/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0062\n",
            "Epoch 646/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0014 - val_loss: 0.0062\n",
            "Epoch 647/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0063\n",
            "Epoch 648/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0063\n",
            "Epoch 649/1000\n",
            "12/12 [==============================] - 0s 884us/step - loss: 0.0013 - val_loss: 0.0063\n",
            "Epoch 650/1000\n",
            "12/12 [==============================] - 0s 704us/step - loss: 0.0013 - val_loss: 0.0063\n",
            "Epoch 651/1000\n",
            "12/12 [==============================] - 0s 710us/step - loss: 0.0013 - val_loss: 0.0063\n",
            "Epoch 652/1000\n",
            "12/12 [==============================] - 0s 793us/step - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 653/1000\n",
            "12/12 [==============================] - 0s 522us/step - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 654/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 655/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 656/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 657/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 658/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 659/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 660/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 661/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 662/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 663/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 664/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 665/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 666/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 667/1000\n",
            "12/12 [==============================] - 0s 763us/step - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 668/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 669/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0067\n",
            "Epoch 670/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0067\n",
            "Epoch 671/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0067\n",
            "Epoch 672/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0067\n",
            "Epoch 673/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0013 - val_loss: 0.0067\n",
            "Epoch 674/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0067\n",
            "Epoch 675/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0067\n",
            "Epoch 676/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0068\n",
            "Epoch 677/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0068\n",
            "Epoch 678/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0068\n",
            "Epoch 679/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0068\n",
            "Epoch 680/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0068\n",
            "Epoch 681/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0068\n",
            "Epoch 682/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0068\n",
            "Epoch 683/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0069\n",
            "Epoch 684/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0069\n",
            "Epoch 685/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0069\n",
            "Epoch 686/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0069\n",
            "Epoch 687/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0069\n",
            "Epoch 688/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 0.0069\n",
            "Epoch 689/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0069\n",
            "Epoch 690/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0069\n",
            "Epoch 691/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0070\n",
            "Epoch 692/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0070\n",
            "Epoch 693/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0070\n",
            "Epoch 694/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0070\n",
            "Epoch 695/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0070\n",
            "Epoch 696/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0070\n",
            "Epoch 697/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0070\n",
            "Epoch 698/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0070\n",
            "Epoch 699/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0070\n",
            "Epoch 700/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0071\n",
            "Epoch 701/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0071\n",
            "Epoch 702/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0071\n",
            "Epoch 703/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0071\n",
            "Epoch 704/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0071\n",
            "Epoch 705/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0012 - val_loss: 0.0071\n",
            "Epoch 706/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0011 - val_loss: 0.0071\n",
            "Epoch 707/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0071\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 708/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0071\n",
            "Epoch 709/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0071\n",
            "Epoch 710/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0071\n",
            "Epoch 711/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0071\n",
            "Epoch 712/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0071\n",
            "Epoch 713/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 714/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 715/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 716/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 717/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 718/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 719/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 720/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 721/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 722/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 723/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 724/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 725/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 726/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 727/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 728/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 729/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 730/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 731/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 732/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 733/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0072\n",
            "Epoch 734/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0073\n",
            "Epoch 735/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0073\n",
            "Epoch 736/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0073\n",
            "Epoch 737/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0073\n",
            "Epoch 738/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0073\n",
            "Epoch 739/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0073\n",
            "Epoch 740/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0073\n",
            "Epoch 741/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0073\n",
            "Epoch 742/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0011 - val_loss: 0.0073\n",
            "Epoch 743/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0011 - val_loss: 0.0073\n",
            "Epoch 744/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 745/1000\n",
            "12/12 [==============================] - 0s 846us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 746/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 747/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 748/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 749/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 750/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 751/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 752/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 753/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 754/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 755/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 756/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 757/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 758/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 759/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 760/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 761/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 762/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 763/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 764/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 0.0010 - val_loss: 0.0073\n",
            "Epoch 765/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.9953e-04 - val_loss: 0.0072\n",
            "Epoch 766/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 9.9735e-04 - val_loss: 0.0072\n",
            "Epoch 767/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.9521e-04 - val_loss: 0.0072\n",
            "Epoch 768/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.9310e-04 - val_loss: 0.0072\n",
            "Epoch 769/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.9096e-04 - val_loss: 0.0072\n",
            "Epoch 770/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.8876e-04 - val_loss: 0.0072\n",
            "Epoch 771/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.8665e-04 - val_loss: 0.0072\n",
            "Epoch 772/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 9.8446e-04 - val_loss: 0.0072\n",
            "Epoch 773/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.8238e-04 - val_loss: 0.0072\n",
            "Epoch 774/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.8023e-04 - val_loss: 0.0072\n",
            "Epoch 775/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.7810e-04 - val_loss: 0.0072\n",
            "Epoch 776/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.7599e-04 - val_loss: 0.0072\n",
            "Epoch 777/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.7379e-04 - val_loss: 0.0072\n",
            "Epoch 778/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.7174e-04 - val_loss: 0.0072\n",
            "Epoch 779/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 9.6965e-04 - val_loss: 0.0072\n",
            "Epoch 780/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.6763e-04 - val_loss: 0.0072\n",
            "Epoch 781/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.6552e-04 - val_loss: 0.0072\n",
            "Epoch 782/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 9.6335e-04 - val_loss: 0.0072\n",
            "Epoch 783/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.6138e-04 - val_loss: 0.0072\n",
            "Epoch 784/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.5930e-04 - val_loss: 0.0072\n",
            "Epoch 785/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 9.5728e-04 - val_loss: 0.0072\n",
            "Epoch 786/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.5532e-04 - val_loss: 0.0071\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 787/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.5330e-04 - val_loss: 0.0071\n",
            "Epoch 788/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.5126e-04 - val_loss: 0.0071\n",
            "Epoch 789/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 9.4918e-04 - val_loss: 0.0071\n",
            "Epoch 790/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.4727e-04 - val_loss: 0.0071\n",
            "Epoch 791/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.4522e-04 - val_loss: 0.0071\n",
            "Epoch 792/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.4322e-04 - val_loss: 0.0071\n",
            "Epoch 793/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.4125e-04 - val_loss: 0.0071\n",
            "Epoch 794/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 9.3926e-04 - val_loss: 0.0071\n",
            "Epoch 795/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.3722e-04 - val_loss: 0.0071\n",
            "Epoch 796/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.3525e-04 - val_loss: 0.0071\n",
            "Epoch 797/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.3333e-04 - val_loss: 0.0071\n",
            "Epoch 798/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.3135e-04 - val_loss: 0.0071\n",
            "Epoch 799/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.2946e-04 - val_loss: 0.0071\n",
            "Epoch 800/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.2750e-04 - val_loss: 0.0070\n",
            "Epoch 801/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.2560e-04 - val_loss: 0.0070\n",
            "Epoch 802/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 9.2367e-04 - val_loss: 0.0070\n",
            "Epoch 803/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.2175e-04 - val_loss: 0.0070\n",
            "Epoch 804/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.1982e-04 - val_loss: 0.0070\n",
            "Epoch 805/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 9.1783e-04 - val_loss: 0.0070\n",
            "Epoch 806/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.1602e-04 - val_loss: 0.0070\n",
            "Epoch 807/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 9.1411e-04 - val_loss: 0.0070\n",
            "Epoch 808/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 9.1221e-04 - val_loss: 0.0070\n",
            "Epoch 809/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.1027e-04 - val_loss: 0.0070\n",
            "Epoch 810/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.0842e-04 - val_loss: 0.0070\n",
            "Epoch 811/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 9.0650e-04 - val_loss: 0.0070\n",
            "Epoch 812/1000\n",
            "12/12 [==============================] - 0s 669us/step - loss: 9.0469e-04 - val_loss: 0.0069\n",
            "Epoch 813/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 9.0283e-04 - val_loss: 0.0069\n",
            "Epoch 814/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 9.0103e-04 - val_loss: 0.0069\n",
            "Epoch 815/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 8.9910e-04 - val_loss: 0.0069\n",
            "Epoch 816/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.9727e-04 - val_loss: 0.0069\n",
            "Epoch 817/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 8.9538e-04 - val_loss: 0.0069\n",
            "Epoch 818/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.9347e-04 - val_loss: 0.0069\n",
            "Epoch 819/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 8.9173e-04 - val_loss: 0.0069\n",
            "Epoch 820/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.8991e-04 - val_loss: 0.0069\n",
            "Epoch 821/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.8801e-04 - val_loss: 0.0069\n",
            "Epoch 822/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 8.8621e-04 - val_loss: 0.0068\n",
            "Epoch 823/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.8438e-04 - val_loss: 0.0068\n",
            "Epoch 824/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.8257e-04 - val_loss: 0.0068\n",
            "Epoch 825/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 8.8079e-04 - val_loss: 0.0068\n",
            "Epoch 826/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.7894e-04 - val_loss: 0.0068\n",
            "Epoch 827/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.7717e-04 - val_loss: 0.0068\n",
            "Epoch 828/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.7541e-04 - val_loss: 0.0068\n",
            "Epoch 829/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.7355e-04 - val_loss: 0.0068\n",
            "Epoch 830/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 8.7176e-04 - val_loss: 0.0068\n",
            "Epoch 831/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 8.6997e-04 - val_loss: 0.0067\n",
            "Epoch 832/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.6824e-04 - val_loss: 0.0067\n",
            "Epoch 833/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 8.6650e-04 - val_loss: 0.0067\n",
            "Epoch 834/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.6472e-04 - val_loss: 0.0067\n",
            "Epoch 835/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 8.6297e-04 - val_loss: 0.0067\n",
            "Epoch 836/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.6124e-04 - val_loss: 0.0067\n",
            "Epoch 837/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.5957e-04 - val_loss: 0.0067\n",
            "Epoch 838/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.5780e-04 - val_loss: 0.0067\n",
            "Epoch 839/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.5599e-04 - val_loss: 0.0066\n",
            "Epoch 840/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.5431e-04 - val_loss: 0.0066\n",
            "Epoch 841/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.5254e-04 - val_loss: 0.0066\n",
            "Epoch 842/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.5074e-04 - val_loss: 0.0066\n",
            "Epoch 843/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 8.4904e-04 - val_loss: 0.0066\n",
            "Epoch 844/1000\n",
            "12/12 [==============================] - 0s 663us/step - loss: 8.4733e-04 - val_loss: 0.0066\n",
            "Epoch 845/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 8.4571e-04 - val_loss: 0.0066\n",
            "Epoch 846/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 8.4396e-04 - val_loss: 0.0066\n",
            "Epoch 847/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.4229e-04 - val_loss: 0.0065\n",
            "Epoch 848/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.4053e-04 - val_loss: 0.0065\n",
            "Epoch 849/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.3886e-04 - val_loss: 0.0065\n",
            "Epoch 850/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.3713e-04 - val_loss: 0.0065\n",
            "Epoch 851/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.3542e-04 - val_loss: 0.0065\n",
            "Epoch 852/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.3371e-04 - val_loss: 0.0065\n",
            "Epoch 853/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.3212e-04 - val_loss: 0.0065\n",
            "Epoch 854/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.3033e-04 - val_loss: 0.0065\n",
            "Epoch 855/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.2872e-04 - val_loss: 0.0064\n",
            "Epoch 856/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.2705e-04 - val_loss: 0.0064\n",
            "Epoch 857/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 8.2540e-04 - val_loss: 0.0064\n",
            "Epoch 858/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.2374e-04 - val_loss: 0.0064\n",
            "Epoch 859/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 8.2203e-04 - val_loss: 0.0064\n",
            "Epoch 860/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.2038e-04 - val_loss: 0.0064\n",
            "Epoch 861/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 8.1882e-04 - val_loss: 0.0064\n",
            "Epoch 862/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.1714e-04 - val_loss: 0.0063\n",
            "Epoch 863/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.1546e-04 - val_loss: 0.0063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 864/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.1384e-04 - val_loss: 0.0063\n",
            "Epoch 865/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.1219e-04 - val_loss: 0.0063\n",
            "Epoch 866/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 8.1059e-04 - val_loss: 0.0063\n",
            "Epoch 867/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.0885e-04 - val_loss: 0.0063\n",
            "Epoch 868/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 8.0734e-04 - val_loss: 0.0063\n",
            "Epoch 869/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.0567e-04 - val_loss: 0.0062\n",
            "Epoch 870/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.0406e-04 - val_loss: 0.0062\n",
            "Epoch 871/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 8.0252e-04 - val_loss: 0.0062\n",
            "Epoch 872/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 8.0081e-04 - val_loss: 0.0062\n",
            "Epoch 873/1000\n",
            "12/12 [==============================] - 0s 694us/step - loss: 7.9920e-04 - val_loss: 0.0062\n",
            "Epoch 874/1000\n",
            "12/12 [==============================] - 0s 693us/step - loss: 7.9760e-04 - val_loss: 0.0062\n",
            "Epoch 875/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 7.9599e-04 - val_loss: 0.0061\n",
            "Epoch 876/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 7.9437e-04 - val_loss: 0.0061\n",
            "Epoch 877/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.9283e-04 - val_loss: 0.0061\n",
            "Epoch 878/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.9124e-04 - val_loss: 0.0061\n",
            "Epoch 879/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.8960e-04 - val_loss: 0.0061\n",
            "Epoch 880/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.8805e-04 - val_loss: 0.0061\n",
            "Epoch 881/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.8655e-04 - val_loss: 0.0061\n",
            "Epoch 882/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.8494e-04 - val_loss: 0.0060\n",
            "Epoch 883/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.8333e-04 - val_loss: 0.0060\n",
            "Epoch 884/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.8181e-04 - val_loss: 0.0060\n",
            "Epoch 885/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.8020e-04 - val_loss: 0.0060\n",
            "Epoch 886/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 7.7862e-04 - val_loss: 0.0060\n",
            "Epoch 887/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.7707e-04 - val_loss: 0.0060\n",
            "Epoch 888/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.7552e-04 - val_loss: 0.0060\n",
            "Epoch 889/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.7390e-04 - val_loss: 0.0059\n",
            "Epoch 890/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.7232e-04 - val_loss: 0.0059\n",
            "Epoch 891/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.7087e-04 - val_loss: 0.0059\n",
            "Epoch 892/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 7.6929e-04 - val_loss: 0.0059\n",
            "Epoch 893/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.6778e-04 - val_loss: 0.0059\n",
            "Epoch 894/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.6626e-04 - val_loss: 0.0059\n",
            "Epoch 895/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.6464e-04 - val_loss: 0.0058\n",
            "Epoch 896/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.6312e-04 - val_loss: 0.0058\n",
            "Epoch 897/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 7.6155e-04 - val_loss: 0.0058\n",
            "Epoch 898/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.6012e-04 - val_loss: 0.0058\n",
            "Epoch 899/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.5860e-04 - val_loss: 0.0058\n",
            "Epoch 900/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.5700e-04 - val_loss: 0.0058\n",
            "Epoch 901/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 7.5549e-04 - val_loss: 0.0057\n",
            "Epoch 902/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.5388e-04 - val_loss: 0.0057\n",
            "Epoch 903/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.5239e-04 - val_loss: 0.0057\n",
            "Epoch 904/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.5092e-04 - val_loss: 0.0057\n",
            "Epoch 905/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.4948e-04 - val_loss: 0.0057\n",
            "Epoch 906/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.4789e-04 - val_loss: 0.0057\n",
            "Epoch 907/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.4633e-04 - val_loss: 0.0056\n",
            "Epoch 908/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.4480e-04 - val_loss: 0.0056\n",
            "Epoch 909/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.4333e-04 - val_loss: 0.0056\n",
            "Epoch 910/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.4179e-04 - val_loss: 0.0056\n",
            "Epoch 911/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 7.4033e-04 - val_loss: 0.0056\n",
            "Epoch 912/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.3886e-04 - val_loss: 0.0056\n",
            "Epoch 913/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.3735e-04 - val_loss: 0.0055\n",
            "Epoch 914/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 7.3591e-04 - val_loss: 0.0055\n",
            "Epoch 915/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.3443e-04 - val_loss: 0.0055\n",
            "Epoch 916/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.3291e-04 - val_loss: 0.0055\n",
            "Epoch 917/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 7.3148e-04 - val_loss: 0.0055\n",
            "Epoch 918/1000\n",
            "12/12 [==============================] - 0s 669us/step - loss: 7.3000e-04 - val_loss: 0.0055\n",
            "Epoch 919/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 7.2849e-04 - val_loss: 0.0054\n",
            "Epoch 920/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 7.2698e-04 - val_loss: 0.0054\n",
            "Epoch 921/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.2549e-04 - val_loss: 0.0054\n",
            "Epoch 922/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.2402e-04 - val_loss: 0.0054\n",
            "Epoch 923/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 7.2262e-04 - val_loss: 0.0054\n",
            "Epoch 924/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.2117e-04 - val_loss: 0.0054\n",
            "Epoch 925/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.1963e-04 - val_loss: 0.0053\n",
            "Epoch 926/1000\n",
            "12/12 [==============================] - 0s 664us/step - loss: 7.1821e-04 - val_loss: 0.0053\n",
            "Epoch 927/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 7.1685e-04 - val_loss: 0.0053\n",
            "Epoch 928/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.1531e-04 - val_loss: 0.0053\n",
            "Epoch 929/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.1389e-04 - val_loss: 0.0053\n",
            "Epoch 930/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.1249e-04 - val_loss: 0.0053\n",
            "Epoch 931/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.1099e-04 - val_loss: 0.0052\n",
            "Epoch 932/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.0955e-04 - val_loss: 0.0052\n",
            "Epoch 933/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.0810e-04 - val_loss: 0.0052\n",
            "Epoch 934/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.0667e-04 - val_loss: 0.0052\n",
            "Epoch 935/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.0523e-04 - val_loss: 0.0052\n",
            "Epoch 936/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.0379e-04 - val_loss: 0.0052\n",
            "Epoch 937/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 7.0232e-04 - val_loss: 0.0051\n",
            "Epoch 938/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 7.0095e-04 - val_loss: 0.0051\n",
            "Epoch 939/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.9956e-04 - val_loss: 0.0051\n",
            "Epoch 940/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 0s 1ms/step - loss: 6.9804e-04 - val_loss: 0.0051\n",
            "Epoch 941/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.9666e-04 - val_loss: 0.0051\n",
            "Epoch 942/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.9526e-04 - val_loss: 0.0051\n",
            "Epoch 943/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.9381e-04 - val_loss: 0.0050\n",
            "Epoch 944/1000\n",
            "12/12 [==============================] - 0s 559us/step - loss: 6.9241e-04 - val_loss: 0.0050\n",
            "Epoch 945/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.9095e-04 - val_loss: 0.0050\n",
            "Epoch 946/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 6.8960e-04 - val_loss: 0.0050\n",
            "Epoch 947/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.8818e-04 - val_loss: 0.0050\n",
            "Epoch 948/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 6.8677e-04 - val_loss: 0.0050\n",
            "Epoch 949/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.8539e-04 - val_loss: 0.0049\n",
            "Epoch 950/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.8400e-04 - val_loss: 0.0049\n",
            "Epoch 951/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.8254e-04 - val_loss: 0.0049\n",
            "Epoch 952/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.8109e-04 - val_loss: 0.0049\n",
            "Epoch 953/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.7978e-04 - val_loss: 0.0049\n",
            "Epoch 954/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.7844e-04 - val_loss: 0.0049\n",
            "Epoch 955/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.7696e-04 - val_loss: 0.0048\n",
            "Epoch 956/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.7563e-04 - val_loss: 0.0048\n",
            "Epoch 957/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 6.7414e-04 - val_loss: 0.0048\n",
            "Epoch 958/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 6.7282e-04 - val_loss: 0.0048\n",
            "Epoch 959/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.7145e-04 - val_loss: 0.0048\n",
            "Epoch 960/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.7004e-04 - val_loss: 0.0048\n",
            "Epoch 961/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.6866e-04 - val_loss: 0.0047\n",
            "Epoch 962/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.6731e-04 - val_loss: 0.0047\n",
            "Epoch 963/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.6596e-04 - val_loss: 0.0047\n",
            "Epoch 964/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 6.6461e-04 - val_loss: 0.0047\n",
            "Epoch 965/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.6325e-04 - val_loss: 0.0047\n",
            "Epoch 966/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.6183e-04 - val_loss: 0.0046\n",
            "Epoch 967/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.6046e-04 - val_loss: 0.0046\n",
            "Epoch 968/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.5912e-04 - val_loss: 0.0046\n",
            "Epoch 969/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.5775e-04 - val_loss: 0.0046\n",
            "Epoch 970/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.5639e-04 - val_loss: 0.0046\n",
            "Epoch 971/1000\n",
            "12/12 [==============================] - 0s 665us/step - loss: 6.5507e-04 - val_loss: 0.0046\n",
            "Epoch 972/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.5368e-04 - val_loss: 0.0045\n",
            "Epoch 973/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.5236e-04 - val_loss: 0.0045\n",
            "Epoch 974/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.5099e-04 - val_loss: 0.0045\n",
            "Epoch 975/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 6.4972e-04 - val_loss: 0.0045\n",
            "Epoch 976/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.4838e-04 - val_loss: 0.0045\n",
            "Epoch 977/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 6.4700e-04 - val_loss: 0.0045\n",
            "Epoch 978/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.4569e-04 - val_loss: 0.0044\n",
            "Epoch 979/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.4437e-04 - val_loss: 0.0044\n",
            "Epoch 980/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.4304e-04 - val_loss: 0.0044\n",
            "Epoch 981/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.4170e-04 - val_loss: 0.0044\n",
            "Epoch 982/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.4041e-04 - val_loss: 0.0044\n",
            "Epoch 983/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.3903e-04 - val_loss: 0.0044\n",
            "Epoch 984/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.3780e-04 - val_loss: 0.0043\n",
            "Epoch 985/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 6.3650e-04 - val_loss: 0.0043\n",
            "Epoch 986/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.3512e-04 - val_loss: 0.0043\n",
            "Epoch 987/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.3385e-04 - val_loss: 0.0043\n",
            "Epoch 988/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.3251e-04 - val_loss: 0.0043\n",
            "Epoch 989/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.3126e-04 - val_loss: 0.0043\n",
            "Epoch 990/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 6.2990e-04 - val_loss: 0.0042\n",
            "Epoch 991/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.2866e-04 - val_loss: 0.0042\n",
            "Epoch 992/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.2734e-04 - val_loss: 0.0042\n",
            "Epoch 993/1000\n",
            "12/12 [==============================] - 0s 667us/step - loss: 6.2603e-04 - val_loss: 0.0042\n",
            "Epoch 994/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.2477e-04 - val_loss: 0.0042\n",
            "Epoch 995/1000\n",
            "12/12 [==============================] - 0s 1ms/step - loss: 6.2350e-04 - val_loss: 0.0042\n",
            "Epoch 996/1000\n",
            "12/12 [==============================] - 0s 960us/step - loss: 6.2217e-04 - val_loss: 0.0041\n",
            "Epoch 997/1000\n",
            "12/12 [==============================] - 0s 790us/step - loss: 6.2087e-04 - val_loss: 0.0041\n",
            "Epoch 998/1000\n",
            "12/12 [==============================] - 0s 259us/step - loss: 6.1951e-04 - val_loss: 0.0041\n",
            "Epoch 999/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.1826e-04 - val_loss: 0.0041\n",
            "Epoch 1000/1000\n",
            "12/12 [==============================] - 0s 666us/step - loss: 6.1699e-04 - val_loss: 0.0041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_input' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-236-7f085cab51e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtest_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'test_input' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5Cy13HFeRmt",
        "outputId": "e8a72721-1384-4ab1-d550-854ddfa77225"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_1 (Bidirection (None, 100)               21200     \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 21,301\n",
            "Trainable params: 21,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CTubRqKeRmt",
        "outputId": "dfc9d428-93d1-489b-c788-3f764dd3ce0c"
      },
      "source": [
        "inputs = tf.random.normal([32, 10, 8])\n",
        "lstm = tf.keras.layers.LSTM(1)\n",
        "output = lstm(inputs)\n",
        "print(output.shape)\n",
        "\n",
        "lstm = tf.keras.layers.LSTM(1, return_sequences=True, return_state=True)\n",
        "whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n",
        "print(whole_seq_output.shape)\n",
        "\n",
        "print(final_memory_state.shape)\n",
        "\n",
        "print(final_carry_state.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 1)\n",
            "(32, 10, 1)\n",
            "(32, 1)\n",
            "(32, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voH95og7eRmt"
      },
      "source": [
        "from keras.preprocessing import sequence \n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Embedding \n",
        "from keras.layers import LSTM \n",
        "from keras.datasets import imdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLueF3E5eRmt"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = 2000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYLSA3gfeRmt",
        "outputId": "9b7e061d-5a2f-48ae-9503-521c3d2253d2"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 2,\n",
              " 66,\n",
              " 2,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 2,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 2,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2,\n",
              " 2,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 2,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 2,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 2,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 2,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 2,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 2,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 2,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4QyNQP3eRmt"
      },
      "source": [
        "x_train = sequence.pad_sequences(x_train, maxlen=80) \n",
        "x_test = sequence.pad_sequences(x_test, maxlen=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPnePBdfeRmt"
      },
      "source": [
        "model = Sequential() \n",
        "model.add(Embedding(2000, 128)) \n",
        "model.add(LSTM(128, dropout = 0.2, recurrent_dropout = 0.2)) \n",
        "model.add(Dense(1, activation = 'sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ef38rLIeRmt"
      },
      "source": [
        "model.compile(loss = 'binary_crossentropy', \n",
        "   optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnT3mkUReRmt",
        "outputId": "fa2ae3ea-61b7-4e0a-f969-39860b1ff767"
      },
      "source": [
        "model.fit(\n",
        "   x_train, y_train, \n",
        "   batch_size = 32, \n",
        "   epochs = 15, \n",
        "   validation_data = (x_test, y_test)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/15\n",
            "24992/25000 [============================>.] - ETA: 0s - loss: 0.4821 - accuracy: 0.7668"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-257-651952d959b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m    \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m    \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m    \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m )\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    208\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m                                          verbose=0)\n\u001b[0m\u001b[0;32m    211\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                     \u001b[1;31m# Same labels assumed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3792\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3794\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \"\"\"\n\u001b[1;32m-> 1605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1645\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aW9DowAeRmt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}